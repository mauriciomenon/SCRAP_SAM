--- /Users/menon/git/scrap_sam_rework/src/dashboard/Class/run.py	2025-09-02 02:16:21.505841+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Class/run.py	2025-09-02 21:57:43.944007+00:00
@@ -6,22 +6,24 @@
 import socket
 import platform
 from pathlib import Path
 from datetime import datetime
 
+
 def get_python_command():
     """Determina o comando Python com base no sistema operacional."""
     system = platform.system().lower()
-    
-    if system == 'darwin':  # macOS
-        return 'python3'
-    elif system == 'linux':
-        return 'python3'
-    elif system == 'windows':
-        return 'python'
+
+    if system == "darwin":  # macOS
+        return "python3"
+    elif system == "linux":
+        return "python3"
+    elif system == "windows":
+        return "python"
     else:
-        return 'python'  # fallback padrão
+        return "python"  # fallback padrão
+
 
 # Configura o comando Python correto para o sistema
 PYTHON_CMD = get_python_command()
 
 # Adiciona o diretório atual ao PYTHONPATH usando Path
@@ -35,87 +37,99 @@
 from src.utils.log_manager import LogManager
 
 # Configurações globais
 warnings.filterwarnings("ignore")
 
+
 def setup_logging():
     """Configura o sistema de logging."""
     log_dir = Path("logs")
     log_dir.mkdir(exist_ok=True)
-    
+
     log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
     logging.basicConfig(
         level=logging.INFO,
         format=log_format,
         handlers=[
             logging.FileHandler(log_dir / "dashboard.log", encoding="utf-8"),
             logging.StreamHandler(),
         ],
     )
 
+
 def get_available_port(starting_port=8080):
     """Tenta encontrar uma porta disponível a partir da porta inicial."""
     port = starting_port
     while True:
         try:
             with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
-                if sock.connect_ex(('localhost', port)) != 0:
+                if sock.connect_ex(("localhost", port)) != 0:
                     return port
         except:
             pass
         port += 1
 
+
 def main():
     try:
         setup_logging()
-        
+
         base_dir = Path.cwd()
         downloads_dir = base_dir / "downloads"
         downloads_dir.mkdir(exist_ok=True)
-        
+
         file_manager = FileManager(downloads_dir)
-        
+
         try:
             latest_file = file_manager.get_latest_file("ssa_pendentes")
             file_info = file_manager.get_file_info(latest_file)
             DATA_FILE_PATH = latest_file
             print(f"\nUsando arquivo: {file_info['name']}")
-            print(f"Última modificação: {file_info['modified'].strftime('%d/%m/%Y %H:%M:%S')}")
+            print(
+                f"Última modificação: {file_info['modified'].strftime('%d/%m/%Y %H:%M:%S')}"
+            )
         except FileNotFoundError:
-            DATA_FILE_PATH = downloads_dir / "SSAs Pendentes Geral - 05-11-2024_0753AM.xlsx"
+            DATA_FILE_PATH = (
+                downloads_dir / "SSAs Pendentes Geral - 05-11-2024_0753AM.xlsx"
+            )
             print(f"\nUsando arquivo padrão: {DATA_FILE_PATH.name}")
 
-        print("""
+        print(
+            """
 ██████╗  █████╗ ███████╗██╗  ██╗    ███████╗███████╗ █████╗ 
 ██╔══██╗██╔══██╗██╔════╝██║  ██║    ██╔════╝██╔════╝██╔══██╗
 ██║  ██║███████║███████╗███████║    ███████╗███████╗███████║
 ██║  ██║██╔══██║╚════██║██╔══██║    ╚════██║╚════██║██╔══██║
 ██████╔╝██║  ██║███████║██║  ██║    ███████║███████║██║  ██║
 ╚═════╝ ╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝    ╚══════╝╚══════╝╚═╝  ╚═╝
-        """)
+        """
+        )
 
         print("\nIniciando carregamento dos dados...")
         loader = DataLoader(DATA_FILE_PATH)
         df = loader.load_data()
         print(f"Dados carregados com sucesso. Total de SSAs: {len(df)}")
-        
+
         print("\nIniciando dashboard...")
         app = SSADashboard(df)
-        
+
         port = get_available_port(8080)
-        
-        print(f"""
+
+        print(
+            f"""
 Dashboard iniciado com sucesso!
 URL: http://localhost:{port}
 Data: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
 Pressione CTRL+C para encerrar.
-        """)
-        
+        """
+        )
+
         app.run_server(debug=True, port=port)
-        
+
     except Exception as e:
         logging.error(f"Erro ao iniciar aplicação: {str(e)}")
         logging.error(traceback.format_exc())
         sys.exit(1)
 
+
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Class/run.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/data/ssa_data.py	2025-09-02 02:16:21.533803+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/data/ssa_data.py	2025-09-02 21:57:43.992195+00:00
@@ -2,13 +2,15 @@
 from dataclasses import dataclass, fields
 from datetime import datetime
 from typing import Dict, Optional
 import pandas as pd
 
+
 @dataclass
 class SSAData:
     """Estrutura de dados para uma SSA."""
+
     numero: str
     situacao: str
     derivada: Optional[str]
     localizacao: str
     desc_localizacao: str
@@ -64,11 +66,15 @@
             "derivada": self.derivada,
             "localizacao": self.localizacao,
             "desc_localizacao": self.desc_localizacao,
             "equipamento": self.equipamento,
             "semana_cadastro": self.semana_cadastro,
-            "emitida_em": self.emitida_em.strftime("%Y-%m-%d %H:%M:%S") if self.emitida_em else None,
+            "emitida_em": (
+                self.emitida_em.strftime("%Y-%m-%d %H:%M:%S")
+                if self.emitida_em
+                else None
+            ),
             "descricao": self.descricao,
             "setor_emissor": self.setor_emissor,
             "setor_executor": self.setor_executor,
             "solicitante": self.solicitante,
             "servico_origem": self.servico_origem,
@@ -91,11 +97,13 @@
             "Derivada de": self.derivada or "-",
             "Localização": self.localizacao,
             "Desc. Localização": self.desc_localizacao,
             "Equipamento": self.equipamento,
             "Semana Cadastro": self.semana_cadastro,
-            "Emitida em": self.emitida_em.strftime("%d/%m/%Y %H:%M") if self.emitida_em else "-",
+            "Emitida em": (
+                self.emitida_em.strftime("%d/%m/%Y %H:%M") if self.emitida_em else "-"
+            ),
             "Descrição": self.descricao,
             "Setor Emissor": self.setor_emissor,
             "Setor Executor": self.setor_executor,
             "Solicitante": self.solicitante,
             "Serviço Origem": self.servico_origem,
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/data/ssa_data.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/data/ssa_columns.py	2025-09-02 02:16:21.534059+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/data/ssa_columns.py	2025-09-02 21:57:44.002787+00:00
@@ -1,9 +1,10 @@
 # src/data/ssa_columns.py
 from typing import Dict, Any
 import pandas as pd
 from datetime import datetime
+
 
 class SSAColumns:
     """Mantém os índices e nomes das colunas."""
 
     # Índices
@@ -89,11 +90,11 @@
         DERIVADA: str,
         LOCALIZACAO: str,
         DESC_LOCALIZACAO: str,
         EQUIPAMENTO: str,
         SEMANA_CADASTRO: str,
-        EMITIDA_EM: 'datetime64[ns]',
+        EMITIDA_EM: "datetime64[ns]",
         DESC_SSA: str,
         SETOR_EMISSOR: str,
         SETOR_EXECUTOR: str,
         SOLICITANTE: str,
         SERVICO_ORIGEM: str,
@@ -136,14 +137,14 @@
     def validate_column_type(cls, index: int, value: Any) -> bool:
         """Valida o tipo de dado de uma coluna."""
         expected_type = cls.COLUMN_TYPES.get(index)
         if not expected_type:
             return True  # Se não tiver tipo definido, considera válido
-        
-        if expected_type == 'datetime64[ns]':
+
+        if expected_type == "datetime64[ns]":
             return isinstance(value, (datetime, pd.Timestamp)) or pd.isna(value)
-        
+
         return isinstance(value, expected_type) or pd.isna(value)
 
     @classmethod
     def is_required(cls, index: int) -> bool:
         """Verifica se a coluna é obrigatória."""
@@ -157,13 +158,21 @@
     @classmethod
     def get_display_config(cls) -> Dict[str, Dict]:
         """Retorna configurações de exibição para as colunas."""
         return {
             cls.get_name(idx): {
-                "width": "150px" if idx not in [cls.DESC_SSA, cls.DESC_LOCALIZACAO] else "300px",
+                "width": (
+                    "150px"
+                    if idx not in [cls.DESC_SSA, cls.DESC_LOCALIZACAO]
+                    else "300px"
+                ),
                 "textAlign": "left",
                 "overflow": "hidden",
                 "textOverflow": "ellipsis",
-                "whiteSpace": "normal" if idx in [cls.DESC_SSA, cls.DESC_LOCALIZACAO] else "nowrap",
+                "whiteSpace": (
+                    "normal"
+                    if idx in [cls.DESC_SSA, cls.DESC_LOCALIZACAO]
+                    else "nowrap"
+                ),
             }
             for idx in cls.COLUMN_NAMES.keys()
         }
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/data/ssa_columns.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/utils/file_manager.py	2025-09-02 02:16:21.520024+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/utils/file_manager.py	2025-09-02 21:57:44.014946+00:00
@@ -4,69 +4,71 @@
 import logging
 from datetime import datetime
 from typing import Optional, Dict
 from pathlib import Path
 
+
 class FileManager:
     """Gerencia o carregamento e identificação de arquivos de SSA."""
 
     def __init__(self, base_directory: str):
         """
         Inicializa o gerenciador de arquivos.
-        
+
         Args:
             base_directory (str): Diretório base onde procurar os arquivos
         """
         self.base_directory = Path(base_directory)
         self.file_patterns: Dict[str, re.Pattern] = {
-            'ssa_pendentes': re.compile(
+            "ssa_pendentes": re.compile(
                 r"SSAs Pendentes Geral - (\d{2})-(\d{2})-(\d{4})_(\d{4})(AM|PM)\.xlsx"
             ),
-            'ssa_programadas': re.compile(
+            "ssa_programadas": re.compile(
                 r"SSAs Programadas - (\d{2})-(\d{2})-(\d{4})_(\d{4})(AM|PM)\.xlsx"
             ),
             # Adicione outros padrões conforme necessário
         }
 
     def _convert_to_datetime(self, match: re.Match) -> datetime:
         """
         Converte os grupos do match em objeto datetime.
-        
+
         Args:
             match: Match object do regex com grupos de data/hora
-            
+
         Returns:
             datetime: Data e hora extraídas do nome do arquivo
         """
         try:
             day, month, year, time, period = match.groups()
             hour = int(time[:2])
             minute = int(time[2:])
-            
+
             # Converte para formato 24 horas
             if period == "PM" and hour != 12:
                 hour += 12
             elif period == "AM" and hour == 12:
                 hour = 0
-                
+
             return datetime(int(year), int(month), int(day), hour, minute)
         except Exception as e:
             logging.error(f"Erro ao converter data/hora do arquivo: {str(e)}")
             raise
 
-    def get_latest_file(self, pattern_key: str, 
-                       subdirectory: Optional[str] = None) -> str:
+    def get_latest_file(
+        self, pattern_key: str, subdirectory: Optional[str] = None
+    ) -> str:
         """
         Encontra o arquivo mais recente que corresponde ao padrão especificado.
-        
+
         Args:
             pattern_key (str): Chave do padrão de arquivo ('ssa_pendentes', etc)
             subdirectory (str, optional): Subdiretório opcional para buscar
-            
+
         Returns:
             str: Caminho completo do arquivo mais recente
-            
+
         Raises:
             FileNotFoundError: Se nenhum arquivo correspondente for encontrado
             KeyError: Se o pattern_key não existir
         """
         try:
@@ -77,23 +79,21 @@
             search_dir = self.base_directory
             if subdirectory:
                 search_dir = search_dir / subdirectory
 
             if not search_dir.exists():
-                raise FileNotFoundError(
-                    f"Diretório '{search_dir}' não encontrado"
-                )
+                raise FileNotFoundError(f"Diretório '{search_dir}' não encontrado")
 
             latest_file = None
             latest_time = None
 
             # Lista todos os arquivos no diretório
             for file_path in search_dir.glob("*.xlsx"):
                 match = pattern.match(file_path.name)
                 if match:
                     file_datetime = self._convert_to_datetime(match)
-                    
+
                     if latest_time is None or file_datetime > latest_time:
                         latest_time = file_datetime
                         latest_file = file_path
 
             if latest_file:
@@ -113,11 +113,11 @@
             raise
 
     def register_pattern(self, key: str, pattern: str):
         """
         Registra um novo padrão de arquivo.
-        
+
         Args:
             key (str): Chave para identificar o padrão
             pattern (str): Padrão regex para o nome do arquivo
         """
         try:
@@ -128,14 +128,14 @@
             raise
 
     def validate_file(self, file_path: str) -> bool:
         """
         Valida se um arquivo existe e pode ser lido.
-        
+
         Args:
             file_path (str): Caminho do arquivo
-            
+
         Returns:
             bool: True se o arquivo é válido
         """
         try:
             path = Path(file_path)
@@ -154,28 +154,28 @@
             return False
 
     def get_file_info(self, file_path: str) -> Dict:
         """
         Retorna informações sobre um arquivo.
-        
+
         Args:
             file_path (str): Caminho do arquivo
-            
+
         Returns:
             Dict com informações do arquivo
         """
         try:
             path = Path(file_path)
             if not path.exists():
                 raise FileNotFoundError(f"Arquivo não encontrado: {file_path}")
 
             stats = path.stat()
             return {
-                'name': path.name,
-                'size': stats.st_size,
-                'created': datetime.fromtimestamp(stats.st_ctime),
-                'modified': datetime.fromtimestamp(stats.st_mtime),
-                'is_valid': self.validate_file(file_path)
+                "name": path.name,
+                "size": stats.st_size,
+                "created": datetime.fromtimestamp(stats.st_ctime),
+                "modified": datetime.fromtimestamp(stats.st_mtime),
+                "is_valid": self.validate_file(file_path),
             }
         except Exception as e:
             logging.error(f"Erro ao obter informações do arquivo: {str(e)}")
-            raise
\ No newline at end of file
+            raise
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/utils/file_manager.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/utils/data_validator.py	2025-09-02 02:16:21.513333+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/utils/data_validator.py	2025-09-02 21:57:44.075335+00:00
@@ -3,18 +3,21 @@
 from typing import List, Dict, Optional
 from datetime import datetime
 from dataclasses import dataclass
 from ..data.ssa_data import SSAData
 
+
 @dataclass
 class ValidationResult:
     """Resultado da validação de dados."""
+
     is_valid: bool
     issues: List[str]
     statistics: Dict
     timestamp: datetime
 
+
 class SSADataValidator:
     """Classe para validação de dados das SSAs."""
 
     def __init__(self):
         self.logger = logging.getLogger(__name__)
@@ -27,111 +30,128 @@
         try:
             # 1. Contagem por responsável
             resp_counts = {}
             for ssa in ssa_objects:
                 if ssa.responsavel_execucao:
-                    resp_counts[ssa.responsavel_execucao] = resp_counts.get(ssa.responsavel_execucao, 0) + 1
-
-            stats['resp_counts'] = resp_counts
+                    resp_counts[ssa.responsavel_execucao] = (
+                        resp_counts.get(ssa.responsavel_execucao, 0) + 1
+                    )
+
+            stats["resp_counts"] = resp_counts
 
             # 2. Verificação de inconsistências
             for resp, count in resp_counts.items():
                 ssas_list = [s for s in ssa_objects if s.responsavel_execucao == resp]
                 if len(ssas_list) != count:
-                    issues.append(f"Inconsistência na contagem para {resp}: contagem={count}, real={len(ssas_list)}")
+                    issues.append(
+                        f"Inconsistência na contagem para {resp}: contagem={count}, real={len(ssas_list)}"
+                    )
 
             # 3. Verificação de estados
             for ssa in ssa_objects:
                 # Verifica combinações inválidas de estados
                 if ssa.responsavel_execucao and not ssa.setor_executor:
-                    issues.append(f"SSA {ssa.numero} tem responsável mas não tem setor executor")
+                    issues.append(
+                        f"SSA {ssa.numero} tem responsável mas não tem setor executor"
+                    )
 
             # 4. Estatísticas gerais
-            stats.update({
-                'total_ssas': len(ssa_objects),
-                'ssas_com_responsavel': len([s for s in ssa_objects if s.responsavel_execucao]),
-                'ssas_sem_responsavel': len([s for s in ssa_objects if not s.responsavel_execucao]),
-                'setores_executores': len(set(s.setor_executor for s in ssa_objects if s.setor_executor)),
-                'timestamp': datetime.now(),
-            })
+            stats.update(
+                {
+                    "total_ssas": len(ssa_objects),
+                    "ssas_com_responsavel": len(
+                        [s for s in ssa_objects if s.responsavel_execucao]
+                    ),
+                    "ssas_sem_responsavel": len(
+                        [s for s in ssa_objects if not s.responsavel_execucao]
+                    ),
+                    "setores_executores": len(
+                        set(s.setor_executor for s in ssa_objects if s.setor_executor)
+                    ),
+                    "timestamp": datetime.now(),
+                }
+            )
 
         except Exception as e:
             self.logger.error(f"Erro na validação: {str(e)}")
             issues.append(f"Erro na validação: {str(e)}")
 
         return ValidationResult(
             is_valid=len(issues) == 0,
             issues=issues,
             statistics=stats,
-            timestamp=datetime.now()
+            timestamp=datetime.now(),
         )
 
     def verify_data_integrity(self, ssa_objects: List[SSAData]) -> Dict:
         """Verifica integridade periódica dos dados."""
         integrity_report = {
-            'timestamp': datetime.now(),
-            'total_records': len(ssa_objects),
-            'checks': [],
-            'warnings': []
+            "timestamp": datetime.now(),
+            "total_records": len(ssa_objects),
+            "checks": [],
+            "warnings": [],
         }
 
         try:
             # 1. Verificação de dados obrigatórios
             missing_required = [
-                ssa.numero for ssa in ssa_objects 
+                ssa.numero
+                for ssa in ssa_objects
                 if not all([ssa.numero, ssa.situacao, ssa.prioridade_emissao])
             ]
             if missing_required:
-                integrity_report['warnings'].append(
+                integrity_report["warnings"].append(
                     f"SSAs com dados obrigatórios faltando: {', '.join(missing_required)}"
                 )
 
             # 2. Verificação de datas
             future_dates = [
-                ssa.numero for ssa in ssa_objects 
+                ssa.numero
+                for ssa in ssa_objects
                 if ssa.emitida_em and ssa.emitida_em > datetime.now()
             ]
             if future_dates:
-                integrity_report['warnings'].append(
+                integrity_report["warnings"].append(
                     f"SSAs com datas futuras: {', '.join(future_dates)}"
                 )
 
             # 3. Verificação de duplicatas
             numeros_ssa = [ssa.numero for ssa in ssa_objects]
             duplicates = set([num for num in numeros_ssa if numeros_ssa.count(num) > 1])
             if duplicates:
-                integrity_report['warnings'].append(
+                integrity_report["warnings"].append(
                     f"SSAs duplicadas encontradas: {', '.join(duplicates)}"
                 )
 
             # 4. Estatísticas
-            integrity_report['checks'] = {
-                'duplicates_found': len(duplicates),
-                'future_dates_found': len(future_dates),
-                'missing_required_found': len(missing_required),
-                'total_warnings': len(integrity_report['warnings'])
+            integrity_report["checks"] = {
+                "duplicates_found": len(duplicates),
+                "future_dates_found": len(future_dates),
+                "missing_required_found": len(missing_required),
+                "total_warnings": len(integrity_report["warnings"]),
             }
 
         except Exception as e:
             self.logger.error(f"Erro na verificação de integridade: {str(e)}")
-            integrity_report['warnings'].append(f"Erro na verificação: {str(e)}")
+            integrity_report["warnings"].append(f"Erro na verificação: {str(e)}")
 
         return integrity_report
 
-    def check_graph_data_consistency(self, ssa_objects: List[SSAData], 
-                                   graph_data: Dict) -> List[str]:
+    def check_graph_data_consistency(
+        self, ssa_objects: List[SSAData], graph_data: Dict
+    ) -> List[str]:
         """Verifica consistência entre dados do gráfico e objetos SSA."""
         inconsistencies = []
 
         try:
             # Contagem real por responsável
             real_counts = {}
             for ssa in ssa_objects:
                 if ssa.responsavel_execucao:
-                    real_counts[ssa.responsavel_execucao] = real_counts.get(
-                        ssa.responsavel_execucao, 0
-                    ) + 1
+                    real_counts[ssa.responsavel_execucao] = (
+                        real_counts.get(ssa.responsavel_execucao, 0) + 1
+                    )
 
             # Compara com dados do gráfico
             for resp, count in graph_data.items():
                 real_count = real_counts.get(resp, 0)
                 if count != real_count:
@@ -143,16 +163,16 @@
                         f"SSAs para {resp}: "
                         f"{[s.numero for s in ssa_objects if s.responsavel_execucao == resp]}"
                     )
 
         except Exception as e:
-            self.logger.error(f"Erro na verificação de consistência do gráfico: {str(e)}")
+            self.logger.error(
+                f"Erro na verificação de consistência do gráfico: {str(e)}"
+            )
             inconsistencies.append(f"Erro na verificação: {str(e)}")
 
         return inconsistencies
-
-
 
     def diagnose_responsavel_data(
         self, ssa_objects: List[SSAData], area_emissora: str = None
     ) -> Dict:
         """Diagnóstico detalhado dos dados de responsáveis."""
@@ -205,11 +225,10 @@
 
         except Exception as e:
             logging.error(f"Erro no diagnóstico: {str(e)}")
             return None
 
-
     # Atualize a classe SSADataValidator para incluir verificações específicas:
     def validate_responsavel_consistency(
         self, ssa_objects: List[SSAData], area_emissora: str = None
     ) -> List[str]:
         """Valida consistência dos dados de responsáveis."""
@@ -222,19 +241,23 @@
                 if dados["total"] != len(dados["ssas"]):
                     issues.append(
                         f"Inconsistência na contagem de SSAs para responsável execução {resp}: "
                         f"contagem={dados['total']}, real={len(dados['ssas'])}"
                     )
-                    issues.append(f"SSAs do responsável {resp}: {', '.join(dados['ssas'])}")
+                    issues.append(
+                        f"SSAs do responsável {resp}: {', '.join(dados['ssas'])}"
+                    )
 
             for resp, dados in diagnostico["por_responsavel_prog"].items():
                 if dados["total"] != len(dados["ssas"]):
                     issues.append(
                         f"Inconsistência na contagem de SSAs para responsável programação {resp}: "
                         f"contagem={dados['total']}, real={len(dados['ssas'])}"
                     )
-                    issues.append(f"SSAs do responsável {resp}: {', '.join(dados['ssas'])}")
+                    issues.append(
+                        f"SSAs do responsável {resp}: {', '.join(dados['ssas'])}"
+                    )
 
             # Verifica dados nulos ou malformados
             for ssa in ssa_objects:
                 if ssa.responsavel_execucao and not isinstance(
                     ssa.responsavel_execucao, str
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/utils/data_validator.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/utils/log_manager.py	2025-09-02 02:16:21.514368+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/utils/log_manager.py	2025-09-02 21:57:44.092791+00:00
@@ -5,10 +5,11 @@
 import zipfile
 from datetime import datetime, timedelta
 from typing import Dict
 from flask import request
 
+
 class LogManager:
     """Gerencia o logging com rastreamento de IP e ações dos usuários."""
 
     def __init__(self):
         self.logger = logging.getLogger("DashboardLogger")
@@ -38,26 +39,27 @@
     def log_with_ip(self, level, message):
         """Log message with IP address from Flask request context."""
         try:
             # Tenta obter o IP do request do Flask
             from flask import request
+
             try:
                 ip = request.remote_addr
             except RuntimeError:
                 ip = "system"
         except Exception:
             ip = "system"
 
         # Controle de frequência de logs
         current_time = datetime.now()
         log_key = f"{ip}_{message}"
-        
+
         if log_key in self._last_log:
             # Só loga novamente após 5 minutos para a mesma mensagem do mesmo IP
             if (current_time - self._last_log[log_key]).total_seconds() < 300:
                 return
-            
+
         self._last_log[log_key] = current_time
 
         try:
             if ip != "system" and ip not in self.connected_ips:
                 self.connected_ips.add(ip)
@@ -109,104 +111,118 @@
         """Limpa logs antigos do arquivo de log."""
         try:
             log_file = "dashboard_activity.log"
             if not os.path.exists(log_file):
                 return
-            
+
             # Lê todas as linhas do arquivo
-            with open(log_file, 'r', encoding='utf-8') as f:
+            with open(log_file, "r", encoding="utf-8") as f:
                 lines = f.readlines()
-            
+
             # Filtra apenas logs recentes
             cutoff_date = datetime.now() - timedelta(days=days)
             recent_logs = []
-            
+
             for line in lines:
                 try:
                     # Extrai a data do log (assume formato padrão no início da linha)
-                    log_date_str = line.split('-')[0].strip()
+                    log_date_str = line.split("-")[0].strip()
                     log_date = datetime.strptime(log_date_str, "%Y-%m-%d %H:%M:%S,%f")
-                    
+
                     if log_date >= cutoff_date:
                         recent_logs.append(line)
                 except (ValueError, IndexError):
                     # Se não conseguir extrair a data, mantém o log
                     recent_logs.append(line)
-            
+
             # Reescreve o arquivo apenas com logs recentes
-            with open(log_file, 'w', encoding='utf-8') as f:
+            with open(log_file, "w", encoding="utf-8") as f:
                 f.writelines(recent_logs)
-                
+
             self.logger.info(f"Logs mais antigos que {days} dias foram removidos")
-            
+
         except Exception as e:
             self.logger.error(f"Erro ao limpar logs antigos: {str(e)}")
 
     def backup_logs(self, backup_dir: str = "log_backups"):
         """Cria backup dos logs atuais."""
         try:
             if not os.path.exists(backup_dir):
                 os.makedirs(backup_dir)
-            
+
             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-            backup_file = os.path.join(backup_dir, f"dashboard_activity_{timestamp}.log")
-            
+            backup_file = os.path.join(
+                backup_dir, f"dashboard_activity_{timestamp}.log"
+            )
+
             # Copia o arquivo de log atual
             if os.path.exists("dashboard_activity.log"):
                 shutil.copy2("dashboard_activity.log", backup_file)
-                
+
                 # Compacta o backup
-                with zipfile.ZipFile(f"{backup_file}.zip", 'w', zipfile.ZIP_DEFLATED) as zipf:
+                with zipfile.ZipFile(
+                    f"{backup_file}.zip", "w", zipfile.ZIP_DEFLATED
+                ) as zipf:
                     zipf.write(backup_file, os.path.basename(backup_file))
-                
+
                 # Remove o arquivo não compactado
                 os.remove(backup_file)
-                
+
                 self.logger.info(f"Backup dos logs criado: {backup_file}.zip")
             else:
                 self.logger.warning("Arquivo de log não encontrado para backup")
-            
+
         except Exception as e:
             self.logger.error(f"Erro ao criar backup dos logs: {str(e)}")
 
     def get_log_statistics(self) -> Dict:
         """Retorna estatísticas dos logs."""
         stats = {
             "total_users": len(self.active_users),
             "total_connections": len(self.connected_ips),
-            "active_users": len([u for u, info in self.active_users.items() 
-                               if (datetime.now() - info["last_activity"]).seconds < 3600]),
-            "total_actions": sum(info["action_count"] for info in self.active_users.values()),
+            "active_users": len(
+                [
+                    u
+                    for u, info in self.active_users.items()
+                    if (datetime.now() - info["last_activity"]).seconds < 3600
+                ]
+            ),
+            "total_actions": sum(
+                info["action_count"] for info in self.active_users.values()
+            ),
             "last_connection": None,
             "most_active_ip": None,
-            "most_actions": 0
+            "most_actions": 0,
         }
-        
+
         if self.active_users:
             # Encontra o usuário mais recente
-            latest_user = max(self.active_users.items(), 
-                            key=lambda x: x[1]["last_activity"])
+            latest_user = max(
+                self.active_users.items(), key=lambda x: x[1]["last_activity"]
+            )
             stats["last_connection"] = latest_user[1]["last_activity"]
-            
+
             # Encontra o usuário mais ativo
-            most_active = max(self.active_users.items(), 
-                            key=lambda x: x[1]["action_count"])
+            most_active = max(
+                self.active_users.items(), key=lambda x: x[1]["action_count"]
+            )
             stats["most_active_ip"] = most_active[0]
             stats["most_actions"] = most_active[1]["action_count"]
-        
+
         return stats
 
     def cleanup_inactive_users(self, timeout_minutes: int = 30):
         """Remove usuários inativos."""
         now = datetime.now()
         timeout = timedelta(minutes=timeout_minutes)
-        
+
         inactive_users = [
-            ip for ip, info in self.active_users.items()
+            ip
+            for ip, info in self.active_users.items()
             if now - info["last_activity"] > timeout
         ]
-        
+
         for ip in inactive_users:
             self.log_with_ip("INFO", f"Removendo usuário inativo: {ip}")
             del self.active_users[ip]
             if ip in self.connected_ips:
                 self.connected_ips.remove(ip)
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/utils/log_manager.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/dashboard/ssa_visualizer.py	2025-09-02 02:16:21.527261+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/dashboard/ssa_visualizer.py	2025-09-02 21:57:44.094920+00:00
@@ -202,17 +202,18 @@
             xaxis_title="Ano-Semana (ISO)",
             yaxis_title="Quantidade de SSAs",
             template="plotly_white",
             barmode="stack",
             showlegend=True,
-            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
+            legend=dict(
+                orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1
+            ),
             xaxis={"tickangle": -45},
             margin={"l": 50, "r": 20, "t": 50, "b": 100},
         )
 
         return fig
-
 
     def add_weeks_in_state_chart(self, df_filtered=None) -> go.Figure:
         """Cria gráfico mostrando distribuição de SSAs por tempo no estado."""
         df_to_use = df_filtered if df_filtered is not None else self.df
         weeks_in_state = self.week_analyzer.calculate_weeks_in_state()
@@ -225,11 +226,13 @@
         max_weeks = value_counts.index.max()
 
         if max_weeks > 50:
             bins = list(range(0, int(max_weeks) + 10, 10))
             labels = [f"{bins[i]}-{bins[i+1]-1} semanas" for i in range(len(bins) - 1)]
-            binned_data = pd.cut(value_counts.index, bins=bins, labels=labels, right=False)
+            binned_data = pd.cut(
+                value_counts.index, bins=bins, labels=labels, right=False
+            )
             value_counts = value_counts.groupby(binned_data).sum()
         else:
             value_counts.index = [f"{int(x)} semanas" for x in value_counts.index]
 
         hover_text = []
@@ -263,11 +266,13 @@
             except Exception as e:
                 logging.warning(f"Erro ao processar intervalo {interval}: {str(e)}")
                 continue
 
         valid_indices = [
-            i for i in range(len(hover_text)) if str(value_counts.index[i]).strip() != ""
+            i
+            for i in range(len(hover_text))
+            if str(value_counts.index[i]).strip() != ""
         ]
 
         if not valid_indices:
             return self._create_empty_chart()
 
@@ -280,11 +285,13 @@
                     textposition="auto",
                     name="SSAs por Semana",
                     marker_color="rgb(64, 83, 177)",
                     hovertext=[hover_text[i] for i in valid_indices],
                     hoverinfo="text",
-                    customdata=[list(ssas_by_interval.values())[i] for i in valid_indices],
+                    customdata=[
+                        list(ssas_by_interval.values())[i] for i in valid_indices
+                    ],
                     hoverlabel=dict(bgcolor="white", font_size=12, font_family="Arial"),
                     showlegend=False,
                 )
             ]
         )
@@ -335,22 +342,26 @@
                     return None
                 return int(week_str[4:])
             except ValueError:
                 return None
 
-        weeks_in_state = self.df.iloc[:, SSAColumns.SEMANA_CADASTRO].apply(get_week_number)
+        weeks_in_state = self.df.iloc[:, SSAColumns.SEMANA_CADASTRO].apply(
+            get_week_number
+        )
         current_week = int(self.current_date.strftime("%Y%W")[4:])
 
         # Garante que a diferença seja sempre positiva
         return weeks_in_state.apply(
             lambda x: max(0, current_week - x) if x is not None else None
         )
 
     def analyze_weeks(self, use_programmed: bool = True) -> pd.DataFrame:
         """Analisa distribuição de SSAs por semana com validação melhorada."""
         week_column = (
-            SSAColumns.SEMANA_PROGRAMADA if use_programmed else SSAColumns.SEMANA_CADASTRO
+            SSAColumns.SEMANA_PROGRAMADA
+            if use_programmed
+            else SSAColumns.SEMANA_CADASTRO
         )
 
         week_data = []
         for _, row in self.df.iterrows():
             week_str = str(row.iloc[week_column])
@@ -392,11 +403,10 @@
 
         # Ordenar por ano e semana
         analysis = analysis.sort_values(["year", "week"])
 
         return analysis
-
 
     def _create_empty_chart(self) -> go.Figure:
         """Creates an empty chart with a title when no data is available."""
         return go.Figure().update_layout(
             title="Distribuição de SSAs por Tempo no Estado Atual",
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/dashboard/ssa_visualizer.py
--- /Users/menon/git/scrap_sam_rework/src/scrapers/__init__.py	2025-09-02 02:27:53.974080+00:00
+++ /Users/menon/git/scrap_sam_rework/src/scrapers/__init__.py	2025-09-02 21:57:44.102731+00:00
@@ -13,6 +13,6 @@
 
 __version__ = "2.0.0"
 __author__ = "GitHub Copilot"
 __date__ = "2025-09-01"
 
-__all__ = ["SAMNavigator", "ErrorTracker", "run_scraping"]
\ No newline at end of file
+__all__ = ["SAMNavigator", "ErrorTracker", "run_scraping"]
would reformat /Users/menon/git/scrap_sam_rework/src/scrapers/__init__.py
--- /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/Scrap-Playwright.py	2025-09-02 02:16:21.639906+00:00
+++ /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/Scrap-Playwright.py	2025-09-02 21:57:44.229649+00:00
@@ -150,11 +150,10 @@
         except Exception as e:
             print(f"Erro ao configurar opções do relatório: {e}")
             self.page.screenshot(path="report_options_error.png")
             raise       
     '''
-
 
     def select_report_options(self):
         """Seleciona opções do relatório usando JavaScript e força uma atualização final."""
         try:
             print("Selecionando 'Relatório com Detalhes'...")
@@ -247,21 +246,26 @@
     def verify_selections(self, checkboxes):
         """Verifica se todas as opções foram selecionadas corretamente."""
         for name, selector in checkboxes.items():
             try:
                 # Usar evaluate para verificar o estado do checkbox
-                is_checked = self.page.evaluate("""(selector) => {
+                is_checked = self.page.evaluate(
+                    """(selector) => {
                     const element = document.querySelector(selector);
                     return element ? element.checked : false;
-                }""", selector)
+                }""",
+                    selector,
+                )
 
                 if not is_checked:
-                    raise ValueError(f"Checkbox '{name}' não está selecionado como esperado")
+                    raise ValueError(
+                        f"Checkbox '{name}' não está selecionado como esperado"
+                    )
 
             except Exception as e:
                 print(f"Erro ao verificar seleção de '{name}': {e}")
-                raise   
+                raise
 
     def wait_for_loading_complete(self):
         """Aguarda a barra de progresso aparecer e depois desaparecer."""
         try:
             loading_bar_id = "SAMTemplateAssets_wt93_block_IguazuTheme_wt30_block_wt31_OutSystemsUIWeb_wt2_block_RichWidgets_wt15_block_wtdivWait"
@@ -316,11 +320,13 @@
                     # Clicar no botão de três pontos
                     self.page.click(f"xpath={three_dots_xpath}")
                     print("Clicado no botão de três pontos.")
 
                     # Aguardar o menu aparecer
-                    self.page.wait_for_selector("text=Exportar para Excel", state="visible")
+                    self.page.wait_for_selector(
+                        "text=Exportar para Excel", state="visible"
+                    )
                     print("Opção 'Exportar para Excel' visível.")
 
                     # Clicar na opção de exportar
                     self.page.click("text=Exportar para Excel", timeout=30000)
                     print("Clicado em 'Exportar para Excel'")
@@ -432,11 +438,11 @@
 
     # Clicar na lupa para gerar o relatório
     navigator.click_search()
 
     # Aguardar para garantir que o relatório seja gerado
-    page.wait_for_load_state('networkidle')
+    page.wait_for_load_state("networkidle")
 
     # Após select_report_options, adicionar uma espera extra
     navigator.select_report_options()
     page.wait_for_load_state("networkidle")
     page.wait_for_timeout(5000)  # Dar tempo extra para estabilizar
would reformat /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/Scrap-Playwright.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Dashboard_SM.py	2025-09-02 02:16:21.493978+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Dashboard_SM.py	2025-09-02 21:57:44.260957+00:00
@@ -12,10 +12,11 @@
     SSAColumns,
     SSAAnalyzer,
     SSAVisualizer,
     SSAReporter,
 )
+
 
 class SSADashboard:
     """Dashboard interativo para análise de SSAs."""
 
     def __init__(self, df: pd.DataFrame):
@@ -507,10 +508,11 @@
         score = (
             metrics["taxa_programacao"] * 0.5 + metrics["taxa_execucao_simples"] * 0.5
         )
         return round(score * 100, 2)
 
+
 if __name__ == "__main__":
     try:
         # Test the dashboard with sample data
         import pandas as pd
 
--- /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/scrap_BeautifulSoup.py	2025-09-02 02:16:21.647711+00:00
+++ /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/scrap_BeautifulSoup.py	2025-09-02 21:57:44.266531+00:00
@@ -1,42 +1,44 @@
 from bs4 import BeautifulSoup
 import pandas as pd
 
 # Carregar o conteúdo do arquivo HTML
-file_path = 'c:\\Users\\menon\\Downloads\\https _apps.itaipu.gov.br_SAM_SMA_Reports_SSAsExecuted.aspx.htm'
-with open(file_path, 'r', encoding='utf-8') as file:
+file_path = "c:\\Users\\menon\\Downloads\\https _apps.itaipu.gov.br_SAM_SMA_Reports_SSAsExecuted.aspx.htm"
+with open(file_path, "r", encoding="utf-8") as file:
     content = file.read()
 
 # Analisar o conteúdo HTML
-soup = BeautifulSoup(content, 'html.parser')
+soup = BeautifulSoup(content, "html.parser")
 
 # Encontrar todos os elementos input e button
-elements = soup.find_all(['input', 'button'])
+elements = soup.find_all(["input", "button"])
 
 # Preparar uma lista para armazenar os dados
 data = []
 
 # Extrair informações relevantes de cada elemento
 for element in elements:
     element_type = element.name
-    element_id = element.get('id', '')
-    element_name = element.get('name', '')
-    element_value = element.get('value', '')
-    element_class = ' '.join(element.get('class', []))
-    
-    data.append({
-        'Type': element_type,
-        'ID': element_id,
-        'Name': element_name,
-        'Value': element_value,
-        'Class': element_class
-    })
+    element_id = element.get("id", "")
+    element_name = element.get("name", "")
+    element_value = element.get("value", "")
+    element_class = " ".join(element.get("class", []))
+
+    data.append(
+        {
+            "Type": element_type,
+            "ID": element_id,
+            "Name": element_name,
+            "Value": element_value,
+            "Class": element_class,
+        }
+    )
 
 # Criar um DataFrame com os dados
 df = pd.DataFrame(data)
 
 # Exibir o DataFrame
 print(df)
 
 # Salvar o DataFrame como uma tabela em um arquivo CSV
-output_path = 'c:\\Users\\menon\\Downloads\\web_elements_updated.csv'
+output_path = "c:\\Users\\menon\\Downloads\\web_elements_updated.csv"
 df.to_csv(output_path, index=False)
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Dashboard_SM.py
would reformat /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/scrap_BeautifulSoup.py
--- /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/Scrap-Playwright_otimizado.py	2025-09-02 02:16:21.641209+00:00
+++ /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/Scrap-Playwright_otimizado.py	2025-09-02 21:57:44.267571+00:00
@@ -150,14 +150,17 @@
             self.page.wait_for_selector(
                 "input[id*='ctl00'][id*='wtContent']", state="visible", timeout=10000
             )
 
             if not self.wait_for_loading_complete(timeout=90000):
-                raise Exception("Timeout aguardando carregamento após selecionar relatório detalhado")
+                raise Exception(
+                    "Timeout aguardando carregamento após selecionar relatório detalhado"
+                )
 
             # Mantido o JavaScript original dos checkboxes (sem alteração)
-            success = self.page.evaluate("""() => {
+            success = self.page.evaluate(
+                """() => {
                 try {
                     const checkboxesToCheck = ['ctl00', 'ctl04', 'ctl08', 'ctl02', 'ctl06', 'ctl10'];
                     const checkboxesToUncheck = ['ctl12'];
                     
                     const triggerEvents = (element) => {
@@ -194,11 +197,12 @@
                     return true;
                 } catch (error) {
                     console.error('Erro ao selecionar checkboxes:', error);
                     return false;
                 }
-            }""")
+            }"""
+            )
 
             if not success:
                 raise Exception("Falha ao selecionar opções via JavaScript")
 
             self.page.wait_for_timeout(1000)
@@ -206,13 +210,17 @@
             # Espera completa após os checkboxes
             print("Aguardando carregamento completo após configurar checkboxes...")
             max_attempts = 5
             for attempt in range(max_attempts):
                 if self.wait_for_loading_complete(timeout=90000):
-                    print("Carregamento completo confirmado, prosseguindo com exportação...")
+                    print(
+                        "Carregamento completo confirmado, prosseguindo com exportação..."
+                    )
                     break
-                print(f"Tentativa {attempt + 1}/{max_attempts} de verificar carregamento...")
+                print(
+                    f"Tentativa {attempt + 1}/{max_attempts} de verificar carregamento..."
+                )
                 self.page.wait_for_timeout(5000)
             else:
                 raise Exception(
                     "Não foi possível confirmar carregamento completo após checkboxes"
                 )
@@ -247,33 +255,38 @@
 
                 except Exception as e:
                     print(f"Erro ao verificar seleção de '{name}': {e}")
                     raise
 
-    def wait_for_loading_complete(self, timeout: int = 60000, after_checkboxes: bool = False):
+    def wait_for_loading_complete(
+        self, timeout: int = 60000, after_checkboxes: bool = False
+    ):
         """Aguarda carregamento da página com verificação adaptativa."""
         try:
             print("Verificando estado da página...")
             start_time = time.time()
             consecutive_success = 0
 
             while (time.time() - start_time) < (timeout / 1000):
                 # Verificação específica pós-checkboxes
                 if after_checkboxes:
-                    loading_complete = self.page.evaluate("""
+                    loading_complete = self.page.evaluate(
+                        """
                             () => {
                                 // Para checkboxes, focamos apenas na barra principal
                                 const loadingBar = document.querySelector('[id*="wtdivWait"]');
                                 if (loadingBar && window.getComputedStyle(loadingBar).display !== 'none') {
                                     return false;
                                 }
                                 return true;
                             }
-                        """)
+                        """
+                    )
                 else:
                     # Verificação completa normal
-                    loading_complete = self.page.evaluate("""
+                    loading_complete = self.page.evaluate(
+                        """
                             () => {
                                 const loadingBar = document.querySelector('[id*="wtdivWait"]');
                                 if (loadingBar && window.getComputedStyle(loadingBar).display !== 'none') {
                                     return false;
                                 }
@@ -294,11 +307,12 @@
                                     }
                                 }
                                 
                                 return true;
                             }
-                        """)
+                        """
+                    )
 
                 if loading_complete:
                     consecutive_success += 1
                     print(f"Verificação bem-sucedida ({consecutive_success}/3)")
 
@@ -306,15 +320,19 @@
                     required_success = 2 if after_checkboxes else 3
 
                     if consecutive_success >= required_success:
                         self.page.wait_for_load_state("networkidle", timeout=5000)
                         self.page.wait_for_timeout(2000)
-                        print("Carregamento completo confirmado após verificações consecutivas")
+                        print(
+                            "Carregamento completo confirmado após verificações consecutivas"
+                        )
                         return True
                 else:
                     if consecutive_success > 0:
-                        print("Resetando contador de verificações - loading detectado novamente")
+                        print(
+                            "Resetando contador de verificações - loading detectado novamente"
+                        )
                     consecutive_success = 0
 
                 self.page.wait_for_timeout(2000)
                 if consecutive_success == 0:
                     print("Ainda carregando... aguardando")
@@ -340,11 +358,12 @@
 
             try:
                 print("Tentando exportação via clique direto...")
                 with self.page.expect_download(timeout=90000) as download_promise:
                     # Verifica e clica no menu com retry
-                    success = self.page.evaluate("""
+                    success = self.page.evaluate(
+                        """
                         () => {
                             return new Promise((resolve) => {
                                 // Função para encontrar e clicar no botão do menu
                                 const clickMenuButton = () => {
                                     const menuButton = document.querySelector('[id*="wtMenuDropdown"] i');
@@ -377,21 +396,23 @@
                                 };
                                 
                                 tryClick();
                             });
                         }
-                    """)
+                    """
+                    )
 
                     if not success:
                         raise Exception("Não foi possível clicar no menu")
 
                     # Espera o menu aparecer
                     print("Aguardando menu de exportação...")
                     self.page.wait_for_timeout(2000)
 
                     # Verifica se o botão de exportar está visível e clicável
-                    button_ready = self.page.evaluate("""
+                    button_ready = self.page.evaluate(
+                        """
                         () => {
                             const exportLinks = Array.from(document.querySelectorAll('a'))
                                 .filter(a => a.textContent.includes('Exportar para Excel'));
                             
                             const isVisible = (element) => {
@@ -414,32 +435,35 @@
                                 visibleButton.style.pointerEvents = 'auto';
                                 return true;
                             }
                             return false;
                         }
-                    """)
+                    """
+                    )
 
                     if not button_ready:
                         raise Exception("Botão de exportação não está pronto")
 
                     # Espera final antes do clique
                     print("Aguardando botão estabilizar...")
                     self.page.wait_for_timeout(1000)
 
                     # Clique via JavaScript para garantir
                     print("Clicando no botão de exportação...")
-                    success = self.page.evaluate("""
+                    success = self.page.evaluate(
+                        """
                         () => {
                             const exportButton = Array.from(document.querySelectorAll('a'))
                                 .find(a => a.textContent.includes('Exportar para Excel'));
                             if (exportButton) {
                                 exportButton.click();
                                 return true;
                             }
                             return false;
                         }
-                    """)
+                    """
+                    )
 
                     if not success:
                         raise Exception("Falha ao clicar no botão de exportação")
 
                     print("Aguardando download iniciar...")
@@ -469,11 +493,12 @@
 
     def _export_via_javascript(self):
         """Método JavaScript de fallback para exportação."""
         try:
             with self.page.expect_download(timeout=90000) as download_promise:
-                success = self.page.evaluate("""
+                success = self.page.evaluate(
+                    """
                     () => {
                         return new Promise((resolve) => {
                             const attemptExport = (attempt = 0) => {
                                 if (attempt >= 5) {
                                     resolve(false);
@@ -500,11 +525,12 @@
                             };
                             
                             attemptExport();
                         });
                     }
-                """)
+                """
+                )
 
                 if success:
                     download = download_promise.value
                     download_file_path = os.path.join(
                         self.download_path, download.suggested_filename
would reformat /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/Scrap-Playwright_otimizado.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/data/data_loader.py	2025-09-02 02:16:21.530138+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/data/data_loader.py	2025-09-02 21:57:44.282789+00:00
@@ -9,10 +9,11 @@
 from ..utils.file_manager import FileManager
 from .ssa_data import SSAData
 from .ssa_columns import SSAColumns
 from ..utils.data_validator import SSADataValidator
 
+
 class DataLoader:
     """Carrega e prepara os dados das SSAs."""
 
     def __init__(self, excel_path: str):
         self.excel_path = excel_path
@@ -60,11 +61,13 @@
 
     def _convert_dates(self):
         """Converte e valida datas mantendo o tipo apropriado."""
         try:
             # Se a coluna já for datetime, não precisa converter
-            if pd.api.types.is_datetime64_any_dtype(self.df.iloc[:, SSAColumns.EMITIDA_EM]):
+            if pd.api.types.is_datetime64_any_dtype(
+                self.df.iloc[:, SSAColumns.EMITIDA_EM]
+            ):
                 logging.info("Coluna já está em formato datetime")
                 return
 
             # Converte diretamente para datetime usando o formato correto
             self.df.iloc[:, SSAColumns.EMITIDA_EM] = pd.to_datetime(
@@ -150,11 +153,13 @@
                 except Exception as e:
                     logging.error(f"Erro ao converter coluna {col}: {str(e)}")
 
             # Padroniza prioridades para maiúsculas
             self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] = (
-                self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].str.upper().str.strip()
+                self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO]
+                .str.upper()
+                .str.strip()
             )
 
             # Converte colunas opcionais
             optional_string_columns = [
                 SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO,
@@ -218,11 +223,13 @@
 
             # Converte para objetos SSAData
             self._convert_to_objects()
 
             # NOVO: Validação com o SSADataValidator
-            validation_result = self.validator.validate_data_consistency(self.ssa_objects)
+            validation_result = self.validator.validate_data_consistency(
+                self.ssa_objects
+            )
             if not validation_result.is_valid:
                 logging.warning("=== Problemas de Consistência Encontrados ===")
                 for issue in validation_result.issues:
                     logging.warning(issue)
 
@@ -280,11 +287,10 @@
                 )
 
         # Registra todos os problemas em uma única mensagem
         if issues:
             logging.warning("Problemas encontrados nos dados: " + "; ".join(issues))
-
 
     def _convert_to_objects(self) -> int:
         """
         Converte as linhas do DataFrame em objetos SSAData.
 
@@ -313,11 +319,13 @@
                         responsavel = responsavel.upper()
                         unique_responsaveis.add(responsavel)
                         conversions["exec"]["total"] += 1
 
                     # Processamento do responsável programação
-                    resp_prog = str(row.iloc[SSAColumns.RESPONSAVEL_PROGRAMACAO]).strip()
+                    resp_prog = str(
+                        row.iloc[SSAColumns.RESPONSAVEL_PROGRAMACAO]
+                    ).strip()
                     if resp_prog.lower() in ["nan", "none", ""]:
                         resp_prog = None
                     elif resp_prog:
                         resp_prog = resp_prog.upper()
                         unique_responsaveis_prog.add(resp_prog)
@@ -327,31 +335,39 @@
                     ssa = SSAData(
                         numero=str(row.iloc[SSAColumns.NUMERO_SSA]).strip(),
                         situacao=str(row.iloc[SSAColumns.SITUACAO]).strip(),
                         derivada=str(row.iloc[SSAColumns.DERIVADA]).strip() or None,
                         localizacao=str(row.iloc[SSAColumns.LOCALIZACAO]).strip(),
-                        desc_localizacao=str(row.iloc[SSAColumns.DESC_LOCALIZACAO]).strip(),
+                        desc_localizacao=str(
+                            row.iloc[SSAColumns.DESC_LOCALIZACAO]
+                        ).strip(),
                         equipamento=str(row.iloc[SSAColumns.EQUIPAMENTO]).strip(),
-                        semana_cadastro=str(row.iloc[SSAColumns.SEMANA_CADASTRO]).strip(),
+                        semana_cadastro=str(
+                            row.iloc[SSAColumns.SEMANA_CADASTRO]
+                        ).strip(),
                         emitida_em=(
                             row.iloc[SSAColumns.EMITIDA_EM]
                             if pd.notna(row.iloc[SSAColumns.EMITIDA_EM])
                             else None
                         ),
                         descricao=str(row.iloc[SSAColumns.DESC_SSA]).strip(),
                         setor_emissor=str(row.iloc[SSAColumns.SETOR_EMISSOR]).strip(),
                         setor_executor=str(row.iloc[SSAColumns.SETOR_EXECUTOR]).strip(),
                         solicitante=str(row.iloc[SSAColumns.SOLICITANTE]).strip(),
                         servico_origem=str(row.iloc[SSAColumns.SERVICO_ORIGEM]).strip(),
-                        prioridade_emissao=str(row.iloc[SSAColumns.GRAU_PRIORIDADE_EMISSAO])
+                        prioridade_emissao=str(
+                            row.iloc[SSAColumns.GRAU_PRIORIDADE_EMISSAO]
+                        )
                         .strip()
                         .upper(),
                         prioridade_planejamento=str(
                             row.iloc[SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO]
                         ).strip()
                         or None,
-                        execucao_simples=str(row.iloc[SSAColumns.EXECUCAO_SIMPLES]).strip(),
+                        execucao_simples=str(
+                            row.iloc[SSAColumns.EXECUCAO_SIMPLES]
+                        ).strip(),
                         responsavel_programacao=resp_prog,  # Já processado acima
                         semana_programada=str(
                             row.iloc[SSAColumns.SEMANA_PROGRAMADA]
                         ).strip()
                         or None,
@@ -434,11 +450,10 @@
             logging.info(f"Total SSAs: {len(ssas_resp)}")
             logging.info("Números das SSAs:")
             for ssa in ssas_resp:
                 logging.info(f"  - SSA {ssa.numero}: {ssa.situacao}")
 
-
     def _log_primeiro_objeto(self):
         """Log do primeiro objeto para verificação."""
         first_ssa = self.ssa_objects[0]
         logging.info("\n=== Primeiro Objeto Convertido (Verificação) ===")
         logging.info(f"Número: {first_ssa.numero}")
@@ -482,25 +497,30 @@
 
             # Validação de tipos
             if setor is not None and not isinstance(setor, str):
                 raise ValueError(f"Setor deve ser string, recebido {type(setor)}")
             if prioridade is not None and not isinstance(prioridade, str):
-                raise ValueError(f"Prioridade deve ser string, recebido {type(prioridade)}")
+                raise ValueError(
+                    f"Prioridade deve ser string, recebido {type(prioridade)}"
+                )
             if data_inicio is not None and not isinstance(data_inicio, datetime):
                 raise ValueError(
                     f"Data início deve ser datetime, recebido {type(data_inicio)}"
                 )
             if data_fim is not None and not isinstance(data_fim, datetime):
-                raise ValueError(f"Data fim deve ser datetime, recebido {type(data_fim)}")
+                raise ValueError(
+                    f"Data fim deve ser datetime, recebido {type(data_fim)}"
+                )
 
             # Filtro por setor com validação melhorada
             if setor:
                 setor = setor.strip().upper()
                 filtered_ssas = [
                     ssa
                     for ssa in filtered_ssas
-                    if ssa.setor_executor and ssa.setor_executor.strip().upper() == setor
+                    if ssa.setor_executor
+                    and ssa.setor_executor.strip().upper() == setor
                 ]
                 logging.info(f"Filtro por setor '{setor}': {len(filtered_ssas)} SSAs")
 
             # Filtro por prioridade
             if prioridade:
@@ -531,15 +551,19 @@
                 filtered_ssas = [
                     ssa
                     for ssa in filtered_ssas
                     if ssa.emitida_em and ssa.emitida_em <= data_fim
                 ]
-                logging.info(f"Filtro por data fim {data_fim}: {len(filtered_ssas)} SSAs")
+                logging.info(
+                    f"Filtro por data fim {data_fim}: {len(filtered_ssas)} SSAs"
+                )
 
             # Diagnóstico após todos os filtros
             if filtered_ssas:
-                diagnostico = self.validator.diagnose_responsavel_data(filtered_ssas, setor)
+                diagnostico = self.validator.diagnose_responsavel_data(
+                    filtered_ssas, setor
+                )
                 logging.info("=== Diagnóstico após Filtros ===")
                 logging.info(f"Total de SSAs filtradas: {len(filtered_ssas)}")
                 logging.info("Responsáveis na Execução:")
                 for resp, dados in diagnostico["por_responsavel_exec"].items():
                     logging.info(f"  - {resp}: {dados['total']} SSAs")
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/data/data_loader.py
--- /Users/menon/git/scrap_sam_rework/src/utils/Acha_botao.py	2025-09-02 02:16:21.429774+00:00
+++ /Users/menon/git/scrap_sam_rework/src/utils/Acha_botao.py	2025-09-02 21:57:44.311200+00:00
@@ -5,49 +5,64 @@
 import time
 
 driver = webdriver.Chrome()
 driver.get("https://apps.itaipu.gov.br/SAM_SMA_Reports/SSAsExecuted.aspx")
 
+
 def highlight_element(element, color="blue", border=4):
-    driver.execute_script("""
+    driver.execute_script(
+        """
     arguments[0].style.border = "%spx solid %s";
-    """ % (border, color), element)
+    """
+        % (border, color),
+        element,
+    )
+
 
 # Espere até que a página esteja carregada
-WebDriverWait(driver, 10).until(
-    EC.presence_of_element_located((By.TAG_NAME, "body"))
-)
+WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
 
 # Tente localizar o botão por ID
 try:
     button = WebDriverWait(driver, 10).until(
-        EC.presence_of_element_located((By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_OutSystemsUIWeb_wt60_block_wtWidget_wtSearchButton"))
+        EC.presence_of_element_located(
+            (
+                By.ID,
+                "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_OutSystemsUIWeb_wt60_block_wtWidget_wtSearchButton",
+            )
+        )
     )
     highlight_element(button)
     print("Botão encontrado por ID e destacado em azul")
 except:
     print("Botão não encontrado por ID")
 
 # Tente localizar o botão por texto (ícone de busca)
 try:
     search_icon = WebDriverWait(driver, 10).until(
-        EC.presence_of_element_located((By.XPATH, "//span[contains(@class, 'fa-search')]"))
+        EC.presence_of_element_located(
+            (By.XPATH, "//span[contains(@class, 'fa-search')]")
+        )
     )
-    parent_button = search_icon.find_element_by_xpath("..")  # Pega o elemento pai do ícone
+    parent_button = search_icon.find_element_by_xpath(
+        ".."
+    )  # Pega o elemento pai do ícone
     highlight_element(parent_button, color="green")
     print("Botão encontrado por ícone de busca e destacado em verde")
 except:
     print("Botão não encontrado por ícone de busca")
 
 # Tente localizar qualquer elemento com 'search' no ID
 try:
-    search_elements = driver.find_elements_by_xpath("//*[contains(@id, 'search') or contains(@id, 'Search')]")
+    search_elements = driver.find_elements_by_xpath(
+        "//*[contains(@id, 'search') or contains(@id, 'Search')]"
+    )
     for i, element in enumerate(search_elements):
         highlight_element(element, color=f"rgb({i*30}, {i*30}, 255)")
         print(f"Elemento de busca {i+1} encontrado e destacado")
 except:
     print("Nenhum elemento de busca adicional encontrado")
 
 print("Inspeção visual ativa por 30 segundos. Verifique os elementos destacados.")
 time.sleep(30)
 
-driver.quit()
\ No newline at end of file
+driver.quit()
would reformat /Users/menon/git/scrap_sam_rework/src/utils/Acha_botao.py
--- /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/scrap_SAM.py	2025-09-02 02:16:21.649139+00:00
+++ /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/scrap_SAM.py	2025-09-02 21:57:44.385644+00:00
@@ -12,86 +12,122 @@
 gecko_driver_path = os.path.join(base_path, "drivers", "geckodriver", "geckodriver.exe")
 firefox_binary_path = os.path.join(base_path, "drivers", "firefox", "firefox.exe")
 
 # Verifique se o Firefox foi instalado manualmente
 if not os.path.exists(firefox_binary_path):
-    print(f"Por favor, instale o Firefox manualmente a partir de {firefox_binary_path}.")
+    print(
+        f"Por favor, instale o Firefox manualmente a partir de {firefox_binary_path}."
+    )
     exit(1)
 
 # Caminho para o perfil do Firefox
-profile_path = "C:/Users/menon/AppData/Roaming/Mozilla/Firefox/Profiles/xm9lfsi1.default-esr"
+profile_path = (
+    "C:/Users/menon/AppData/Roaming/Mozilla/Firefox/Profiles/xm9lfsi1.default-esr"
+)
 
 # Configuração do WebDriver
 options = webdriver.FirefoxOptions()
 options.binary_location = firefox_binary_path
 
 # Configurar preferências de download para o Firefox
 options.set_preference("browser.download.folderList", 2)
 options.set_preference("browser.download.manager.showWhenStarting", False)
 options.set_preference("browser.download.dir", os.path.join(base_path, "downloads"))
-options.set_preference("browser.helperApps.neverAsk.saveToDisk", "application/vnd.ms-excel, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet, application/octet-stream")
-options.set_preference("pdfjs.disabled", True)  # Desabilita visualizador de PDF embutido
+options.set_preference(
+    "browser.helperApps.neverAsk.saveToDisk",
+    "application/vnd.ms-excel, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet, application/octet-stream",
+)
+options.set_preference(
+    "pdfjs.disabled", True
+)  # Desabilita visualizador de PDF embutido
 
 # Especificar o caminho para o GeckoDriver
-service = Service(gecko_driver_path, log_output=os.path.join(base_path, "geckodriver.log"))
+service = Service(
+    gecko_driver_path, log_output=os.path.join(base_path, "geckodriver.log")
+)
 
 driver = webdriver.Firefox(service=service, options=options)
 
 try:
     # Acessa a página
     driver.get("https://apps.itaipu.gov.br/SAM_SMA_Reports/SSAsExecuted.aspx")
-    
+
     # Aguarda o usuário fazer login manualmente
     print("Por favor, faça login manualmente e depois pressione Enter.")
     input("Pressione Enter depois de fazer login...")
 
     # Espera a página carregar o campo Setor Emissor
     WebDriverWait(driver, 30).until(
-        EC.presence_of_element_located((By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtSSADashboardFilter_SectorExecutor"))
+        EC.presence_of_element_located(
+            (
+                By.ID,
+                "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtSSADashboardFilter_SectorExecutor",
+            )
+        )
     )
 
     # Preenche o campo Setor Emissor
     setor_emissor = driver.find_element(
-        By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtSSADashboardFilter_SectorExecutor"
+        By.ID,
+        "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtSSADashboardFilter_SectorExecutor",
     )
     setor_emissor.send_keys("IEE3")
-    
+
     # Preenche os campos de Data de Execução (Ano/Semana)
     data_execucao_inicial = driver.find_element(
-        By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtPlanningYearWeekStart_input2"
+        By.ID,
+        "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtPlanningYearWeekStart_input2",
     )
     data_execucao_inicial.send_keys("202401")
-    
+
     data_execucao_final = driver.find_element(
-        By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtPlanningYearWeekEnd_input2"
+        By.ID,
+        "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtPlanningYearWeekEnd_input2",
     )
     data_execucao_final.send_keys("202426")
-    
+
     # Clica no botão de procurar
     procurar_button = WebDriverWait(driver, 30).until(
-        EC.presence_of_element_located((By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_OutSystemsUIWeb_wt57_block_wtWidget_wtSearchButton"))
+        EC.presence_of_element_located(
+            (
+                By.ID,
+                "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_OutSystemsUIWeb_wt57_block_wtWidget_wtSearchButton",
+            )
+        )
     )
     driver.execute_script("arguments[0].scrollIntoView();", procurar_button)
     driver.execute_script("arguments[0].click();", procurar_button)
-    
+
     # Espera a página carregar os resultados (tempo ajustável conforme necessário)
     WebDriverWait(driver, 60).until(
-        EC.presence_of_element_located((By.XPATH, "//i[@class='iguazu-ico iguazu-ico-more3 iguazu-ico-size-double']"))
+        EC.presence_of_element_located(
+            (
+                By.XPATH,
+                "//i[@class='iguazu-ico iguazu-ico-more3 iguazu-ico-size-double']",
+            )
+        )
     )
-    
+
     # Clica nos três pontinhos para exportar
-    menu_button = driver.find_element(By.XPATH, "//i[@class='iguazu-ico iguazu-ico-more3 iguazu-ico-size-double']")
+    menu_button = driver.find_element(
+        By.XPATH, "//i[@class='iguazu-ico iguazu-ico-more3 iguazu-ico-size-double']"
+    )
     driver.execute_script("arguments[0].scrollIntoView();", menu_button)
     driver.execute_script("arguments[0].click();", menu_button)
-    
+
     # Clica na opção de exportar para Excel
     export_button = WebDriverWait(driver, 30).until(
-        EC.presence_of_element_located((By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMenuDropdown_wtConditionalMenu_IguazuTheme_wt54_block_OutSystemsUIWeb_wt6_block_wtDropdownList_wtDropdownList_wtLink_ExportToExcel"))
+        EC.presence_of_element_located(
+            (
+                By.ID,
+                "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMenuDropdown_wtConditionalMenu_IguazuTheme_wt54_block_OutSystemsUIWeb_wt6_block_wtDropdownList_wtDropdownList_wtLink_ExportToExcel",
+            )
+        )
     )
     driver.execute_script("arguments[0].scrollIntoView();", export_button)
     driver.execute_script("arguments[0].click();", export_button)
-    
+
     # Espera o download do arquivo Excel (ajuste o tempo conforme necessário)
     time.sleep(20)  # Aguarde pelo tempo necessário para que o download seja concluído
 
 finally:
     # Para facilitar o debug, vamos deixar o navegador aberto comentando a linha abaixo:
@@ -103,10 +139,10 @@
 
 # Verifica se o arquivo foi baixado
 if os.path.exists(excel_path):
     # Carrega o arquivo Excel baixado e processa com pandas
     df = pd.read_excel(excel_path)
-    
+
     # Exemplo de manipulação de dados com pandas
     print(df.head())
 else:
     print(f"O arquivo {excel_path} não foi encontrado.")
would reformat /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/scrap_SAM.py
--- /Users/menon/git/scrap_sam_rework/src/utils/scrap_installer.py	2025-09-02 02:16:21.648825+00:00
+++ /Users/menon/git/scrap_sam_rework/src/utils/scrap_installer.py	2025-09-02 21:57:44.412696+00:00
@@ -15,16 +15,18 @@
 
 # Criar diretórios de destino se não existirem
 os.makedirs(firefox_path, exist_ok=True)
 os.makedirs(gecko_driver_path, exist_ok=True)
 
+
 # Função para baixar um arquivo
 def download_file(url, dest):
     response = requests.get(url, stream=True)
-    with open(dest, 'wb') as f:
+    with open(dest, "wb") as f:
         for chunk in response.iter_content(chunk_size=8192):
             f.write(chunk)
+
 
 # Baixar o instalador do Firefox
 firefox_installer_path = os.path.join(firefox_path, "firefox_installer.exe")
 download_file(firefox_url, firefox_installer_path)
 
@@ -32,10 +34,10 @@
 subprocess.run([firefox_installer_path, "/S"], check=True)
 
 # Baixar e extrair GeckoDriver
 gecko_driver_zip_path = os.path.join(gecko_driver_path, "geckodriver.zip")
 download_file(gecko_driver_url, gecko_driver_zip_path)
-with zipfile.ZipFile(gecko_driver_zip_path, 'r') as zip_ref:
+with zipfile.ZipFile(gecko_driver_zip_path, "r") as zip_ref:
     zip_ref.extractall(gecko_driver_path)
 os.remove(gecko_driver_zip_path)
 
 print("Firefox e GeckoDriver foram baixados e instalados com sucesso.")
would reformat /Users/menon/git/scrap_sam_rework/src/utils/scrap_installer.py
--- /Users/menon/git/scrap_sam_rework/tests/test_sanity_imports.py	2025-09-02 18:53:12.389029+00:00
+++ /Users/menon/git/scrap_sam_rework/tests/test_sanity_imports.py	2025-09-02 21:57:44.434010+00:00
@@ -1,14 +1,18 @@
 import importlib
 import pytest
 
-@pytest.mark.parametrize("mod", [
-    "pandas",
-    "selenium",
-    "dash",
-    "plotly",
-    "requests",
-    "bs4",
-    "yaml",
-])
+
+@pytest.mark.parametrize(
+    "mod",
+    [
+        "pandas",
+        "selenium",
+        "dash",
+        "plotly",
+        "requests",
+        "bs4",
+        "yaml",
+    ],
+)
 def test_can_import(mod):
     importlib.import_module(mod)
would reformat /Users/menon/git/scrap_sam_rework/tests/test_sanity_imports.py
--- /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/scrap_SAM_BETA.py	2025-09-02 02:16:21.650027+00:00
+++ /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/scrap_SAM_BETA.py	2025-09-02 21:57:44.451270+00:00
@@ -6,13 +6,19 @@
 from selenium.webdriver.firefox.service import Service
 from selenium.webdriver.common.by import By
 from selenium.webdriver.support.ui import WebDriverWait
 from selenium.webdriver.support import expected_conditions as EC
 from selenium.webdriver.firefox.options import Options
-from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException, ElementClickInterceptedException
+from selenium.common.exceptions import (
+    WebDriverException,
+    TimeoutException,
+    NoSuchElementException,
+    ElementClickInterceptedException,
+)
 import time
 import pandas as pd
+
 
 def download_geckodriver(target_path):
     base_url = "https://github.com/mozilla/geckodriver/releases/download/v0.33.0/"
     if sys.platform.startswith("win"):
         filename = "geckodriver-v0.33.0-win64.zip"
@@ -24,43 +30,57 @@
         raise OSError("Sistema operacional não suportado")
 
     url = base_url + filename
     response = requests.get(url)
     zip_path = os.path.join(target_path, filename)
-    
-    with open(zip_path, 'wb') as f:
+
+    with open(zip_path, "wb") as f:
         f.write(response.content)
-    
+
     if filename.endswith(".zip"):
-        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
+        with zipfile.ZipFile(zip_path, "r") as zip_ref:
             zip_ref.extractall(target_path)
     else:
         import tarfile
+
         with tarfile.open(zip_path, "r:gz") as tar:
             tar.extractall(path=target_path)
-    
+
     os.remove(zip_path)
-    
+
     if sys.platform.startswith("win"):
         return os.path.join(target_path, "geckodriver.exe")
     else:
         return os.path.join(target_path, "geckodriver")
 
+
 def find_or_download_geckodriver():
     possible_paths = [
         "geckodriver.exe" if sys.platform.startswith("win") else "geckodriver",
-        os.path.join(os.path.dirname(os.path.abspath(__file__)), "geckodriver.exe" if sys.platform.startswith("win") else "geckodriver"),
-        os.path.join(os.path.dirname(os.path.abspath(__file__)), "drivers", "geckodriver.exe" if sys.platform.startswith("win") else "geckodriver"),
-        "C:\\WebDriver\\bin\\geckodriver.exe" if sys.platform.startswith("win") else "/usr/local/bin/geckodriver",
+        os.path.join(
+            os.path.dirname(os.path.abspath(__file__)),
+            "geckodriver.exe" if sys.platform.startswith("win") else "geckodriver",
+        ),
+        os.path.join(
+            os.path.dirname(os.path.abspath(__file__)),
+            "drivers",
+            "geckodriver.exe" if sys.platform.startswith("win") else "geckodriver",
+        ),
+        (
+            "C:\\WebDriver\\bin\\geckodriver.exe"
+            if sys.platform.startswith("win")
+            else "/usr/local/bin/geckodriver"
+        ),
     ]
-    
+
     for path in possible_paths:
         if os.path.isfile(path):
             return path
-    
+
     print("GeckoDriver não encontrado. Baixando...")
     return download_geckodriver(os.path.dirname(os.path.abspath(__file__)))
+
 
 # Configuração de caminhos
 base_path = os.path.dirname(os.path.abspath(__file__))
 download_path = os.path.join(base_path, "downloads")
 
@@ -73,103 +93,146 @@
 # Configuração do Firefox
 options = Options()
 options.set_preference("browser.download.folderList", 2)
 options.set_preference("browser.download.manager.showWhenStarting", False)
 options.set_preference("browser.download.dir", download_path)
-options.set_preference("browser.helperApps.neverAsk.saveToDisk", "application/vnd.ms-excel, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet, application/octet-stream")
+options.set_preference(
+    "browser.helperApps.neverAsk.saveToDisk",
+    "application/vnd.ms-excel, application/vnd.openxmlformats-officedocument.spreadsheetml.sheet, application/octet-stream",
+)
 options.set_preference("pdfjs.disabled", True)
+
 
 # Função para tentar uma ação com retry
 def retry_action(action, max_attempts=5, delay=5):
     for attempt in range(max_attempts):
         try:
             return action()
-        except (TimeoutException, NoSuchElementException, ElementClickInterceptedException) as e:
+        except (
+            TimeoutException,
+            NoSuchElementException,
+            ElementClickInterceptedException,
+        ) as e:
             if attempt == max_attempts - 1:
                 raise e
-            print(f"Tentativa {attempt + 1} falhou. Tentando novamente em {delay} segundos...")
+            print(
+                f"Tentativa {attempt + 1} falhou. Tentando novamente em {delay} segundos..."
+            )
             time.sleep(delay)
+
 
 # Inicializar o driver
 try:
-    service = Service(gecko_driver_path, log_output=os.path.devnull)  # Suprime o aviso de log
+    service = Service(
+        gecko_driver_path, log_output=os.path.devnull
+    )  # Suprime o aviso de log
     driver = webdriver.Firefox(service=service, options=options)
 except WebDriverException as e:
     print(f"Erro ao inicializar o WebDriver: {e}")
     print("Verifique se o Firefox está instalado corretamente.")
     sys.exit(1)
 
 try:
     # Acessa a página
     driver.get("https://apps.itaipu.gov.br/SAM_SMA_Reports/SSAsExecuted.aspx")
-    
+
     # Aguarda o usuário fazer login manualmente
     input("Por favor, faça login manualmente e pressione Enter quando concluir...")
 
     # Espera e preenche o campo Setor Emissor
     def fill_setor_emissor():
         setor_emissor = WebDriverWait(driver, 30).until(
-            EC.element_to_be_clickable((By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtSSADashboardFilter_SectorExecutor"))
+            EC.element_to_be_clickable(
+                (
+                    By.ID,
+                    "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtSSADashboardFilter_SectorExecutor",
+                )
+            )
         )
         setor_emissor.clear()
         setor_emissor.send_keys("IEE3")
         print("Campo Setor Emissor preenchido com sucesso.")
 
     retry_action(fill_setor_emissor)
-    
+
     # Preenche os campos de Data de Execução
     def fill_datas():
         data_execucao_inicial = WebDriverWait(driver, 30).until(
-            EC.element_to_be_clickable((By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtPlanningYearWeekStart_input2"))
+            EC.element_to_be_clickable(
+                (
+                    By.ID,
+                    "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtPlanningYearWeekStart_input2",
+                )
+            )
         )
         data_execucao_inicial.clear()
         data_execucao_inicial.send_keys("202401")
-        
+
         data_execucao_final = WebDriverWait(driver, 30).until(
-            EC.element_to_be_clickable((By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtPlanningYearWeekEnd_input2"))
+            EC.element_to_be_clickable(
+                (
+                    By.ID,
+                    "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMainContent_wtMainContent_SAM_SMA_CW_wt107_block_wtPlanningYearWeekEnd_input2",
+                )
+            )
         )
         data_execucao_final.clear()
         data_execucao_final.send_keys("202426")
         print("Campos de data preenchidos com sucesso.")
 
     retry_action(fill_datas)
-    
+
     # Clica no botão de procurar
     def click_procurar():
         try:
             # Tenta encontrar o botão de várias maneiras
             procurar_button = WebDriverWait(driver, 60).until(
-                EC.presence_of_element_located((By.XPATH, "//button[contains(@class, 'btn-primary') and contains(., 'Procurar')]"))
+                EC.presence_of_element_located(
+                    (
+                        By.XPATH,
+                        "//button[contains(@class, 'btn-primary') and contains(., 'Procurar')]",
+                    )
+                )
             )
             print("Botão 'Procurar' encontrado.")
 
             # Verifica se há algum overlay bloqueando o botão
-            overlay = driver.find_elements(By.XPATH, "//div[contains(@class, 'modal-backdrop') or contains(@class, 'loading-overlay')]")
+            overlay = driver.find_elements(
+                By.XPATH,
+                "//div[contains(@class, 'modal-backdrop') or contains(@class, 'loading-overlay')]",
+            )
             if overlay:
                 print("Overlay detectado. Aguardando sua remoção...")
                 WebDriverWait(driver, 30).until(EC.invisibility_of_element(overlay[0]))
 
             # Tenta clicar no botão de várias maneiras
             try:
                 procurar_button.click()
             except:
-                driver.execute_script("arguments[0].scrollIntoView(true);", procurar_button)
+                driver.execute_script(
+                    "arguments[0].scrollIntoView(true);", procurar_button
+                )
                 driver.execute_script("arguments[0].click();", procurar_button)
 
             print("Botão de procurar clicado com sucesso.")
 
             # Aguarda o carregamento dos resultados
             WebDriverWait(driver, 60).until(
-                EC.presence_of_element_located((By.XPATH, "//table[contains(@class, 'table-generic')]"))
+                EC.presence_of_element_located(
+                    (By.XPATH, "//table[contains(@class, 'table-generic')]")
+                )
             )
             print("Resultados carregados com sucesso.")
         except Exception as e:
             print(f"Erro ao clicar no botão Procurar: {e}")
             print("Tentando método alternativo...")
-            
+
             # Método alternativo: usar JavaScript para clicar em todos os botões visíveis
-            buttons = driver.find_elements(By.XPATH, "//button[not(contains(@style,'display:none')) and not(contains(@style,'display: none'))]")
+            buttons = driver.find_elements(
+                By.XPATH,
+                "//button[not(contains(@style,'display:none')) and not(contains(@style,'display: none'))]",
+            )
             for button in buttons:
                 try:
                     driver.execute_script("arguments[0].click();", button)
                     print(f"Clicado no botão: {button.text}")
                     time.sleep(2)  # Espera curta após cada clique
@@ -177,41 +240,55 @@
                     pass
 
             # Verifica novamente se os resultados foram carregados
             try:
                 WebDriverWait(driver, 60).until(
-                    EC.presence_of_element_located((By.XPATH, "//table[contains(@class, 'table-generic')]"))
+                    EC.presence_of_element_located(
+                        (By.XPATH, "//table[contains(@class, 'table-generic')]")
+                    )
                 )
                 print("Resultados carregados com sucesso após método alternativo.")
             except:
-                print("Não foi possível carregar os resultados mesmo após o método alternativo.")
+                print(
+                    "Não foi possível carregar os resultados mesmo após o método alternativo."
+                )
                 raise
 
     retry_action(click_procurar)
-    
+
     # Espera e clica nos três pontinhos para exportar
     def click_menu():
         menu_button = WebDriverWait(driver, 60).until(
-            EC.element_to_be_clickable((By.XPATH, "//i[@class='iguazu-ico iguazu-ico-more3 iguazu-ico-size-double']"))
+            EC.element_to_be_clickable(
+                (
+                    By.XPATH,
+                    "//i[@class='iguazu-ico iguazu-ico-more3 iguazu-ico-size-double']",
+                )
+            )
         )
         driver.execute_script("arguments[0].scrollIntoView();", menu_button)
         driver.execute_script("arguments[0].click();", menu_button)
         print("Menu de opções aberto com sucesso.")
 
     retry_action(click_menu)
-    
+
     # Clica na opção de exportar para Excel
     def click_export():
         export_button = WebDriverWait(driver, 30).until(
-            EC.element_to_be_clickable((By.ID, "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMenuDropdown_wtConditionalMenu_IguazuTheme_wt54_block_OutSystemsUIWeb_wt6_block_wtDropdownList_wtDropdownList_wtLink_ExportToExcel"))
+            EC.element_to_be_clickable(
+                (
+                    By.ID,
+                    "SAMTemplateAssets_wt14_block_IguazuTheme_wt30_block_wtMenuDropdown_wtConditionalMenu_IguazuTheme_wt54_block_OutSystemsUIWeb_wt6_block_wtDropdownList_wtDropdownList_wtLink_ExportToExcel",
+                )
+            )
         )
         driver.execute_script("arguments[0].scrollIntoView();", export_button)
         driver.execute_script("arguments[0].click();", export_button)
         print("Opção de exportar para Excel clicada com sucesso.")
 
     retry_action(click_export)
-    
+
     # Espera o download do arquivo Excel
     print("Aguardando o download do arquivo Excel...")
     time.sleep(30)  # Aumentado para 30 segundos
 
 except Exception as e:
@@ -230,6 +307,6 @@
 if os.path.exists(excel_path):
     df = pd.read_excel(excel_path)
     print(df.head())
 else:
     print(f"O arquivo {excel_path} não foi encontrado.")
-    print("Verifique se o download foi concluído com sucesso.")
\ No newline at end of file
+    print("Verifique se o download foi concluído com sucesso.")
would reformat /Users/menon/git/scrap_sam_rework/src/scrapers/legacy/scrap_SAM_BETA.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Scrap-Playwright_otimizado_tratamento_de_erro_rede.py	2025-09-02 02:16:21.504330+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Scrap-Playwright_otimizado_tratamento_de_erro_rede.py	2025-09-02 21:57:44.462467+00:00
@@ -24,10 +24,11 @@
     status: int
     method: str
     error_type: str
     details: str
     severity: ErrorSeverity
+
 
 @dataclass
 class ConsoleError:
     timestamp: str
     type: str
@@ -319,21 +320,25 @@
         "execucao": "input[id*='ctl06'][id*='wtContent']",
         "derivadas": "input[id*='ctl10'][id*='wtContent']",
         "apr": "input[id*='ctl12'][id*='wtContent']",
     }
 
+
 class SAMNavigator:
     def __init__(self, page: Page):
         self.page = page
         self.locators = SAMLocators()
         self.download_path = os.path.join(os.getcwd(), "Downloads")
         os.makedirs(self.download_path, exist_ok=True)
         self.error_tracker = ErrorTracker(page)
 
     def _safe_action(
-        self, action_fn, error_msg: str, screenshot_name: Optional[str] = None,
-        retry_count: int = 3
+        self,
+        action_fn,
+        error_msg: str,
+        screenshot_name: Optional[str] = None,
+        retry_count: int = 3,
     ):
         """Wrapper para executar ações com tratamento de erro padronizado."""
         for attempt in range(retry_count):
             try:
                 return action_fn()
@@ -346,11 +351,11 @@
 
                 if attempt == retry_count - 1:
                     raise
 
                 # Espera exponencial entre tentativas
-                wait_time = (2 ** attempt) * 1000
+                wait_time = (2**attempt) * 1000
                 self.page.wait_for_timeout(wait_time)
 
     def login(self, username: str, password: str):
         def _do_login():
             self.page.goto("https://apps.itaipu.gov.br/SAM/NoPermission.aspx")
@@ -370,10 +375,11 @@
 
         self._safe_action(_do_navigation, "Erro na navegação", "navigation_error")
 
     def wait_for_filter_field(self):
         """Aguarda o campo 'Setor Executor' com retry."""
+
         def _wait_for_field():
             self.page.wait_for_selector(
                 self.locators.FILTER["setor_executor"],
                 state="visible",
                 timeout=20000,
@@ -382,11 +388,11 @@
             return True
 
         return self._safe_action(
             _wait_for_field,
             "Erro ao localizar campo 'Setor Executor'",
-            "filter_field_error"
+            "filter_field_error",
         )
 
     def fill_filter(self, executor_setor_value: str):
         def _do_fill():
             input_selector = self.locators.FILTER["setor_executor"]
@@ -753,12 +759,12 @@
                     download = download_promise.value
                     download_file_path = os.path.join(
                         self.download_path, download.suggested_filename
                     )
                     download.save_as(download_file_path)
-                    self.error_tracker.download_end_time = datetime.now()  
-                    self.error_tracker.last_download_path = download_file_path  
+                    self.error_tracker.download_end_time = datetime.now()
+                    self.error_tracker.last_download_path = download_file_path
 
                     print(f"Download concluído: {download_file_path}")
                     return True
 
             except Exception as click_e:
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Scrap-Playwright_otimizado_tratamento_de_erro_rede.py
--- /Users/menon/git/scrap_sam_rework/src/scrapers/Scrap-Playwright_otimizado_tratamento_de_erro_rede.py	2025-09-02 02:16:21.646899+00:00
+++ /Users/menon/git/scrap_sam_rework/src/scrapers/Scrap-Playwright_otimizado_tratamento_de_erro_rede.py	2025-09-02 21:57:44.548432+00:00
@@ -24,10 +24,11 @@
     status: int
     method: str
     error_type: str
     details: str
     severity: ErrorSeverity
+
 
 @dataclass
 class ConsoleError:
     timestamp: str
     type: str
@@ -319,21 +320,25 @@
         "execucao": "input[id*='ctl06'][id*='wtContent']",
         "derivadas": "input[id*='ctl10'][id*='wtContent']",
         "apr": "input[id*='ctl12'][id*='wtContent']",
     }
 
+
 class SAMNavigator:
     def __init__(self, page: Page):
         self.page = page
         self.locators = SAMLocators()
         self.download_path = os.path.join(os.getcwd(), "Downloads")
         os.makedirs(self.download_path, exist_ok=True)
         self.error_tracker = ErrorTracker(page)
 
     def _safe_action(
-        self, action_fn, error_msg: str, screenshot_name: Optional[str] = None,
-        retry_count: int = 3
+        self,
+        action_fn,
+        error_msg: str,
+        screenshot_name: Optional[str] = None,
+        retry_count: int = 3,
     ):
         """Wrapper para executar ações com tratamento de erro padronizado."""
         for attempt in range(retry_count):
             try:
                 return action_fn()
@@ -346,11 +351,11 @@
 
                 if attempt == retry_count - 1:
                     raise
 
                 # Espera exponencial entre tentativas
-                wait_time = (2 ** attempt) * 1000
+                wait_time = (2**attempt) * 1000
                 self.page.wait_for_timeout(wait_time)
 
     def login(self, username: str, password: str):
         def _do_login():
             self.page.goto("https://apps.itaipu.gov.br/SAM/NoPermission.aspx")
@@ -370,10 +375,11 @@
 
         self._safe_action(_do_navigation, "Erro na navegação", "navigation_error")
 
     def wait_for_filter_field(self):
         """Aguarda o campo 'Setor Executor' com retry."""
+
         def _wait_for_field():
             self.page.wait_for_selector(
                 self.locators.FILTER["setor_executor"],
                 state="visible",
                 timeout=20000,
@@ -382,11 +388,11 @@
             return True
 
         return self._safe_action(
             _wait_for_field,
             "Erro ao localizar campo 'Setor Executor'",
-            "filter_field_error"
+            "filter_field_error",
         )
 
     def fill_filter(self, executor_setor_value: str):
         def _do_fill():
             input_selector = self.locators.FILTER["setor_executor"]
@@ -753,12 +759,12 @@
                     download = download_promise.value
                     download_file_path = os.path.join(
                         self.download_path, download.suggested_filename
                     )
                     download.save_as(download_file_path)
-                    self.error_tracker.download_end_time = datetime.now()  
-                    self.error_tracker.last_download_path = download_file_path  
+                    self.error_tracker.download_end_time = datetime.now()
+                    self.error_tracker.last_download_path = download_file_path
 
                     print(f"Download concluído: {download_file_path}")
                     return True
 
             except Exception as click_e:
would reformat /Users/menon/git/scrap_sam_rework/src/scrapers/Scrap-Playwright_otimizado_tratamento_de_erro_rede.py
--- /Users/menon/git/scrap_sam_rework/src/scrapers/scrap_sam_main.py	2025-09-02 02:26:32.772116+00:00
+++ /Users/menon/git/scrap_sam_rework/src/scrapers/scrap_sam_main.py	2025-09-02 21:57:44.670730+00:00
@@ -24,10 +24,11 @@
     status: int
     method: str
     error_type: str
     details: str
     severity: ErrorSeverity
+
 
 @dataclass
 class ConsoleError:
     timestamp: str
     type: str
@@ -319,21 +320,25 @@
         "execucao": "input[id*='ctl06'][id*='wtContent']",
         "derivadas": "input[id*='ctl10'][id*='wtContent']",
         "apr": "input[id*='ctl12'][id*='wtContent']",
     }
 
+
 class SAMNavigator:
     def __init__(self, page: Page):
         self.page = page
         self.locators = SAMLocators()
         self.download_path = os.path.join(os.getcwd(), "Downloads")
         os.makedirs(self.download_path, exist_ok=True)
         self.error_tracker = ErrorTracker(page)
 
     def _safe_action(
-        self, action_fn, error_msg: str, screenshot_name: Optional[str] = None,
-        retry_count: int = 3
+        self,
+        action_fn,
+        error_msg: str,
+        screenshot_name: Optional[str] = None,
+        retry_count: int = 3,
     ):
         """Wrapper para executar ações com tratamento de erro padronizado."""
         for attempt in range(retry_count):
             try:
                 return action_fn()
@@ -346,11 +351,11 @@
 
                 if attempt == retry_count - 1:
                     raise
 
                 # Espera exponencial entre tentativas
-                wait_time = (2 ** attempt) * 1000
+                wait_time = (2**attempt) * 1000
                 self.page.wait_for_timeout(wait_time)
 
     def login(self, username: str, password: str):
         def _do_login():
             self.page.goto("https://apps.itaipu.gov.br/SAM/NoPermission.aspx")
@@ -370,10 +375,11 @@
 
         self._safe_action(_do_navigation, "Erro na navegação", "navigation_error")
 
     def wait_for_filter_field(self):
         """Aguarda o campo 'Setor Executor' com retry."""
+
         def _wait_for_field():
             self.page.wait_for_selector(
                 self.locators.FILTER["setor_executor"],
                 state="visible",
                 timeout=20000,
@@ -382,11 +388,11 @@
             return True
 
         return self._safe_action(
             _wait_for_field,
             "Erro ao localizar campo 'Setor Executor'",
-            "filter_field_error"
+            "filter_field_error",
         )
 
     def fill_filter(self, executor_setor_value: str):
         def _do_fill():
             input_selector = self.locators.FILTER["setor_executor"]
@@ -753,12 +759,12 @@
                     download = download_promise.value
                     download_file_path = os.path.join(
                         self.download_path, download.suggested_filename
                     )
                     download.save_as(download_file_path)
-                    self.error_tracker.download_end_time = datetime.now()  
-                    self.error_tracker.last_download_path = download_file_path  
+                    self.error_tracker.download_end_time = datetime.now()
+                    self.error_tracker.last_download_path = download_file_path
 
                     print(f"Download concluído: {download_file_path}")
                     return True
 
             except Exception as click_e:
would reformat /Users/menon/git/scrap_sam_rework/src/scrapers/scrap_sam_main.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/dashboard/ssa_dashboard.py	2025-09-02 02:16:21.528887+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/dashboard/ssa_dashboard.py	2025-09-02 21:57:44.791882+00:00
@@ -1553,11 +1553,10 @@
             ],
             fluid=True,
             className="p-4",
         )
 
-
     def setup_callbacks(self):
         """
         Configure todos os callbacks do dashboard com recursos aprimorados.
         Gerencia atualizações de gráficos, interações modais e atualização de dados.
         """
@@ -1583,17 +1582,17 @@
             ],
         )
         def update_all_charts(resp_prog, resp_exec, setor_emissor, setor_executor):
             """
             Updates all dashboard components based on filter selections.
-            
+
             Args:
                 resp_prog (str): Selected programming responsible
                 resp_exec (str): Selected execution responsible
                 setor_emissor (str): Selected issuing sector
                 setor_executor (str): Selected executing sector
-            
+
             Returns:
                 tuple: Updated values for all dashboard components
             """
             try:
                 # Log filter applications
@@ -1608,15 +1607,17 @@
                 df_filtered = self.df.copy()
 
                 # Apply filters safely with proper error handling
                 if resp_prog:
                     df_filtered = df_filtered[
-                        df_filtered.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO] == resp_prog
+                        df_filtered.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO]
+                        == resp_prog
                     ]
                 if resp_exec:
                     df_filtered = df_filtered[
-                        df_filtered.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO] == resp_exec
+                        df_filtered.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO]
+                        == resp_exec
                     ]
                 if setor_emissor:
                     df_filtered = df_filtered[
                         df_filtered.iloc[:, SSAColumns.SETOR_EMISSOR] == setor_emissor
                     ]
@@ -1642,12 +1643,16 @@
                 fig_exec = self._enhance_bar_chart(
                     fig_exec, "resp_exec", "SSAs por Executor", df_filtered
                 )
 
                 # Create week charts with proper data handling
-                fig_programmed_week = filtered_visualizer.create_week_chart(use_programmed=True)
-                fig_registration_week = filtered_visualizer.create_week_chart(use_programmed=False)
+                fig_programmed_week = filtered_visualizer.create_week_chart(
+                    use_programmed=True
+                )
+                fig_registration_week = filtered_visualizer.create_week_chart(
+                    use_programmed=False
+                )
 
                 # Enhance week charts with interactive features
                 fig_programmed_week = self._enhance_bar_chart(
                     fig_programmed_week,
                     "week_programmed",
@@ -1660,11 +1665,13 @@
                     "SSAs Cadastradas",
                     df_filtered,
                 )
 
                 # Update detail section visibility
-                detail_style = {"display": "block"}  # Always show details after filter application
+                detail_style = {
+                    "display": "block"
+                }  # Always show details after filter application
 
                 # Create and enhance detail charts
                 fig_detail_state = self._enhance_bar_chart(
                     self._create_detail_state_chart(df_filtered),
                     "state",
@@ -1732,10 +1739,11 @@
                     empty_fig,
                     empty_fig,
                     [],
                     empty_fig,
                 )
+
         @self.app.callback(
             [
                 Output("ssa-modal", "is_open"),
                 Output("ssa-modal-body", "children"),
                 Output("ssa-modal-title", "children"),
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Class/src/dashboard/ssa_dashboard.py
--- /Users/menon/git/scrap_sam_rework/src/dashboard/Report_from_excel.py	2025-09-02 02:16:21.494452+00:00
+++ /Users/menon/git/scrap_sam_rework/src/dashboard/Report_from_excel.py	2025-09-02 21:57:45.409772+00:00
@@ -2,11 +2,11 @@
 from datetime import datetime, date
 import pandas as pd
 import numpy as np
 import plotly.express as px
 import plotly.graph_objects as go
-from dash import Dash, dcc, html, Input, Output,State, MATCH, ALL, dash_table  
+from dash import Dash, dcc, html, Input, Output, State, MATCH, ALL, dash_table
 import dash_bootstrap_components as dbc
 import logging
 from dash import dash_table
 from typing import Union, Tuple
 from typing import Dict, List, Optional
@@ -25,13 +25,15 @@
 warnings.filterwarnings("ignore")
 
 # Configuração do caminho do arquivo no início do script
 DATA_FILE_PATH = r"C:\Users\menon\git\trabalho\SCRAP-SAM\DashboardSM\Downloads\SSAs Pendentes Geral - 11-11-2024_0904AM.xlsx"
 
+
 @dataclass
 class SSAData:
     """Estrutura de dados para uma SSA."""
+
     numero: str
     situacao: str
     derivada: Optional[str]
     localizacao: str
     desc_localizacao: str
@@ -58,11 +60,15 @@
         return {
             "numero": self.numero,
             "situacao": self.situacao,
             "setor_executor": self.setor_executor,
             "prioridade": self.prioridade_emissao,
-            "emitida_em": self.emitida_em.strftime("%Y-%m-%d %H:%M:%S") if self.emitida_em else None,
+            "emitida_em": (
+                self.emitida_em.strftime("%Y-%m-%d %H:%M:%S")
+                if self.emitida_em
+                else None
+            ),
         }
 
 
 class DataLoader:
     """Carrega e prepara os dados das SSAs."""
@@ -107,16 +113,17 @@
 
         except Exception as e:
             log_issue(f"Erro ao processar data: {str(e)}")
             return None
 
-
     def _convert_dates(self):
         """Converte e valida datas mantendo o tipo apropriado."""
         try:
             # Se a coluna já for datetime, não precisa converter
-            if pd.api.types.is_datetime64_any_dtype(self.df.iloc[:, SSAColumns.EMITIDA_EM]):
+            if pd.api.types.is_datetime64_any_dtype(
+                self.df.iloc[:, SSAColumns.EMITIDA_EM]
+            ):
                 logging.info("Coluna já está em formato datetime")
                 return
 
             # Converte diretamente para datetime usando o formato correto
             self.df.iloc[:, SSAColumns.EMITIDA_EM] = pd.to_datetime(
@@ -149,20 +156,20 @@
                 header=1,  # Cabeçalho na segunda linha
             )
 
             # NOVO: Diagnóstico de datas antes da conversão
             date_diagnosis = diagnose_dates(self.df, SSAColumns.EMITIDA_EM)
-            if date_diagnosis['error_count'] > 0:
+            if date_diagnosis["error_count"] > 0:
                 logging.info("=== Diagnóstico de Datas ===")
                 logging.info(f"Total de linhas: {date_diagnosis['total_rows']}")
                 logging.info(f"Problemas encontrados: {date_diagnosis['error_count']}")
-                for prob in date_diagnosis['problematic_rows']:
+                for prob in date_diagnosis["problematic_rows"]:
                     logging.info(f"\nLinha {prob['index'] + 1}:")
                     logging.info(f"  Valor encontrado: {prob['value']}")
                     logging.info(f"  Motivo: {prob['reason']}")
                     logging.info("  Dados da linha:")
-                    for key, value in prob['row_data'].items():
+                    for key, value in prob["row_data"].items():
                         logging.info(f"    {key}: {value}")
 
             # Converte as datas usando o novo método
             self._convert_dates()
 
@@ -194,11 +201,13 @@
                 except Exception as e:
                     logging.error(f"Erro ao converter coluna {col}: {str(e)}")
 
             # Padroniza prioridades para maiúsculas
             self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] = (
-                self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].str.upper().str.strip()
+                self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO]
+                .str.upper()
+                .str.strip()
             )
 
             # Converte colunas opcionais
             optional_string_columns = [
                 SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO,
@@ -419,10 +428,11 @@
         return self.ssa_objects
 
 
 class SSAColumns:
     """Mantém os índices e nomes das colunas."""
+
     # Índices
     NUMERO_SSA = 0
     SITUACAO = 1
     DERIVADA = 2
     LOCALIZACAO = 3
@@ -513,11 +523,11 @@
         return f"{self.year}{self.week:02d}"
 
 
 class WeekCalculator:
     """Handles ISO week-based calculations with year transition awareness."""
-    
+
     @staticmethod
     def get_iso_calendar(dt: Union[date, datetime]) -> Tuple[int, int, int]:
         """
         Gets ISO calendar information safely.
         Returns tuple of (year, week, weekday).
@@ -538,56 +548,55 @@
 
     @staticmethod
     def get_last_week_of_year(year: int) -> int:
         """
         Determines if a year has 52 or 53 ISO weeks.
-        
+
         Args:
             year: The year to check
-            
+
         Returns:
             Number of weeks (52 or 53)
         """
         # December 28th is always in the last week of the ISO year
         dec_28 = date(year, 12, 28)
         _, last_week, _ = dec_28.isocalendar()
         return last_week
 
     @classmethod
     def calculate_week_difference(
-        cls,
-        week1: Optional[WeekInfo],
-        week2: Optional[WeekInfo]
+        cls, week1: Optional[WeekInfo], week2: Optional[WeekInfo]
     ) -> Optional[int]:
         """
         Calculates the difference between two ISO weeks, handling year transitions.
-        
+
         Args:
             week1: First WeekInfo object
             week2: Second WeekInfo object
-            
+
         Returns:
             Number of weeks difference or None if invalid input
         """
         if not week1 or not week2:
             return None
-            
+
         if week1.year == week2.year:
             return week2.week - week1.week
-            
+
         # Handle year transitions
         total_weeks = 0
-        
+
         # Add weeks for complete years between
         for year in range(week1.year, week2.year):
             total_weeks += cls.get_last_week_of_year(year)
-            
+
         # Adjust for partial weeks in start and end year
         total_weeks -= week1.week
         total_weeks += week2.week
-        
+
         return total_weeks
+
 
 class SSAWeekAnalyzer:
     """Analyzes SSA data with respect to weeks, following ISO standard."""
 
     def __init__(self, df: pd.DataFrame):
@@ -1202,37 +1211,37 @@
         weeks_in_state = self.week_analyzer.calculate_weeks_in_state()
         valid_weeks = weeks_in_state.dropna()
 
         if valid_weeks.empty:
             return go.Figure().update_layout(
-                    self._get_standard_layout(
-                        title="Distribuição de SSAs por Tempo no Estado Atual",
-                        xaxis_title="Semanas no Estado",
-                        yaxis_title="Quantidade de SSAs",
-                        chart_type="bar",
-                        annotations=[
-                            {
-                                "text": "Não há dados válidos disponíveis",
-                                "xref": "paper",
-                                "yref": "paper",
-                                "showarrow": False,
-                                "font": {"size": 14},
-                            }
-                        ],
-                    )
+                self._get_standard_layout(
+                    title="Distribuição de SSAs por Tempo no Estado Atual",
+                    xaxis_title="Semanas no Estado",
+                    yaxis_title="Quantidade de SSAs",
+                    chart_type="bar",
+                    annotations=[
+                        {
+                            "text": "Não há dados válidos disponíveis",
+                            "xref": "paper",
+                            "yref": "paper",
+                            "showarrow": False,
+                            "font": {"size": 14},
+                        }
+                    ],
                 )
+            )
 
         # Agrupa em intervalos de semanas
         value_counts = valid_weeks.value_counts().sort_index()
         max_weeks = value_counts.index.max()
 
         if max_weeks > 50:
             bins = list(range(0, int(max_weeks) + 10, 10))
             labels = [f"{bins[i]}-{bins[i+1]-1}" for i in range(len(bins) - 1)]
             binned_data = pd.cut(
-                    value_counts.index, bins=bins, labels=labels, right=False
-                )
+                value_counts.index, bins=bins, labels=labels, right=False
+            )
             value_counts = value_counts.groupby(binned_data).sum()
 
         # Criar dicionário para armazenar SSAs por intervalo
         ssas_by_interval = {}
         hover_text = []
@@ -1252,63 +1261,63 @@
             ssa_preview = "<br>".join(ssas_in_interval[:5])
             if len(ssas_in_interval) > 5:
                 ssa_preview += f"<br>... (+{len(ssas_in_interval)-5} SSAs)"
 
             hover_text.append(
-                    f"<b>Intervalo:</b> {interval}<br>"
-                    f"<b>Total SSAs:</b> {len(ssas_in_interval)}<br>"
-                    f"<b>Primeiras SSAs:</b><br>{ssa_preview}"
-                )
-
-        fig = go.Figure([
+                f"<b>Intervalo:</b> {interval}<br>"
+                f"<b>Total SSAs:</b> {len(ssas_in_interval)}<br>"
+                f"<b>Primeiras SSAs:</b><br>{ssa_preview}"
+            )
+
+        fig = go.Figure(
+            [
                 go.Bar(
                     x=value_counts.index,
                     y=value_counts.values,
                     text=value_counts.values,
                     textposition="auto",
                     name="SSAs por Semana",
                     marker_color="rgb(64, 83, 177)",
                     hovertext=hover_text,
                     hoverinfo="text",
-                    customdata=[ssas_by_interval[str(interval)] for interval in value_counts.index],
-                    hoverlabel=dict(
-                        bgcolor="white",
-                        font_size=12,
-                        font_family="Arial"
-                    ),
+                    customdata=[
+                        ssas_by_interval[str(interval)]
+                        for interval in value_counts.index
+                    ],
+                    hoverlabel=dict(bgcolor="white", font_size=12, font_family="Arial"),
                 )
-            ])
+            ]
+        )
 
         invalid_count = weeks_in_state.isna().sum()
         total_count = len(weeks_in_state)
 
         fig.update_layout(
-                self._get_standard_layout(
-                    title=f"Distribuição de SSAs por Tempo no Estado Atual<br><sub>({invalid_count}/{total_count} registros inválidos)</sub>",
-                    xaxis_title="Intervalo de Semanas no Estado",
-                    yaxis_title="Quantidade de SSAs",
-                    chart_type="bar",
-                    annotations=[
-                        {
-                            "text": "Clique nas barras para ver detalhes das SSAs",
-                            "xref": "paper",
-                            "yref": "paper",
-                            "x": 0.98,
-                            "y": 0.02,
-                            "showarrow": False,
-                            "font": {"size": 10, "color": "gray"},
-                            "xanchor": "right",
-                        }
-                    ],
-                )
-            )
+            self._get_standard_layout(
+                title=f"Distribuição de SSAs por Tempo no Estado Atual<br><sub>({invalid_count}/{total_count} registros inválidos)</sub>",
+                xaxis_title="Intervalo de Semanas no Estado",
+                yaxis_title="Quantidade de SSAs",
+                chart_type="bar",
+                annotations=[
+                    {
+                        "text": "Clique nas barras para ver detalhes das SSAs",
+                        "xref": "paper",
+                        "yref": "paper",
+                        "x": 0.98,
+                        "y": 0.02,
+                        "showarrow": False,
+                        "font": {"size": 10, "color": "gray"},
+                        "xanchor": "right",
+                    }
+                ],
+            )
+        )
 
         # Configurações adicionais para melhorar a aparência
         fig.update_traces(
-                hovertemplate=None,  # Usar o hover_text customizado
-                hoverlabel_align="left"
-            )
+            hovertemplate=None, hoverlabel_align="left"  # Usar o hover_text customizado
+        )
 
         return fig
 
 
 class SSAReporter:
@@ -1651,23 +1660,25 @@
         return "\n".join(f"- {k}: {v}" for k, v in d.items())
 
 
 class KPICalculator:
     """Calcula KPIs e métricas de performance das SSAs."""
-    
+
     def __init__(self, df: pd.DataFrame):
         self.df = df
 
     def calculate_efficiency_metrics(self) -> Dict:
         """Calcula métricas de eficiência."""
         return {
             "taxa_programacao": len(
                 self.df[self.df.iloc[:, SSAColumns.SEMANA_PROGRAMADA].notna()]
-            ) / len(self.df),
+            )
+            / len(self.df),
             "taxa_execucao_simples": len(
                 self.df[self.df.iloc[:, SSAColumns.EXECUCAO_SIMPLES] == "Sim"]
-            ) / len(self.df),
+            )
+            / len(self.df),
             "distribuicao_prioridade": self.df.iloc[
                 :, SSAColumns.GRAU_PRIORIDADE_EMISSAO
             ]
             .value_counts(normalize=True)
             .to_dict(),
@@ -1682,152 +1693,165 @@
         return round(score * 100, 2)
 
     def calculate_response_times(self) -> Dict[str, float]:
         """Calcula tempos de resposta médios por prioridade."""
         response_times = {}
-        
+
         for priority in self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].unique():
-            mask = (
-                self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == priority
-            )
+            mask = self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == priority
             priority_data = self.df[mask]
-            
+
             if len(priority_data) > 0:
                 # Calcula tempo médio desde emissão até programação
                 mean_time = (
-                    priority_data.iloc[:, SSAColumns.SEMANA_PROGRAMADA].astype(float) -
-                    priority_data.iloc[:, SSAColumns.SEMANA_CADASTRO].astype(float)
+                    priority_data.iloc[:, SSAColumns.SEMANA_PROGRAMADA].astype(float)
+                    - priority_data.iloc[:, SSAColumns.SEMANA_CADASTRO].astype(float)
                 ).mean()
-                
+
                 response_times[priority] = mean_time if not pd.isna(mean_time) else None
-                
+
         return response_times
 
     def calculate_sector_performance(self) -> pd.DataFrame:
         """Calcula performance por setor."""
         sector_metrics = []
-        
+
         for sector in self.df.iloc[:, SSAColumns.SETOR_EXECUTOR].unique():
-            sector_data = self.df[
-                self.df.iloc[:, SSAColumns.SETOR_EXECUTOR] == sector
-            ]
-            
+            sector_data = self.df[self.df.iloc[:, SSAColumns.SETOR_EXECUTOR] == sector]
+
             total_ssas = len(sector_data)
             programmed_ssas = len(
                 sector_data[sector_data.iloc[:, SSAColumns.SEMANA_PROGRAMADA].notna()]
             )
             critical_ssas = len(
                 sector_data[
                     sector_data.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == "S3.7"
                 ]
             )
-            
-            sector_metrics.append({
-                "setor": sector,
-                "total_ssas": total_ssas,
-                "taxa_programacao": (programmed_ssas / total_ssas) if total_ssas > 0 else 0,
-                "ssas_criticas": critical_ssas,
-                "percentual_criticas": (critical_ssas / total_ssas * 100) if total_ssas > 0 else 0
-            })
-        
+
+            sector_metrics.append(
+                {
+                    "setor": sector,
+                    "total_ssas": total_ssas,
+                    "taxa_programacao": (
+                        (programmed_ssas / total_ssas) if total_ssas > 0 else 0
+                    ),
+                    "ssas_criticas": critical_ssas,
+                    "percentual_criticas": (
+                        (critical_ssas / total_ssas * 100) if total_ssas > 0 else 0
+                    ),
+                }
+            )
+
         return pd.DataFrame(sector_metrics)
 
     def calculate_weekly_trends(self) -> pd.DataFrame:
         """Calcula tendências semanais de SSAs."""
         weekly_data = []
-        
+
         for week in sorted(self.df.iloc[:, SSAColumns.SEMANA_CADASTRO].unique()):
-            week_data = self.df[
-                self.df.iloc[:, SSAColumns.SEMANA_CADASTRO] == week
-            ]
-            
+            week_data = self.df[self.df.iloc[:, SSAColumns.SEMANA_CADASTRO] == week]
+
             total_ssas = len(week_data)
             programmed = len(
                 week_data[week_data.iloc[:, SSAColumns.SEMANA_PROGRAMADA].notna()]
             )
             critical = len(
                 week_data[
                     week_data.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == "S3.7"
                 ]
             )
-            
-            weekly_data.append({
-                "semana": week,
-                "total_ssas": total_ssas,
-                "programadas": programmed,
-                "criticas": critical,
-                "taxa_programacao": (programmed / total_ssas) if total_ssas > 0 else 0
-            })
-        
+
+            weekly_data.append(
+                {
+                    "semana": week,
+                    "total_ssas": total_ssas,
+                    "programadas": programmed,
+                    "criticas": critical,
+                    "taxa_programacao": (
+                        (programmed / total_ssas) if total_ssas > 0 else 0
+                    ),
+                }
+            )
+
         return pd.DataFrame(weekly_data)
 
     def get_key_metrics_summary(self) -> Dict:
         """Retorna um resumo das métricas principais."""
         total_ssas = len(self.df)
         metrics = self.calculate_efficiency_metrics()
         health_score = self.get_overall_health_score()
-        
+
         # Calcula média de tempo de resposta para SSAs críticas
         response_times = self.calculate_response_times()
         critical_response = response_times.get("S3.7")
-        
+
         return {
             "total_ssas": total_ssas,
             "health_score": health_score,
             "taxa_programacao": metrics["taxa_programacao"] * 100,
             "taxa_execucao_simples": metrics["taxa_execucao_simples"] * 100,
             "tempo_resposta_criticas": critical_response,
-            "distribuicao_prioridade": metrics["distribuicao_prioridade"]
+            "distribuicao_prioridade": metrics["distribuicao_prioridade"],
         }
 
     def calculate_backlog_metrics(self) -> Dict:
         """Calcula métricas relacionadas ao backlog de SSAs."""
         total_backlog = len(self.df)
-        backlog_by_priority = self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].value_counts().to_dict()
-        backlog_by_sector = self.df.iloc[:, SSAColumns.SETOR_EXECUTOR].value_counts().to_dict()
-        
+        backlog_by_priority = (
+            self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].value_counts().to_dict()
+        )
+        backlog_by_sector = (
+            self.df.iloc[:, SSAColumns.SETOR_EXECUTOR].value_counts().to_dict()
+        )
+
         # Calcula idade média do backlog
         backlog_age = None
         emitida_em = self.df.iloc[:, SSAColumns.EMITIDA_EM]
         if pd.api.types.is_datetime64_any_dtype(emitida_em):
             valid_dates = emitida_em.dropna()
             if not valid_dates.empty:
                 current_date = pd.Timestamp.now()
                 backlog_age = (current_date - valid_dates).mean().days
-        
+
         return {
             "total_backlog": total_backlog,
             "by_priority": backlog_by_priority,
             "by_sector": backlog_by_sector,
-            "average_age_days": backlog_age
+            "average_age_days": backlog_age,
         }
 
     def calculate_risk_metrics(self) -> Dict:
         """Calcula métricas de risco baseadas em prioridade e tempo de espera."""
         risk_metrics = {
-            "high_risk": 0,    # SSAs críticas com mais de 2 semanas
+            "high_risk": 0,  # SSAs críticas com mais de 2 semanas
             "medium_risk": 0,  # SSAs críticas com 1-2 semanas ou normais com >4 semanas
-            "low_risk": 0      # Demais SSAs
+            "low_risk": 0,  # Demais SSAs
         }
-        
+
         weeks_in_state = pd.to_numeric(
-            self.df.iloc[:, SSAColumns.SEMANA_CADASTRO],
-            errors='coerce'
+            self.df.iloc[:, SSAColumns.SEMANA_CADASTRO], errors="coerce"
         )
         current_week = int(date.today().strftime("%Y%W"))
-        
+
         for idx, row in self.df.iterrows():
-            weeks_waiting = current_week - weeks_in_state[idx] if pd.notna(weeks_in_state[idx]) else 0
+            weeks_waiting = (
+                current_week - weeks_in_state[idx]
+                if pd.notna(weeks_in_state[idx])
+                else 0
+            )
             is_critical = row.iloc[SSAColumns.GRAU_PRIORIDADE_EMISSAO] == "S3.7"
-            
+
             if is_critical and weeks_waiting > 2:
                 risk_metrics["high_risk"] += 1
-            elif (is_critical and weeks_waiting > 1) or (not is_critical and weeks_waiting > 4):
+            elif (is_critical and weeks_waiting > 1) or (
+                not is_critical and weeks_waiting > 4
+            ):
                 risk_metrics["medium_risk"] += 1
             else:
                 risk_metrics["low_risk"] += 1
-        
+
         return risk_metrics
 
 
 class SSADashboard:
     """Dashboard interativo para análise de SSAs."""
@@ -1872,11 +1896,13 @@
             [Input("resp-prog-filter", "value"), Input("resp-exec-filter", "value")],
         )
         def update_all_charts(resp_prog, resp_exec):
             """Update all charts with filter data."""
             if resp_prog or resp_exec:
-                self.logger.log_with_ip('INFO', f'Filtros aplicados - Prog: {resp_prog}, Exec: {resp_exec}')
+                self.logger.log_with_ip(
+                    "INFO", f"Filtros aplicados - Prog: {resp_prog}, Exec: {resp_exec}"
+                )
 
             df_filtered = self.df.copy()
             if resp_prog:
                 df_filtered = df_filtered[
                     df_filtered.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO] == resp_prog
@@ -1894,41 +1920,41 @@
 
             # Gerar gráficos com informações de hover e click
             fig_prog = self._enhance_bar_chart(
                 self._create_resp_prog_chart(df_filtered),
                 "resp_prog",
-                "SSAs por Programador"
+                "SSAs por Programador",
             )
             fig_exec = self._enhance_bar_chart(
                 self._create_resp_exec_chart(df_filtered),
                 "resp_exec",
-                "SSAs por Executor"
+                "SSAs por Executor",
             )
 
             # Gráficos de semana com hover e click
             fig_programmed_week = self._enhance_bar_chart(
                 filtered_visualizer.create_week_chart(use_programmed=True),
                 "week_programmed",
-                "SSAs Programadas"
+                "SSAs Programadas",
             )
             fig_registration_week = self._enhance_bar_chart(
                 filtered_visualizer.create_week_chart(use_programmed=False),
                 "week_registration",
-                "SSAs Cadastradas"
-            )
-
-            detail_style = {"display": "block"} if resp_prog or resp_exec else {"display": "none"}
+                "SSAs Cadastradas",
+            )
+
+            detail_style = (
+                {"display": "block"} if resp_prog or resp_exec else {"display": "none"}
+            )
 
             fig_detail_state = self._enhance_bar_chart(
-                self._create_detail_state_chart(df_filtered),
-                "state",
-                "SSAs por Estado"
+                self._create_detail_state_chart(df_filtered), "state", "SSAs por Estado"
             )
             fig_detail_week = self._enhance_bar_chart(
                 filtered_visualizer.create_week_chart(),
                 "week_detail",
-                "SSAs por Semana"
+                "SSAs por Semana",
             )
 
             table_data = self._prepare_table_data(df_filtered)
             weeks_fig = filtered_visualizer.add_weeks_in_state_chart()
 
@@ -1986,14 +2012,20 @@
 
             click_mapping = {
                 "weeks-in-state-chart": (weeks_click, "SSAs no intervalo"),
                 "resp-prog-chart": (prog_click, "SSAs do programador"),
                 "resp-exec-chart": (exec_click, "SSAs do executor"),
-                "programmed-week-chart": (prog_week_click, "SSAs programadas na semana"),
-                "registration-week-chart": (reg_week_click, "SSAs cadastradas na semana"),
+                "programmed-week-chart": (
+                    prog_week_click,
+                    "SSAs programadas na semana",
+                ),
+                "registration-week-chart": (
+                    reg_week_click,
+                    "SSAs cadastradas na semana",
+                ),
                 "detail-state-chart": (detail_state_click, "SSAs no estado"),
-                "detail-week-chart": (detail_week_click, "SSAs na semana (detalhe)")
+                "detail-week-chart": (detail_week_click, "SSAs na semana (detalhe)"),
             }
 
             if trigger_id in click_mapping:
                 click_data, title_prefix = click_mapping[trigger_id]
                 if click_data is None:
@@ -2002,11 +2034,13 @@
                 point_data = click_data["points"][0]
                 label = point_data["x"]
                 ssas = point_data.get("customdata", [])
 
                 if ssas:
-                    self.logger.log_with_ip("INFO", f"Visualização de SSAs: {title_prefix} {label}")
+                    self.logger.log_with_ip(
+                        "INFO", f"Visualização de SSAs: {title_prefix} {label}"
+                    )
 
                 ssa_list = self._create_ssa_list(ssas)
                 title = f"{title_prefix} {label} ({len(ssas)} SSAs)"
 
                 return True, ssa_list, title
@@ -2043,12 +2077,11 @@
             State({"type": "ssa-number", "index": MATCH, "value": ALL}, "value"),
         )
 
         # Callback para atualização automática (a cada 5 minutos)
         @self.app.callback(
-            Output("state-data", "data"),
-            Input("interval-component", "n_intervals")
+            Output("state-data", "data"), Input("interval-component", "n_intervals")
         )
         def update_data(n):
             """Update data periodically."""
             if n:  # Só atualiza após o primeiro intervalo
                 self.logger.log_with_ip("INFO", "Atualização automática dos dados")
@@ -2199,27 +2232,33 @@
                 # Filtros
                 dbc.Row(
                     [
                         dbc.Col(
                             [
-                                html.Label("Responsável Programação:", className="fw-bold"),
+                                html.Label(
+                                    "Responsável Programação:", className="fw-bold"
+                                ),
                                 dcc.Dropdown(
                                     id="resp-prog-filter",
                                     options=[
                                         {"label": resp, "value": resp}
-                                        for resp in self._get_responsaveis()["programacao"]
+                                        for resp in self._get_responsaveis()[
+                                            "programacao"
+                                        ]
                                     ],
                                     placeholder="Selecione um responsável...",
                                     className="mb-2",
                                     clearable=True,
                                 ),
                             ],
                             width=6,
                         ),
                         dbc.Col(
                             [
-                                html.Label("Responsável Execução:", className="fw-bold"),
+                                html.Label(
+                                    "Responsável Execução:", className="fw-bold"
+                                ),
                                 dcc.Dropdown(
                                     id="resp-exec-filter",
                                     options=[
                                         {"label": resp, "value": resp}
                                         for resp in self._get_responsaveis()["execucao"]
@@ -2540,12 +2579,18 @@
                                         dbc.CardBody(
                                             [
                                                 dash_table.DataTable(
                                                     id="ssa-table",
                                                     columns=[
-                                                        {"name": "Número", "id": "numero"},
-                                                        {"name": "Estado", "id": "estado"},
+                                                        {
+                                                            "name": "Número",
+                                                            "id": "numero",
+                                                        },
+                                                        {
+                                                            "name": "Estado",
+                                                            "id": "estado",
+                                                        },
                                                         {
                                                             "name": "Resp. Prog.",
                                                             "id": "resp_prog",
                                                         },
                                                         {
@@ -2652,80 +2697,98 @@
 
     def _create_state_cards(self, state_counts):
         """Cria cards para cada estado de SSA."""
         cards = [
             # Label "Setor:"
-            dbc.Col(
-                html.H6("Setor:", className="mt-3 me-2"),
-                width="auto"
-            )
+            dbc.Col(html.H6("Setor:", className="mt-3 me-2"), width="auto")
         ]
 
         # Calcula o total e adiciona como primeiro card
         total_ssas = sum(state_counts.values())
         cards.append(
             dbc.Col(
                 dbc.Card(
                     [
                         dbc.CardBody(
                             [
-                                html.H6("TOTAL", className="card-title text-center mb-0 small"),
+                                html.H6(
+                                    "TOTAL",
+                                    className="card-title text-center mb-0 small",
+                                ),
                                 html.H3(
-                                    str(total_ssas), 
+                                    str(total_ssas),
                                     className="text-center text-danger fw-bold mb-0",
-                                    style={"fontSize": "1.8rem"}  # Texto um pouco maior
+                                    style={
+                                        "fontSize": "1.8rem"
+                                    },  # Texto um pouco maior
                                 ),
                             ],
-                            className="p-2"  # Menos padding
+                            className="p-2",  # Menos padding
                         )
                     ],
                     className="mb-3",
                     style={
                         "height": "80px",
                         "border-left": "4px solid red",
-                        "width": "150px"  # Largura um pouco maior
+                        "width": "150px",  # Largura um pouco maior
                     },
                 ),
-                width="auto"
+                width="auto",
             )
         )
 
         # Lista ordenada de estados
-        state_list = ['AAD', 'ADM', 'AAT', 'SPG', 'AIM', 'APV', 'APG', 'SCD', 'ADI', 'APL']
+        state_list = [
+            "AAD",
+            "ADM",
+            "AAT",
+            "SPG",
+            "AIM",
+            "APV",
+            "APG",
+            "SCD",
+            "ADI",
+            "APL",
+        ]
 
         # Adiciona os cards de estado na ordem definida
         for state in state_list:
             count = state_counts.get(state, 0)
             cards.append(
                 dbc.Col(
                     dbc.Card(
                         [
                             dbc.CardBody(
                                 [
-                                    html.H6(state, className="card-title text-center mb-0 small"),
+                                    html.H6(
+                                        state,
+                                        className="card-title text-center mb-0 small",
+                                    ),
                                     html.H3(
-                                        str(count), 
+                                        str(count),
                                         className="text-center text-primary mb-0",
-                                        style={"fontSize": "1.8rem"}  # Texto um pouco maior
+                                        style={
+                                            "fontSize": "1.8rem"
+                                        },  # Texto um pouco maior
                                     ),
                                 ],
-                                className="p-2"  # Menos padding
+                                className="p-2",  # Menos padding
                             )
                         ],
                         className="mb-3",
                         style={
                             "height": "80px",
-                            "width": "150px"  # Largura um pouco maior
+                            "width": "150px",  # Largura um pouco maior
                         },
                     ),
-                    width="auto"
+                    width="auto",
                 )
             )
 
         return dbc.Row(
             cards,
-            className="g-2 flex-nowrap align-items-center"  # Alinhamento vertical centralizado
+            className="g-2 flex-nowrap align-items-center",  # Alinhamento vertical centralizado
         )
 
     def _create_resp_summary_cards(self, df_filtered):
         """
         Cria cards de resumo para o usuário filtrado, mostrando estatísticas por estado.
@@ -2802,11 +2865,15 @@
                             ],
                             className="p-2",
                         )
                     ],
                     className="mb-0 border-danger",
-                    style={"width": "120px", "height": "65px", "borderLeft": "4px solid"},
+                    style={
+                        "width": "120px",
+                        "height": "65px",
+                        "borderLeft": "4px solid",
+                    },
                 ),
                 width="auto",
             ),
         ]
 
@@ -3003,25 +3070,27 @@
 
     def _create_resp_exec_chart(self, df):
         """Cria o gráfico de responsáveis na execução."""
         resp_exec_counts = df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].value_counts()
 
-        fig = go.Figure(data=[
-            go.Bar(
-                x=resp_exec_counts.index,
-                y=resp_exec_counts.values,
-                text=resp_exec_counts.values,
-                textposition="auto"
-            )
-        ])
+        fig = go.Figure(
+            data=[
+                go.Bar(
+                    x=resp_exec_counts.index,
+                    y=resp_exec_counts.values,
+                    text=resp_exec_counts.values,
+                    textposition="auto",
+                )
+            ]
+        )
 
         fig.update_layout(
             self.visualizer._get_standard_layout(  # Use o visualizer aqui
                 title="SSAs por Responsável na Execução",
                 xaxis_title="Responsável",
                 yaxis_title="Quantidade",
-                chart_type="bar"
+                chart_type="bar",
             )
         )
 
         return fig
 
@@ -3116,33 +3185,33 @@
 
     def _create_detail_week_chart(self, df):
         """Cria o gráfico de detalhamento por semana usando o visualizador."""
         # Aqui já estamos usando o visualizador corretamente
         filtered_visualizer = SSAVisualizer(df)
-        return filtered_visualizer.create_week_chart(
-            use_programmed=True
-        )
+        return filtered_visualizer.create_week_chart(use_programmed=True)
 
     def _create_detail_state_chart(self, df):
         """Cria o gráfico de detalhamento por estado."""
         state_counts = df.iloc[:, SSAColumns.SITUACAO].value_counts()
 
-        fig = go.Figure(data=[
-            go.Bar(
-                x=state_counts.index,
-                y=state_counts.values,
-                text=state_counts.values,
-                textposition="auto"
-            )
-        ])
+        fig = go.Figure(
+            data=[
+                go.Bar(
+                    x=state_counts.index,
+                    y=state_counts.values,
+                    text=state_counts.values,
+                    textposition="auto",
+                )
+            ]
+        )
 
         fig.update_layout(
             self.visualizer._get_standard_layout(  # Use o visualizer aqui
                 title="SSAs Pendentes por Estado",
                 xaxis_title="Estado",
                 yaxis_title="Quantidade",
-                chart_type="bar"
+                chart_type="bar",
             )
         )
 
         return fig
 
@@ -3175,18 +3244,22 @@
         try:
             # Estatísticas básicas
             total_ssas = len(self.df)
 
             # Estatísticas de prioridade
-            prioridades = self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].value_counts()
+            prioridades = self.df.iloc[
+                :, SSAColumns.GRAU_PRIORIDADE_EMISSAO
+            ].value_counts()
             ssas_criticas = len(
                 self.df[
                     self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].str.upper()
                     == "S3.7"
                 ]
             )
-            taxa_criticidade = (ssas_criticas / total_ssas * 100) if total_ssas > 0 else 0
+            taxa_criticidade = (
+                (ssas_criticas / total_ssas * 100) if total_ssas > 0 else 0
+            )
 
             # Estatísticas de setor e estado
             setores = self.df.iloc[:, SSAColumns.SETOR_EXECUTOR].value_counts()
             estados = self.df.iloc[:, SSAColumns.SITUACAO].value_counts()
 
@@ -3272,37 +3345,44 @@
     def _create_ssa_list(self, ssas):
         """Creates clickable SSA list for modal."""
         if not ssas or len(ssas) == 0:
             return html.Div("Nenhuma SSA encontrada para este período/categoria.")
 
-        return html.Div([
-            html.Div([
-                html.Span(
-                    str(ssa),  # Garante que é string
+        return html.Div(
+            [
+                html.Div(
+                    [
+                        html.Span(
+                            str(ssa),  # Garante que é string
+                            style={
+                                "cursor": "pointer",
+                                "padding": "5px",
+                                "margin": "2px",
+                                "background": "#f8f9fa",
+                                "border-radius": "3px",
+                                "display": "inline-block",
+                                "transition": "background-color 0.2s",
+                                "user-select": "all",  # Permite selecionar todo o texto
+                            },
+                            className="ssa-chip",
+                            id={"type": "ssa-number", "index": i},
+                            title="Clique para copiar",
+                        )
+                        for i, ssa in enumerate(ssas)
+                        if ssa
+                    ],
                     style={
-                        "cursor": "pointer",
-                        "padding": "5px",
-                        "margin": "2px",
-                        "background": "#f8f9fa",
-                        "border-radius": "3px",
-                        "display": "inline-block",
-                        "transition": "background-color 0.2s",
-                        "user-select": "all",  # Permite selecionar todo o texto
+                        "max-height": "400px",
+                        "overflow-y": "auto",
+                        "padding": "10px",
+                        "display": "flex",
+                        "flex-wrap": "wrap",
+                        "gap": "5px",
                     },
-                    className="ssa-chip",
-                    id={"type": "ssa-number", "index": i},
-                    title="Clique para copiar"
-                ) for i, ssa in enumerate(ssas) if ssa
-            ], style={
-                "max-height": "400px",
-                "overflow-y": "auto",
-                "padding": "10px",
-                "display": "flex",
-                "flex-wrap": "wrap",
-                "gap": "5px",
-            })
-        ])
+                )
+            ]
+        )
 
     def _enhance_bar_chart(self, fig, chart_type, title):
         """Enhances bar chart with hover info and clickable data."""
         try:
             for trace in fig.data:
@@ -3314,18 +3394,31 @@
                     if chart_type in ["week_programmed", "week_registration"]:
                         # Para gráficos de semana
                         for i, cat in enumerate(trace.x):
                             week_mask = None
                             if chart_type == "week_programmed":
-                                week_mask = self.df.iloc[:, SSAColumns.SEMANA_PROGRAMADA] == str(cat)
+                                week_mask = self.df.iloc[
+                                    :, SSAColumns.SEMANA_PROGRAMADA
+                                ] == str(cat)
                             else:
-                                week_mask = self.df.iloc[:, SSAColumns.SEMANA_CADASTRO] == str(cat)
-
-                            if trace.name:  # Se tem nome, é um gráfico empilhado por prioridade
-                                week_mask = week_mask & (self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == trace.name)
-
-                            week_ssas = self.df[week_mask].iloc[:, SSAColumns.NUMERO_SSA].tolist()
+                                week_mask = self.df.iloc[
+                                    :, SSAColumns.SEMANA_CADASTRO
+                                ] == str(cat)
+
+                            if (
+                                trace.name
+                            ):  # Se tem nome, é um gráfico empilhado por prioridade
+                                week_mask = week_mask & (
+                                    self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO]
+                                    == trace.name
+                                )
+
+                            week_ssas = (
+                                self.df[week_mask]
+                                .iloc[:, SSAColumns.NUMERO_SSA]
+                                .tolist()
+                            )
 
                             # Texto do hover
                             ssa_preview = "<br>".join(week_ssas[:5])
                             if len(week_ssas) > 5:
                                 ssa_preview += f"<br>... (+{len(week_ssas)-5} SSAs)"
@@ -3344,19 +3437,29 @@
                     else:
                         # Para outros tipos de gráficos (código existente)
                         for cat in trace.x:
                             mask = None
                             if chart_type == "resp_prog":
-                                mask = self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO] == cat
+                                mask = (
+                                    self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO]
+                                    == cat
+                                )
                             elif chart_type == "resp_exec":
-                                mask = self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO] == cat
+                                mask = (
+                                    self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO]
+                                    == cat
+                                )
                             elif chart_type == "state":
                                 mask = self.df.iloc[:, SSAColumns.SITUACAO] == cat
 
                             ssas = []
                             if mask is not None:
-                                ssas = self.df[mask].iloc[:, SSAColumns.NUMERO_SSA].tolist()
+                                ssas = (
+                                    self.df[mask]
+                                    .iloc[:, SSAColumns.NUMERO_SSA]
+                                    .tolist()
+                                )
 
                             ssa_preview = "<br>".join(ssas[:5])
                             if len(ssas) > 5:
                                 ssa_preview += f"<br>... (+{len(ssas)-5} SSAs)"
 
@@ -3370,34 +3473,28 @@
                     trace.update(
                         hovertext=hover_text,
                         hoverinfo="text",
                         customdata=customdata,
                         hoverlabel=dict(
-                            bgcolor="white",
-                            font_size=12,
-                            font_family="Arial"
-                        )
+                            bgcolor="white", font_size=12, font_family="Arial"
+                        ),
                     )
 
                     # Desabilita zoom com scroll do mouse
                     fig.update_layout(
-                        dragmode='pan',
+                        dragmode="pan",
                         showlegend=True,
                         legend=dict(
                             orientation="h",
                             yanchor="bottom",
                             y=1.02,
                             xanchor="right",
-                            x=1
-                        )
+                            x=1,
+                        ),
                     )
 
-                    fig.update_layout(
-                        modebar=dict(
-                            remove=['scrollZoom']
-                        )
-                    )
+                    fig.update_layout(modebar=dict(remove=["scrollZoom"]))
 
         except Exception as e:
             logging.error(f"Erro ao melhorar gráfico: {str(e)}")
             return fig
 
@@ -3446,26 +3543,27 @@
     def log_with_ip(self, level, message):
         """Log message with IP address from Flask request context."""
         try:
             # Tenta obter o IP do request do Flask
             from flask import request
+
             try:
                 ip = request.remote_addr
             except RuntimeError:
                 ip = "system"
         except Exception:
             ip = "system"
 
         # Controle de frequência de logs
         current_time = datetime.now()
         log_key = f"{ip}_{message}"
-        
+
         if log_key in self._last_log:
             # Só loga novamente após 5 minutos para a mesma mensagem do mesmo IP
             if (current_time - self._last_log[log_key]).total_seconds() < 300:
                 return
-            
+
         self._last_log[log_key] = current_time
 
         try:
             if ip != "system" and ip not in self.connected_ips:
                 self.connected_ips.add(ip)
@@ -3517,90 +3615,104 @@
         """Limpa logs antigos do arquivo de log."""
         try:
             log_file = "dashboard_activity.log"
             if not os.path.exists(log_file):
                 return
-            
+
             # Lê todas as linhas do arquivo
-            with open(log_file, 'r', encoding='utf-8') as f:
+            with open(log_file, "r", encoding="utf-8") as f:
                 lines = f.readlines()
-            
+
             # Filtra apenas logs recentes
             cutoff_date = datetime.now() - timedelta(days=days)
             recent_logs = []
-            
+
             for line in lines:
                 try:
                     # Extrai a data do log (assume formato padrão no início da linha)
-                    log_date_str = line.split('-')[0].strip()
+                    log_date_str = line.split("-")[0].strip()
                     log_date = datetime.strptime(log_date_str, "%Y-%m-%d %H:%M:%S,%f")
-                    
+
                     if log_date >= cutoff_date:
                         recent_logs.append(line)
                 except (ValueError, IndexError):
                     # Se não conseguir extrair a data, mantém o log
                     recent_logs.append(line)
-            
+
             # Reescreve o arquivo apenas com logs recentes
-            with open(log_file, 'w', encoding='utf-8') as f:
+            with open(log_file, "w", encoding="utf-8") as f:
                 f.writelines(recent_logs)
-                
+
             self.logger.info(f"Logs mais antigos que {days} dias foram removidos")
-            
+
         except Exception as e:
             self.logger.error(f"Erro ao limpar logs antigos: {str(e)}")
 
     def backup_logs(self, backup_dir: str = "log_backups"):
         """Cria backup dos logs atuais."""
         try:
             if not os.path.exists(backup_dir):
                 os.makedirs(backup_dir)
-            
+
             timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-            backup_file = os.path.join(backup_dir, f"dashboard_activity_{timestamp}.log")
-            
+            backup_file = os.path.join(
+                backup_dir, f"dashboard_activity_{timestamp}.log"
+            )
+
             # Copia o arquivo de log atual
             shutil.copy2("dashboard_activity.log", backup_file)
-            
+
             # Compacta o backup
-            with zipfile.ZipFile(f"{backup_file}.zip", 'w', zipfile.ZIP_DEFLATED) as zipf:
+            with zipfile.ZipFile(
+                f"{backup_file}.zip", "w", zipfile.ZIP_DEFLATED
+            ) as zipf:
                 zipf.write(backup_file, os.path.basename(backup_file))
-            
+
             # Remove o arquivo não compactado
             os.remove(backup_file)
-            
+
             self.logger.info(f"Backup dos logs criado: {backup_file}.zip")
-            
+
         except Exception as e:
             self.logger.error(f"Erro ao criar backup dos logs: {str(e)}")
 
     def get_log_statistics(self) -> Dict:
         """Retorna estatísticas dos logs."""
         stats = {
             "total_users": len(self.active_users),
             "total_connections": len(self.connected_ips),
-            "active_users": len([u for u, info in self.active_users.items() 
-                               if (datetime.now() - info["last_activity"]).seconds < 3600]),
-            "total_actions": sum(info["action_count"] for info in self.active_users.values()),
+            "active_users": len(
+                [
+                    u
+                    for u, info in self.active_users.items()
+                    if (datetime.now() - info["last_activity"]).seconds < 3600
+                ]
+            ),
+            "total_actions": sum(
+                info["action_count"] for info in self.active_users.values()
+            ),
             "last_connection": None,
             "most_active_ip": None,
-            "most_actions": 0
+            "most_actions": 0,
         }
-        
+
         if self.active_users:
             # Encontra o usuário mais recente
-            latest_user = max(self.active_users.items(), 
-                            key=lambda x: x[1]["last_activity"])
+            latest_user = max(
+                self.active_users.items(), key=lambda x: x[1]["last_activity"]
+            )
             stats["last_connection"] = latest_user[1]["last_activity"]
-            
+
             # Encontra o usuário mais ativo
-            most_active = max(self.active_users.items(), 
-                            key=lambda x: x[1]["action_count"])
+            most_active = max(
+                self.active_users.items(), key=lambda x: x[1]["action_count"]
+            )
             stats["most_active_ip"] = most_active[0]
             stats["most_actions"] = most_active[1]["action_count"]
-        
+
         return stats
+
 
 def check_dependencies():
     """Verifica e instala dependências necessárias."""
     try:
         import xlsxwriter
@@ -3673,14 +3785,14 @@
     try:
         # Carrega os dados
         logger.info("Iniciando carregamento dos dados...")
         # Lugar do carregamento do arquivo .xlsx com os dados do scrP
         loader = DataLoader(DATA_FILE_PATH)
-        
-        # metodo antigo abaixo com caminho direto 
+
+        # metodo antigo abaixo com caminho direto
         # loader = DataLoader(r"C:\Users\menon\git\trabalho\SCRAP-SAM\Downloads\SSAs Pendentes Geral - 28-10-2024_1221PM.xlsx")
-        
+
         df = loader.load_data()
         logger.info(f"Dados carregados com sucesso. Total de SSAs: {len(df)}")
 
         # Exemplo de uso dos objetos SSAData
         ssas = loader.get_ssa_objects()
would reformat /Users/menon/git/scrap_sam_rework/src/dashboard/Report_from_excel.py
--- /Users/menon/git/scrap_sam_rework/src/utils/lixo_para_servir_de_base.py	2025-09-02 02:16:21.631423+00:00
+++ /Users/menon/git/scrap_sam_rework/src/utils/lixo_para_servir_de_base.py	2025-09-02 21:57:46.680074+00:00
@@ -12,11 +12,11 @@
 from io import BytesIO
 import os
 import json
 import psutil
 import schedule
-import yaml  
+import yaml
 
 # Imports de análise de dados
 import pandas as pd
 import numpy as np
 
@@ -84,17 +84,18 @@
         responsavel_execucao (Optional[str]): Responsável pela execução
         descricao_execucao (Optional[str]): Descrição da execução
         sistema_origem (str): Sistema onde a SSA foi gerada
         anomalia (Optional[str]): Código da anomalia relacionada
     """
-    self.numero = str(data.get('NUMERO', ''))
-    self.emissao = self._parse_date(data.get('EMITIDA_EM', ''))
-    self.prioridade = str(data.get('PRIORIDADE', ''))
-    self.status = str(data.get('STATUS', ''))
-    self.descricao = str(data.get('DESCRICAO', ''))
-    self.origem = str(data.get('ORIGEM', ''))
-    self.responsavel = str(data.get('RESPONSAVEL', ''))
+
+    self.numero = str(data.get("NUMERO", ""))
+    self.emissao = self._parse_date(data.get("EMITIDA_EM", ""))
+    self.prioridade = str(data.get("PRIORIDADE", ""))
+    self.status = str(data.get("STATUS", ""))
+    self.descricao = str(data.get("DESCRICAO", ""))
+    self.origem = str(data.get("ORIGEM", ""))
+    self.responsavel = str(data.get("RESPONSAVEL", ""))
     numero: str
     situacao: str
     derivada: Optional[str]
     localizacao: str
     desc_localizacao: str
@@ -1255,41 +1256,45 @@
         """Filtra SSAs com base nos critérios fornecidos."""
 
     def _validate_dates(self) -> bool:
         """Valida formato das datas"""
         try:
-            self._df['EMITIDA_EM'] = pd.to_datetime(self._df['EMITIDA_EM'])
+            self._df["EMITIDA_EM"] = pd.to_datetime(self._df["EMITIDA_EM"])
             return True
         except Exception as e:
             self.logger.warning(f"Erro na validação de datas: {str(e)}")
             return False
 
     def _validate_priorities(self) -> bool:
         """Valida prioridades"""
-        valid_priorities = {'ALTA', 'MEDIA', 'BAIXA', 'PROGRAMÁVEL'}
-        current_priorities = set(self._df['PRIORIDADE'].unique())
+        valid_priorities = {"ALTA", "MEDIA", "BAIXA", "PROGRAMÁVEL"}
+        current_priorities = set(self._df["PRIORIDADE"].unique())
         invalid_priorities = current_priorities - valid_priorities
-        
+
         if invalid_priorities:
-            self.logger.warning(f"Prioridades inválidas encontradas: {list(invalid_priorities)}")
+            self.logger.warning(
+                f"Prioridades inválidas encontradas: {list(invalid_priorities)}"
+            )
             return False
         return True
 
     def _validate_relationships(self) -> bool:
         """Valida relacionamentos entre SSAs"""
-        if 'ORIGEM' not in self._df.columns:
+        if "ORIGEM" not in self._df.columns:
             return True
 
-        origins = set(self._df['ORIGEM'].dropna())
-        numbers = set(self._df['NUMERO'].astype(str))
+        origins = set(self._df["ORIGEM"].dropna())
+        numbers = set(self._df["NUMERO"].astype(str))
         invalid_origins = origins - numbers
 
         if invalid_origins:
-            self.logger.warning(f"SSAs derivadas com origem inexistente: {list(invalid_origins)}")
+            self.logger.warning(
+                f"SSAs derivadas com origem inexistente: {list(invalid_origins)}"
+            )
             return False
         return True
-    
+
     def validate_data(self) -> Dict[str, bool]:
         """Valida os dados carregados"""
         if self._df is None:
             self.load()
 
@@ -1308,13 +1313,17 @@
         for _, row in self._df.iterrows():
             try:
                 ssa = SSAData(row.to_dict())
                 self._ssa_objects.append(ssa)
             except Exception as e:
-                self.logger.warning(f"Erro ao converter registro: {row['NUMERO']}. Erro: {str(e)}")
-
-        self.logger.info(f"Convertidos {len(self._ssa_objects)} registros para objetos SSAData")
+                self.logger.warning(
+                    f"Erro ao converter registro: {row['NUMERO']}. Erro: {str(e)}"
+                )
+
+        self.logger.info(
+            f"Convertidos {len(self._ssa_objects)} registros para objetos SSAData"
+        )
 
     def get_ssa_objects(self) -> List[SSAData]:
         """
         Adicionar cache para melhorar performance
 
@@ -1341,11 +1350,11 @@
         try:
             validations = {
                 "dates": self._validate_dates(),
                 "priorities": self._validate_priorities(),
                 "relationships": self._validate_relationships(),
-                "required_fields": self._validate_required_fields()
+                "required_fields": self._validate_required_fields(),
             }
 
             all_valid = all(validations.values())
             if all_valid:
                 logger.info("Validação de dados concluída com sucesso")
@@ -1417,32 +1426,38 @@
         return True
 
     def export_validated_data(self, output_path: str):
         """
         Exporta dados validados com relatório de qualidade.
-        
+
         Args:
             output_path (str): Caminho para salvar os dados exportados
         """
         try:
             # Executa validação completa
             validation_results = {
                 "dates": self._validate_dates(),
                 "priorities": self._validate_priorities(),
                 "relationships": self._validate_relationships(),
-                "required_fields": self._validate_required_fields()
+                "required_fields": self._validate_required_fields(),
             }
 
             # Prepara relatório de validação
-            validation_report = pd.DataFrame([{
-                "Data": datetime.now(),
-                "Total_SSAs": len(self.df),
-                "Datas_Válidas": validation_results["dates"],
-                "Prioridades_Válidas": validation_results["priorities"],
-                "Relacionamentos_Válidos": validation_results["relationships"],
-                "Campos_Obrigatórios_Válidos": validation_results["required_fields"]
-            }])
+            validation_report = pd.DataFrame(
+                [
+                    {
+                        "Data": datetime.now(),
+                        "Total_SSAs": len(self.df),
+                        "Datas_Válidas": validation_results["dates"],
+                        "Prioridades_Válidas": validation_results["priorities"],
+                        "Relacionamentos_Válidos": validation_results["relationships"],
+                        "Campos_Obrigatórios_Válidos": validation_results[
+                            "required_fields"
+                        ],
+                    }
+                ]
+            )
 
             # Cria arquivo Excel com múltiplas abas
             with pd.ExcelWriter(output_path, engine="xlsxwriter") as writer:
                 # Dados principais
                 self.df.to_excel(writer, sheet_name="Dados_SSAs", index=False)
@@ -1569,15 +1584,15 @@
         pd.DataFrame(stats).to_excel(writer, sheet_name="Estatísticas", index=False)
 
     def update_ssa_data(self, ssa_id: str, updates: Dict):
         """
         Atualiza dados de uma SSA específica.
-        
+
         Args:
             ssa_id (str): Número identificador da SSA
             updates (Dict): Dicionário com as atualizações a serem aplicadas
-            
+
         Returns:
             bool: True se a atualização foi bem sucedida
         """
         try:
             # Localiza a SSA
@@ -1614,19 +1629,22 @@
             return False
 
     def _validate_updates(self, updates: Dict) -> bool:
         """
         Valida as atualizações propostas.
-        
+
         Args:
             updates (Dict): Dicionário com as atualizações
-            
+
         Returns:
             bool: True se as atualizações são válidas
         """
-        valid_fields = {field.lower() for field in dir(SSAColumns) 
-                       if not field.startswith('_') and field.isupper()}
+        valid_fields = {
+            field.lower()
+            for field in dir(SSAColumns)
+            if not field.startswith("_") and field.isupper()
+        }
 
         for field in updates:
             if field.lower() not in valid_fields:
                 logger.error(f"Campo inválido: {field}")
                 return False
@@ -1650,29 +1668,29 @@
         return True
 
     def _record_update_history(self, ssa_id: str, updates: Dict):
         """
         Registra histórico de atualizações.
-        
+
         Args:
             ssa_id (str): Número identificador da SSA
             updates (Dict): Dicionário com as atualizações
         """
         history_record = {
             "ssa_id": ssa_id,
             "timestamp": datetime.now(),
-            "updates": updates
+            "updates": updates,
         }
 
         # TODO Aqui você pode implementar o armazenamento do histórico
         # Por exemplo, em um arquivo de log ou banco de dados
         logger.info(f"Histórico de atualização: {history_record}")
 
     def _update_ssa_object(self, ssa_id: str, updates: Dict):
         """
         Atualiza o objeto SSAData correspondente.
-        
+
         Args:
             ssa_id (str): Número identificador da SSA
             updates (Dict): Dicionário com as atualizações
         """
         for ssa in self.ssa_objects:
@@ -1680,25 +1698,26 @@
                 for field, value in updates.items():
                     if hasattr(ssa, field.lower()):
                         setattr(ssa, field.lower(), value)
                 break
 
+
 class KPICalculator:
     """Calcula indicadores chave de desempenho (KPIs) para SSAs."""
 
     def __init__(self, df: pd.DataFrame):
         """
         Inicializa o calculador de KPIs.
-        
+
         Args:
             df (pd.DataFrame): DataFrame com os dados das SSAs
         """
         self.df = df
         self.sla_limits = {
             "S3.7": 24,  # horas
             "S3.6": 48,  # horas
-            "S3.5": 72   # horas
+            "S3.5": 72,  # horas
         }
 
     # Métodos Implementados
 
     def calculate_efficiency_metrics(self) -> Dict:
@@ -1833,20 +1852,20 @@
         }
 
     def calculate_sector_performance(self) -> Dict:
         """
         Calcula KPIs por setor.
-        
+
         Returns:
             Dict: Métricas de performance setorial
         """
         return {
             "taxa_resolucao": self._calculate_resolution_rate_by_sector(),
             "tempo_medio_resposta": self._calculate_response_time_by_sector(),
             "volume_trabalho": self._calculate_workload_by_sector(),
             "eficiencia_setorial": self._calculate_sector_efficiency(),
-            "comparativo_setores": self._compare_sectors()
+            "comparativo_setores": self._compare_sectors(),
         }
 
     def _calculate_resolution_rate_by_sector(self) -> Dict:
         """Calcula taxa de resolução por setor."""
         resolution_rates = {}
@@ -2057,42 +2076,46 @@
     def _analyze_priority_escalation(self) -> Dict:
         """Analisa padrões de escalonamento de prioridades."""
         escalations = {
             "total_escalonamentos": len(
                 self.df[
-                    self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO] != 
-                    self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO]
+                    self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO]
+                    != self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO]
                 ]
             ),
             "por_prioridade_origem": {},
             "por_setor": {},
-            "tendencia_temporal": {}
+            "tendencia_temporal": {},
         }
 
         # Análise por prioridade de origem
         for priority in ["S3.5", "S3.6", "S3.7"]:
             escalated = self.df[
-                (self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == priority) &
-                (self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO] != priority)
+                (self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == priority)
+                & (self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO] != priority)
             ]
             if not escalated.empty:
                 escalations["por_prioridade_origem"][priority] = {
                     "quantidade": len(escalated),
-                    "destinos": escalated.iloc[:, SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO].value_counts().to_dict()
+                    "destinos": escalated.iloc[
+                        :, SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO
+                    ]
+                    .value_counts()
+                    .to_dict(),
                 }
 
         # Análise por setor
         for setor in self.df.iloc[:, SSAColumns.SETOR_EXECUTOR].unique():
             setor_df = self.df[self.df.iloc[:, SSAColumns.SETOR_EXECUTOR] == setor]
             escalated = setor_df[
-                setor_df.iloc[:, SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO] != 
-                setor_df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO]
+                setor_df.iloc[:, SSAColumns.GRAU_PRIORIDADE_PLANEJAMENTO]
+                != setor_df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO]
             ]
             if not escalated.empty:
                 escalations["por_setor"][setor] = {
                     "quantidade": len(escalated),
-                    "taxa": len(escalated) / len(setor_df)
+                    "taxa": len(escalated) / len(setor_df),
                 }
 
         # Análise temporal
         escalations["tendencia_temporal"] = self._analyze_escalation_trends()
 
@@ -2386,34 +2409,50 @@
         return compliance
 
     def _assess_workflow_compliance(self) -> float:
         """Avalia conformidade com fluxo de trabalho."""
         # Verifica sequência correta de estados
-        valid_sequence = len(self.df[
-            (self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO].notna()) &
-            (self.df.iloc[:, SSAColumns.SEMANA_PROGRAMADA].notna()) &
-            (
-                (self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].notna()) |
-                (self.df.iloc[:, SSAColumns.SITUACAO].isin(["Concluída", "Fechada"]))
-            )
-        ]) / len(self.df)
+        valid_sequence = len(
+            self.df[
+                (self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO].notna())
+                & (self.df.iloc[:, SSAColumns.SEMANA_PROGRAMADA].notna())
+                & (
+                    (self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].notna())
+                    | (
+                        self.df.iloc[:, SSAColumns.SITUACAO].isin(
+                            ["Concluída", "Fechada"]
+                        )
+                    )
+                )
+            ]
+        ) / len(self.df)
 
         return valid_sequence
 
     def _assess_responsibility_compliance(self) -> float:
         """
         Avalia conformidade com atribuição de responsabilidades.
-        
+
         Returns:
             float: Taxa de conformidade
         """
-        valid_resp = len(self.df[
-            ((self.df.iloc[:, SSAColumns.SITUACAO] != "Nova") &
-            (self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO].notna())) |
-            ((self.df.iloc[:, SSAColumns.SITUACAO].isin(["Em Execução", "Concluída"])) &
-            (self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].notna()))
-        ]) / len(self.df)
+        valid_resp = len(
+            self.df[
+                (
+                    (self.df.iloc[:, SSAColumns.SITUACAO] != "Nova")
+                    & (self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO].notna())
+                )
+                | (
+                    (
+                        self.df.iloc[:, SSAColumns.SITUACAO].isin(
+                            ["Em Execução", "Concluída"]
+                        )
+                    )
+                    & (self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].notna())
+                )
+            ]
+        ) / len(self.df)
 
         return valid_resp
 
     def calculate_resource_utilization(self) -> Dict:
         """
@@ -2761,93 +2800,121 @@
         }
 
     def _identify_overload(self) -> List[Dict]:
         """
         Identifica situações de sobrecarga.
-        
+
         Returns:
             List[Dict]: Lista de situações de sobrecarga identificadas
         """
         overload = []
 
         # Calcula médias de referência
-        avg_load = len(self.df) / len(pd.concat([
-            self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO],
-            self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO]
-        ]).unique())
+        avg_load = len(self.df) / len(
+            pd.concat(
+                [
+                    self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO],
+                    self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO],
+                ]
+            ).unique()
+        )
 
         # Analisa carga por responsável
-        for resp in pd.concat([
-            self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO],
-            self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO]
-        ]).unique():
+        for resp in pd.concat(
+            [
+                self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO],
+                self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO],
+            ]
+        ).unique():
             if pd.notna(resp) and resp.strip() != "":
                 resp_ssas = self.df[
-                    (self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO] == resp) |
-                    (self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO] == resp)
+                    (self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO] == resp)
+                    | (self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO] == resp)
                 ]
 
                 if len(resp_ssas) > 1.5 * avg_load:  # 50% acima da média
-                    overload.append({
-                        "responsavel": resp,
-                        "total_ssas": len(resp_ssas),
-                        "percentual_acima_media": (len(resp_ssas) / avg_load - 1) * 100,
-                        "ssas_criticas": len(resp_ssas[
-                            resp_ssas.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == "S3.7"
-                        ]),
-                        "media_atraso": resp_ssas.apply(
-                            lambda row: (datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]).total_seconds() / 3600,
-                            axis=1
-                        ).mean(),
-                        "recomendacao": self._generate_overload_recommendation(resp_ssas)
-                    })
+                    overload.append(
+                        {
+                            "responsavel": resp,
+                            "total_ssas": len(resp_ssas),
+                            "percentual_acima_media": (len(resp_ssas) / avg_load - 1)
+                            * 100,
+                            "ssas_criticas": len(
+                                resp_ssas[
+                                    resp_ssas.iloc[
+                                        :, SSAColumns.GRAU_PRIORIDADE_EMISSAO
+                                    ]
+                                    == "S3.7"
+                                ]
+                            ),
+                            "media_atraso": resp_ssas.apply(
+                                lambda row: (
+                                    datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]
+                                ).total_seconds()
+                                / 3600,
+                                axis=1,
+                            ).mean(),
+                            "recomendacao": self._generate_overload_recommendation(
+                                resp_ssas
+                            ),
+                        }
+                    )
 
         return sorted(overload, key=lambda x: x["percentual_acima_media"], reverse=True)
 
     def _generate_overload_recommendation(self, resp_ssas: pd.DataFrame) -> str:
         """
         Gera recomendação para situação de sobrecarga.
-        
+
         Args:
             resp_ssas: DataFrame com SSAs do responsável
-            
+
         Returns:
             str: Recomendação gerada
         """
-        critical_count = len(resp_ssas[
-            resp_ssas.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == "S3.7"
-        ])
-        delayed_count = len(resp_ssas[
-            resp_ssas.apply(
-                lambda row: (datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]).total_seconds() / 3600 > 
-                self.sla_limits.get(row.iloc[SSAColumns.GRAU_PRIORIDADE_EMISSAO], 72),
-                axis=1
-            )
-        ])
+        critical_count = len(
+            resp_ssas[resp_ssas.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == "S3.7"]
+        )
+        delayed_count = len(
+            resp_ssas[
+                resp_ssas.apply(
+                    lambda row: (
+                        datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]
+                    ).total_seconds()
+                    / 3600
+                    > self.sla_limits.get(
+                        row.iloc[SSAColumns.GRAU_PRIORIDADE_EMISSAO], 72
+                    ),
+                    axis=1,
+                )
+            ]
+        )
 
         recommendations = []
         if critical_count > 0:
             recommendations.append(f"Redistribuir {critical_count} SSAs críticas")
         if delayed_count > 0:
             recommendations.append(f"Priorizar {delayed_count} SSAs em atraso")
         if len(resp_ssas) > 10:  # número arbitrário para exemplo
-            recommendations.append("Considerar divisão de carga com outros responsáveis")
+            recommendations.append(
+                "Considerar divisão de carga com outros responsáveis"
+            )
 
         return "; ".join(recommendations) if recommendations else "Monitorar situação"
 
     def calculate_trend_indicators(self) -> Dict:
         """
         Calcula indicadores de tendência.
-        
+
         Returns:
             Dict: Indicadores de tendência calculados
         """
         return {
             "previsao_demanda": self._forecast_demand(),
             "analise_sazonalidade": self._analyze_seasonality(),
             "padroes": self._identify_patterns(),
-            "alertas": self._generate_trend_alerts()
+            "alertas": self._generate_trend_alerts(),
         }
 
     def _forecast_demand(self) -> Dict:
         """Realiza previsão de demanda."""
         # Agrupa por data
@@ -3084,17 +3151,19 @@
     def _analyze_monthly_pattern(self) -> Dict:
         """Analisa padrão mensal."""
         if self.df.empty:
             return {}
 
-        monthly_pattern = self.df.iloc[:, SSAColumns.EMITIDA_EM].dt.day.value_counts().sort_index()
+        monthly_pattern = (
+            self.df.iloc[:, SSAColumns.EMITIDA_EM].dt.day.value_counts().sort_index()
+        )
 
         return {
             "dia_pico": int(monthly_pattern.idxmax()),
             "dia_vale": int(monthly_pattern.idxmin()),
             "distribuicao": monthly_pattern.to_dict(),
-            "concentracao": float(monthly_pattern.max() / monthly_pattern.sum())
+            "concentracao": float(monthly_pattern.max() / monthly_pattern.sum()),
         }
 
     def _identify_patterns(self) -> Dict:
         """Identifica padrões nos dados."""
         patterns = {
@@ -3152,31 +3221,26 @@
         }
 
     def get_overall_health_score(self) -> float:
         """
         Calcula score geral de saúde do sistema.
-        
+
         Returns:
             float: Score de 0 a 100
         """
         metrics = self.calculate_efficiency_metrics()
         quality = self.calculate_quality_metrics()
         responsiveness = self.calculate_responsiveness_metrics()
 
         # Pesos para diferentes aspectos
-        weights = {
-            "programacao": 0.2,
-            "execucao": 0.3,
-            "sla": 0.3,
-            "qualidade": 0.2
-        }
+        weights = {"programacao": 0.2, "execucao": 0.3, "sla": 0.3, "qualidade": 0.2}
 
         score = (
-            metrics["taxa_programacao"] * weights["programacao"] +
-            metrics["taxa_execucao_simples"] * weights["execucao"] +
-            self._calculate_sla_compliance(self.df) * weights["sla"] +
-            self._calculate_quality_score(quality) * weights["qualidade"]
+            metrics["taxa_programacao"] * weights["programacao"]
+            + metrics["taxa_execucao_simples"] * weights["execucao"]
+            + self._calculate_sla_compliance(self.df) * weights["sla"]
+            + self._calculate_quality_score(quality) * weights["qualidade"]
         )
 
         return round(score * 100, 2)
 
     def _calculate_quality_score(self, quality_metrics: Dict) -> float:
@@ -3430,20 +3494,23 @@
         }
 
     def _calculate_avg_response_time(self) -> float:
         """
         Calcula tempo médio de resposta global.
-        
+
         Returns:
             float: Tempo médio em horas
         """
         if self.df.empty:
             return 0
 
         response_times = self.df.apply(
-            lambda row: (datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]).total_seconds() / 3600,
-            axis=1
+            lambda row: (
+                datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]
+            ).total_seconds()
+            / 3600,
+            axis=1,
         )
 
         return abs(response_times.mean())
 
     def get_summary_dashboard(self) -> Dict:
@@ -3463,69 +3530,74 @@
             "areas_criticas": self._identify_critical_areas(),
             "recomendacoes": self._generate_recommendations()[:3],  # top 3
             "tendencias": self._get_trend_summary(),
         }
 
-
     def _analyze_score_trend(self) -> Dict:
         """
         Analisa tendência do score de saúde.
-        
+
         Returns:
             Dict: Análise da tendência do score
         """
         # Divide o período em intervalos
         dates = self.df.iloc[:, SSAColumns.EMITIDA_EM].sort_values()
         total_days = (dates.max() - dates.min()).days
         interval_days = max(1, total_days // 5)  # divide em até 5 períodos
-        
+
         scores = []
         periods = []
         current_date = dates.min()
-        
+
         while current_date <= dates.max():
             next_date = current_date + pd.Timedelta(days=interval_days)
             period_df = self.df[
-                (self.df.iloc[:, SSAColumns.EMITIDA_EM] >= current_date) &
-                (self.df.iloc[:, SSAColumns.EMITIDA_EM] < next_date)
+                (self.df.iloc[:, SSAColumns.EMITIDA_EM] >= current_date)
+                & (self.df.iloc[:, SSAColumns.EMITIDA_EM] < next_date)
             ]
-            
+
             if not period_df.empty:
                 # Calcula score para o período
                 period_score = self.kpi_calculator.get_overall_health_score(period_df)
                 scores.append(period_score)
                 periods.append(current_date)
-            
+
             current_date = next_date
-        
+
         if len(scores) < 2:
             return {
                 "direction": "estável",
                 "variation": 0,
-                "trend": "insufficient_data"
+                "trend": "insufficient_data",
             }
-        
+
         # Calcula tendência
         slope = np.polyfit(range(len(scores)), scores, 1)[0]
-        variation = ((scores[-1] - scores[0]) / scores[0]) * 100 if scores[0] != 0 else float('inf')
-        
+        variation = (
+            ((scores[-1] - scores[0]) / scores[0]) * 100
+            if scores[0] != 0
+            else float("inf")
+        )
+
         result = {
             "direction": "melhorando" if slope > 0 else "piorando",
             "variation": abs(variation),
             "trend": "consistent" if abs(variation) > 10 else "stable",
             "scores": list(zip(periods, scores)),
-            "slope": slope
+            "slope": slope,
         }
-        
+
         # Adiciona previsão
         if len(scores) >= 3:
             next_score = scores[-1] + slope
             result["forecast"] = {
                 "next_period": next_score,
-                "confidence": min(1.0, len(scores) / 10)  # maior confiança com mais dados
+                "confidence": min(
+                    1.0, len(scores) / 10
+                ),  # maior confiança com mais dados
             }
-        
+
         return result
 
     def _analyze_bottlenecks(self) -> Dict:
         """
         Analisa gargalos no sistema.
@@ -3580,11 +3652,13 @@
                             "type": "responsavel",
                             "value": resp,
                             "workload": len(resp_ssas),
                             "critical_ssas": len(
                                 resp_ssas[
-                                    resp_ssas.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO]
+                                    resp_ssas.iloc[
+                                        :, SSAColumns.GRAU_PRIORIDADE_EMISSAO
+                                    ]
                                     == "S3.7"
                                 ]
                             ),
                             "suggestion": "Redistribuir carga de trabalho",
                         }
@@ -3619,29 +3693,34 @@
                         }
                     )
 
         return bottlenecks
 
-
     def _calculate_efficiency_by_type(self) -> Dict:
         """
         Calcula eficiência por tipo de SSA.
 
         Returns:
             Dict: Métricas de eficiência por tipo
         """
-        efficiency_metrics = {"por_servico": {}, "por_categoria": {}, "comparativos": {}}
+        efficiency_metrics = {
+            "por_servico": {},
+            "por_categoria": {},
+            "comparativos": {},
+        }
 
         # Análise por serviço de origem
         for servico in self.df.iloc[:, SSAColumns.SERVICO_ORIGEM].unique():
             servico_df = self.df[self.df.iloc[:, SSAColumns.SERVICO_ORIGEM] == servico]
 
             # Calcula métricas
             total_ssas = len(servico_df)
             concluidas = len(
                 servico_df[
-                    servico_df.iloc[:, SSAColumns.SITUACAO].isin(["Concluída", "Fechada"])
+                    servico_df.iloc[:, SSAColumns.SITUACAO].isin(
+                        ["Concluída", "Fechada"]
+                    )
                 ]
             )
             tempo_medio = abs(
                 (
                     servico_df.iloc[:, SSAColumns.EMITIDA_EM] - datetime.now()
@@ -3653,11 +3732,13 @@
                 "total_ssas": total_ssas,
                 "taxa_conclusao": concluidas / total_ssas if total_ssas > 0 else 0,
                 "tempo_medio": tempo_medio,
                 "execucao_simples": (
                     len(
-                        servico_df[servico_df.iloc[:, SSAColumns.EXECUCAO_SIMPLES] == "Sim"]
+                        servico_df[
+                            servico_df.iloc[:, SSAColumns.EXECUCAO_SIMPLES] == "Sim"
+                        ]
                     )
                     / total_ssas
                     if total_ssas > 0
                     else 0
                 ),
@@ -3688,11 +3769,13 @@
                         ]
                     )
                     / total,
                     "taxa_execucao": len(
                         categoria_df[
-                            categoria_df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].notna()
+                            categoria_df.iloc[
+                                :, SSAColumns.RESPONSAVEL_EXECUCAO
+                            ].notna()
                         ]
                     )
                     / total,
                 }
 
@@ -3723,118 +3806,130 @@
                 },
             }
 
         return efficiency_metrics
 
-
     def _generate_performance_alerts(self) -> List[Dict]:
         """
         Gera alertas de performance.
-        
+
         Returns:
             List[Dict]: Lista de alertas gerados
         """
         alerts = []
-        
+
         # Define limiares
         thresholds = {
             "sla": 0.8,  # 80% de cumprimento
             "workload": 10,  # máximo de SSAs por responsável
             "delay": 48,  # máximo de horas de atraso
-            "critical": 0.2  # 20% de SSAs críticas
+            "critical": 0.2,  # 20% de SSAs críticas
         }
-        
+
         # Verifica SLA
         sla_compliance = self._calculate_sla_compliance(self.df)
         if sla_compliance < thresholds["sla"]:
-            alerts.append({
-                "type": "sla",
-                "severity": "high",
-                "metric": "Cumprimento de SLA",
-                "value": sla_compliance,
-                "threshold": thresholds["sla"],
-                "message": f"Taxa de cumprimento de SLA ({sla_compliance*100:.1f}%) abaixo do esperado",
-                "recommendation": "Revisar processos de atendimento e alocação de recursos"
-            })
-        
+            alerts.append(
+                {
+                    "type": "sla",
+                    "severity": "high",
+                    "metric": "Cumprimento de SLA",
+                    "value": sla_compliance,
+                    "threshold": thresholds["sla"],
+                    "message": f"Taxa de cumprimento de SLA ({sla_compliance*100:.1f}%) abaixo do esperado",
+                    "recommendation": "Revisar processos de atendimento e alocação de recursos",
+                }
+            )
+
         # Verifica carga de trabalho
         workload = {}
-        for resp in pd.concat([
-            self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO],
-            self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO]
-        ]).unique():
+        for resp in pd.concat(
+            [
+                self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO],
+                self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO],
+            ]
+        ).unique():
             if pd.notna(resp) and resp.strip() != "":
                 resp_ssas = self.df[
-                    (self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO] == resp) |
-                    (self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO] == resp)
+                    (self.df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO] == resp)
+                    | (self.df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO] == resp)
                 ]
                 workload[resp] = len(resp_ssas)
-                
+
                 if len(resp_ssas) > thresholds["workload"]:
-                    alerts.append({
-                        "type": "workload",
-                        "severity": "medium",
-                        "metric": "Carga de Trabalho",
-                        "resource": resp,
-                        "value": len(resp_ssas),
-                        "threshold": thresholds["workload"],
-                        "message": f"Sobrecarga detectada para {resp}",
-                        "recommendation": "Redistribuir SSAs entre responsáveis"
-                    })
-        
+                    alerts.append(
+                        {
+                            "type": "workload",
+                            "severity": "medium",
+                            "metric": "Carga de Trabalho",
+                            "resource": resp,
+                            "value": len(resp_ssas),
+                            "threshold": thresholds["workload"],
+                            "message": f"Sobrecarga detectada para {resp}",
+                            "recommendation": "Redistribuir SSAs entre responsáveis",
+                        }
+                    )
+
         # Verifica atrasos
         delays = self.df.apply(
-            lambda row: (datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]).total_seconds() / 3600,
-            axis=1
+            lambda row: (
+                datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]
+            ).total_seconds()
+            / 3600,
+            axis=1,
         )
         if delays.max() > thresholds["delay"]:
             delayed_ssas = self.df[delays > thresholds["delay"]]
-            alerts.append({
-                "type": "delay",
-                "severity": "high",
-                "metric": "Tempo de Atendimento",
-                "value": delays.max(),
-                "threshold": thresholds["delay"],
-                "affected_ssas": len(delayed_ssas),
-                "message": f"SSAs com atraso significativo detectadas",
-                "recommendation": "Priorizar SSAs mais antigas"
-            })
-        
+            alerts.append(
+                {
+                    "type": "delay",
+                    "severity": "high",
+                    "metric": "Tempo de Atendimento",
+                    "value": delays.max(),
+                    "threshold": thresholds["delay"],
+                    "affected_ssas": len(delayed_ssas),
+                    "message": f"SSAs com atraso significativo detectadas",
+                    "recommendation": "Priorizar SSAs mais antigas",
+                }
+            )
+
         # Verifica proporção de SSAs críticas
-        critical_ratio = len(self.df[
-            self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == "S3.7"
-        ]) / len(self.df)
-        
+        critical_ratio = len(
+            self.df[self.df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == "S3.7"]
+        ) / len(self.df)
+
         if critical_ratio > thresholds["critical"]:
-            alerts.append({
-                "type": "critical",
-                "severity": "high",
-                "metric": "SSAs Críticas",
-                "value": critical_ratio,
-                "threshold": thresholds["critical"],
-                "message": f"Alta proporção de SSAs críticas ({critical_ratio*100:.1f}%)",
-                "recommendation": "Investigar causa raiz do aumento de SSAs críticas"
-            })
-        
+            alerts.append(
+                {
+                    "type": "critical",
+                    "severity": "high",
+                    "metric": "SSAs Críticas",
+                    "value": critical_ratio,
+                    "threshold": thresholds["critical"],
+                    "message": f"Alta proporção de SSAs críticas ({critical_ratio*100:.1f}%)",
+                    "recommendation": "Investigar causa raiz do aumento de SSAs críticas",
+                }
+            )
+
         return alerts
 
 
 class SSADashboard:
     """Dashboard interativo para análise de SSAs."""
 
     def __init__(self, df: pd.DataFrame):
         """
         Inicializa o dashboard.
-        
+
         Args:
             df (pd.DataFrame): DataFrame com os dados das SSAs
         """
         self.df = df
         self.app = Dash(
             __name__,
             external_stylesheets=[dbc.themes.BOOTSTRAP],
-            suppress_callback_exceptions=True
+            suppress_callback_exceptions=True,
         )
 
         # Componentes
         self.theme = DashboardTheme()
         self.visualizer = SSAVisualizer(df)
@@ -3916,106 +4011,117 @@
         self.app.run_server(debug=debug)
 
     def _create_advanced_charts(self, df: pd.DataFrame) -> Dict[str, go.Figure]:
         """
         Cria visualizações avançadas e interativas.
-        
+
         Args:
             df: DataFrame filtrado para análise
-            
+
         Returns:
             Dict[str, go.Figure]: Dicionário de visualizações
         """
         charts = {
             "temporal": self._create_temporal_visualization(df),
             "network": self._create_network_visualization(df),
             "sankey": self._create_flow_visualization(df),
             "treemap": self._create_hierarchy_visualization(df),
-            "heatmap": self._create_correlation_heatmap(df)
+            "heatmap": self._create_correlation_heatmap(df),
         }
 
         return charts
 
     def _create_temporal_visualization(self, df: pd.DataFrame) -> go.Figure:
         """Cria visualização temporal interativa."""
         if df.empty:
             return go.Figure()
 
         # Prepara dados
-        temporal_data = df.groupby([
-            df.iloc[:, SSAColumns.EMITIDA_EM].dt.date,
-            df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO]
-        ]).size().reset_index(name='count')
+        temporal_data = (
+            df.groupby(
+                [
+                    df.iloc[:, SSAColumns.EMITIDA_EM].dt.date,
+                    df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO],
+                ]
+            )
+            .size()
+            .reset_index(name="count")
+        )
 
         # Cria gráfico
         fig = go.Figure()
 
-        for priority in temporal_data.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].unique():
+        for priority in temporal_data.iloc[
+            :, SSAColumns.GRAU_PRIORIDADE_EMISSAO
+        ].unique():
             priority_data = temporal_data[
                 temporal_data.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == priority
             ]
 
-            fig.add_trace(go.Scatter(
-                x=priority_data.iloc[:, SSAColumns.EMITIDA_EM],
-                y=priority_data['count'],
-                name=priority,
-                mode='lines+markers',
-                line=dict(
-                    width=2,
-                    color=DashboardTheme.get_priority_color(priority)
-                ),
-                marker=dict(
-                    size=8,
-                    symbol='circle',
-                    color=DashboardTheme.get_priority_color(priority)
-                ),
-                hovertemplate=(
-                    "<b>Data:</b> %{x}<br>" +
-                    "<b>SSAs:</b> %{y}<br>" +
-                    "<b>Prioridade:</b> " + priority +
-                    "<extra></extra>"
+            fig.add_trace(
+                go.Scatter(
+                    x=priority_data.iloc[:, SSAColumns.EMITIDA_EM],
+                    y=priority_data["count"],
+                    name=priority,
+                    mode="lines+markers",
+                    line=dict(
+                        width=2, color=DashboardTheme.get_priority_color(priority)
+                    ),
+                    marker=dict(
+                        size=8,
+                        symbol="circle",
+                        color=DashboardTheme.get_priority_color(priority),
+                    ),
+                    hovertemplate=(
+                        "<b>Data:</b> %{x}<br>"
+                        + "<b>SSAs:</b> %{y}<br>"
+                        + "<b>Prioridade:</b> "
+                        + priority
+                        + "<extra></extra>"
+                    ),
                 )
-            ))
+            )
 
         # Configurações do layout
         fig.update_layout(
             title="Evolução Temporal de SSAs por Prioridade",
             xaxis_title="Data",
             yaxis_title="Quantidade de SSAs",
             template="plotly_white",
-            hovermode='x unified',
+            hovermode="x unified",
             legend=dict(
-                orientation="h",
-                yanchor="bottom",
-                y=1.02,
-                xanchor="right",
-                x=1
+                orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1
             ),
             updatemenus=[
                 dict(
                     buttons=[
                         dict(
                             args=[{"visible": [True] * len(fig.data)}],
                             label="Todas",
-                            method="update"
+                            method="update",
                         )
-                    ] + [
+                    ]
+                    + [
                         dict(
                             args=[{"visible": [i == j for i in range(len(fig.data))]}],
                             label=priority,
-                            method="update"
+                            method="update",
                         )
-                        for j, priority in enumerate(temporal_data.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].unique())
+                        for j, priority in enumerate(
+                            temporal_data.iloc[
+                                :, SSAColumns.GRAU_PRIORIDADE_EMISSAO
+                            ].unique()
+                        )
                     ],
                     direction="down",
                     showactive=True,
                     x=0.1,
                     xanchor="left",
                     y=1.1,
-                    yanchor="top"
+                    yanchor="top",
                 )
-            ]
+            ],
         )
 
         return fig
 
     def _create_network_visualization(self, df: pd.DataFrame) -> go.Figure:
@@ -4026,11 +4132,13 @@
         # Prepara dados de relacionamento
         relationships = []
         for setor in df.iloc[:, SSAColumns.SETOR_EMISSOR].unique():
             setor_df = df[df.iloc[:, SSAColumns.SETOR_EMISSOR] == setor]
             for executor in setor_df.iloc[:, SSAColumns.SETOR_EXECUTOR].unique():
-                count = len(setor_df[setor_df.iloc[:, SSAColumns.SETOR_EXECUTOR] == executor])
+                count = len(
+                    setor_df[setor_df.iloc[:, SSAColumns.SETOR_EXECUTOR] == executor]
+                )
                 relationships.append((setor, executor, count))
 
         # Cria nós únicos
         nodes = list(set([r[0] for r in relationships] + [r[1] for r in relationships]))
         node_indices = {node: i for i, node in enumerate(nodes)}
@@ -4054,20 +4162,21 @@
 
         # Cria gráfico
         fig = go.Figure()
 
         # Adiciona arestas
-        fig.add_trace(go.Scatter(
-            x=edge_x,
-            y=edge_y,
-            mode='lines',
-            line=dict(
-                width=np.array(edge_weights) / max(edge_weights) * 5,
-                color='#888'
-            ),
-            hoverinfo='none'
-        ))
+        fig.add_trace(
+            go.Scatter(
+                x=edge_x,
+                y=edge_y,
+                mode="lines",
+                line=dict(
+                    width=np.array(edge_weights) / max(edge_weights) * 5, color="#888"
+                ),
+                hoverinfo="none",
+            )
+        )
 
         # Adiciona nós
         node_x = []
         node_y = []
         node_text = []
@@ -4075,49 +4184,53 @@
 
         for node in nodes:
             x, y = self._get_node_position(node_indices[node], len(nodes))
             node_x.append(x)
             node_y.append(y)
-            node_text.append(f"Setor: {node}<br>Conexões: {len(node_adjacencies[node])}")
+            node_text.append(
+                f"Setor: {node}<br>Conexões: {len(node_adjacencies[node])}"
+            )
             node_sizes.append(len(node_adjacencies[node]) * 20)
 
-        fig.add_trace(go.Scatter(
-            x=node_x,
-            y=node_y,
-            mode='markers+text',
-            marker=dict(
-                size=node_sizes,
-                color=DashboardTheme.COLORS["primary"],
-                line=dict(width=2, color='white')
-            ),
-            text=nodes,
-            textposition="top center",
-            hovertext=node_text,
-            hoverinfo='text'
-        ))
+        fig.add_trace(
+            go.Scatter(
+                x=node_x,
+                y=node_y,
+                mode="markers+text",
+                marker=dict(
+                    size=node_sizes,
+                    color=DashboardTheme.COLORS["primary"],
+                    line=dict(width=2, color="white"),
+                ),
+                text=nodes,
+                textposition="top center",
+                hovertext=node_text,
+                hoverinfo="text",
+            )
+        )
 
         # Configuração do layout
         fig.update_layout(
             title="Rede de Relacionamentos entre Setores",
             showlegend=False,
-            hovermode='closest',
+            hovermode="closest",
             margin=dict(b=20, l=5, r=5, t=40),
             template="plotly_white",
             xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
-            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
+            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
         )
 
         return fig
 
     def _get_node_position(self, index: int, total_nodes: int) -> Tuple[float, float]:
         """
         Calcula posição do nó em círculo.
-        
+
         Args:
             index: Índice do nó
             total_nodes: Total de nós
-            
+
         Returns:
             Tuple[float, float]: Coordenadas x, y
         """
         angle = 2 * np.pi * index / total_nodes
         return np.cos(angle), np.sin(angle)
@@ -4129,58 +4242,65 @@
 
         # Prepara dados para o diagrama Sankey
         flow_data = []
         # Emissor -> Executor -> Estado
         for _, row in df.iterrows():
-            flow_data.append({
-                'source': row.iloc[SSAColumns.SETOR_EMISSOR],
-                'target': row.iloc[SSAColumns.SETOR_EXECUTOR],
-                'value': 1,
-                'priority': row.iloc[SSAColumns.GRAU_PRIORIDADE_EMISSAO]
-            })
-            flow_data.append({
-                'source': row.iloc[SSAColumns.SETOR_EXECUTOR],
-                'target': row.iloc[SSAColumns.SITUACAO],
-                'value': 1,
-                'priority': row.iloc[SSAColumns.GRAU_PRIORIDADE_EMISSAO]
-            })
+            flow_data.append(
+                {
+                    "source": row.iloc[SSAColumns.SETOR_EMISSOR],
+                    "target": row.iloc[SSAColumns.SETOR_EXECUTOR],
+                    "value": 1,
+                    "priority": row.iloc[SSAColumns.GRAU_PRIORIDADE_EMISSAO],
+                }
+            )
+            flow_data.append(
+                {
+                    "source": row.iloc[SSAColumns.SETOR_EXECUTOR],
+                    "target": row.iloc[SSAColumns.SITUACAO],
+                    "value": 1,
+                    "priority": row.iloc[SSAColumns.GRAU_PRIORIDADE_EMISSAO],
+                }
+            )
 
         # Cria listas únicas de nós
-        all_nodes = list(set(
-            [d['source'] for d in flow_data] +
-            [d['target'] for d in flow_data]
-        ))
+        all_nodes = list(
+            set([d["source"] for d in flow_data] + [d["target"] for d in flow_data])
+        )
         node_indices = {node: i for i, node in enumerate(all_nodes)}
 
         # Prepara dados para plotly
-        link_sources = [node_indices[d['source']] for d in flow_data]
-        link_targets = [node_indices[d['target']] for d in flow_data]
-        link_values = [d['value'] for d in flow_data]
-        link_colors = [DashboardTheme.get_priority_color(d['priority']) for d in flow_data]
+        link_sources = [node_indices[d["source"]] for d in flow_data]
+        link_targets = [node_indices[d["target"]] for d in flow_data]
+        link_values = [d["value"] for d in flow_data]
+        link_colors = [
+            DashboardTheme.get_priority_color(d["priority"]) for d in flow_data
+        ]
 
         # Cria figura
-        fig = go.Figure(data=[go.Sankey(
-            node=dict(
-                pad=15,
-                thickness=20,
-                line=dict(color="black", width=0.5),
-                label=all_nodes,
-                color=DashboardTheme.COLORS["light"]
-            ),
-            link=dict(
-                source=link_sources,
-                target=link_targets,
-                value=link_values,
-                color=link_colors
-            )
-        )])
+        fig = go.Figure(
+            data=[
+                go.Sankey(
+                    node=dict(
+                        pad=15,
+                        thickness=20,
+                        line=dict(color="black", width=0.5),
+                        label=all_nodes,
+                        color=DashboardTheme.COLORS["light"],
+                    ),
+                    link=dict(
+                        source=link_sources,
+                        target=link_targets,
+                        value=link_values,
+                        color=link_colors,
+                    ),
+                )
+            ]
+        )
 
         # Configuração do layout
         fig.update_layout(
-            title="Fluxo de SSAs no Sistema",
-            font_size=10,
-            template="plotly_white"
+            title="Fluxo de SSAs no Sistema", font_size=10, template="plotly_white"
         )
 
         return fig
 
     def _create_hierarchy_visualization(self, df: pd.DataFrame) -> go.Figure:
@@ -4191,26 +4311,30 @@
         # Prepara dados hierárquicos
         hierarchy_data = []
         for setor in df.iloc[:, SSAColumns.SETOR_EXECUTOR].unique():
             setor_df = df[df.iloc[:, SSAColumns.SETOR_EXECUTOR] == setor]
 
-            for priority in setor_df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO].unique():
+            for priority in setor_df.iloc[
+                :, SSAColumns.GRAU_PRIORIDADE_EMISSAO
+            ].unique():
                 priority_df = setor_df[
                     setor_df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == priority
                 ]
 
                 for status in priority_df.iloc[:, SSAColumns.SITUACAO].unique():
-                    count = len(priority_df[
-                        priority_df.iloc[:, SSAColumns.SITUACAO] == status
-                    ])
-
-                    hierarchy_data.append({
-                        "setor": setor,
-                        "prioridade": priority,
-                        "status": status,
-                        "count": count
-                    })
+                    count = len(
+                        priority_df[priority_df.iloc[:, SSAColumns.SITUACAO] == status]
+                    )
+
+                    hierarchy_data.append(
+                        {
+                            "setor": setor,
+                            "prioridade": priority,
+                            "status": status,
+                            "count": count,
+                        }
+                    )
 
         # Cria listas para treemap
         labels = []
         parents = []
         values = []
@@ -4226,205 +4350,226 @@
         # Nível 2: Prioridades
         for item in hierarchy_data:
             setor_priority = f"{item['setor']}_{item['prioridade']}"
             if setor_priority not in labels:
                 labels.append(setor_priority)
-                parents.append(item['setor'])
-                values.append(sum([
-                    i['count'] for i in hierarchy_data
-                    if i['setor'] == item['setor'] and i['prioridade'] == item['prioridade']
-                ]))
-                colors.append(DashboardTheme.get_priority_color(item['prioridade']))
+                parents.append(item["setor"])
+                values.append(
+                    sum(
+                        [
+                            i["count"]
+                            for i in hierarchy_data
+                            if i["setor"] == item["setor"]
+                            and i["prioridade"] == item["prioridade"]
+                        ]
+                    )
+                )
+                colors.append(DashboardTheme.get_priority_color(item["prioridade"]))
 
         # Nível 3: Status
         for item in hierarchy_data:
-            if item['count'] > 0:
+            if item["count"] > 0:
                 label = f"{item['setor']}_{item['prioridade']}_{item['status']}"
                 labels.append(label)
                 parents.append(f"{item['setor']}_{item['prioridade']}")
-                values.append(item['count'])
-                colors.append(DashboardTheme.get_state_color(item['status']))
+                values.append(item["count"])
+                colors.append(DashboardTheme.get_state_color(item["status"]))
 
         # Cria figura
-        fig = go.Figure(go.Treemap(
-            labels=labels,
-            parents=parents,
-            values=values,
-            marker=dict(colors=colors),
-            textinfo="label+value",
-            hovertemplate=(
-                "<b>%{label}</b><br>" +
-                "Quantidade: %{value}<br>" +
-                "<extra></extra>"
-            )
-        ))
+        fig = go.Figure(
+            go.Treemap(
+                labels=labels,
+                parents=parents,
+                values=values,
+                marker=dict(colors=colors),
+                textinfo="label+value",
+                hovertemplate=(
+                    "<b>%{label}</b><br>"
+                    + "Quantidade: %{value}<br>"
+                    + "<extra></extra>"
+                ),
+            )
+        )
 
         # Configuração do layout
         fig.update_layout(
-            title="Distribuição Hierárquica de SSAs",
-            template="plotly_white"
+            title="Distribuição Hierárquica de SSAs", template="plotly_white"
         )
 
         return fig
 
     def _create_correlation_heatmap(self, df: pd.DataFrame) -> go.Figure:
         """
         Cria mapa de calor de correlações.
-        
+
         Args:
             df: DataFrame para análise
-            
+
         Returns:
             go.Figure: Mapa de calor de correlações
         """
         if df.empty:
             return go.Figure()
 
         # Prepara dados para correlação
-        correlation_data = pd.DataFrame({
-            'setor': df.iloc[:, SSAColumns.SETOR_EXECUTOR],
-            'prioridade': df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO],
-            'status': df.iloc[:, SSAColumns.SITUACAO],
-            'tempo_resposta': (datetime.now() - df.iloc[:, SSAColumns.EMITIDA_EM]).dt.total_seconds() / 3600,
-            'execucao_simples': df.iloc[:, SSAColumns.EXECUCAO_SIMPLES] == 'Sim',
-            'tem_programacao': df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO].notna(),
-            'tem_execucao': df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].notna()
-        })
+        correlation_data = pd.DataFrame(
+            {
+                "setor": df.iloc[:, SSAColumns.SETOR_EXECUTOR],
+                "prioridade": df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO],
+                "status": df.iloc[:, SSAColumns.SITUACAO],
+                "tempo_resposta": (
+                    datetime.now() - df.iloc[:, SSAColumns.EMITIDA_EM]
+                ).dt.total_seconds()
+                / 3600,
+                "execucao_simples": df.iloc[:, SSAColumns.EXECUCAO_SIMPLES] == "Sim",
+                "tem_programacao": df.iloc[
+                    :, SSAColumns.RESPONSAVEL_PROGRAMACAO
+                ].notna(),
+                "tem_execucao": df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].notna(),
+            }
+        )
 
         # Codifica variáveis categóricas
-        correlation_data = pd.get_dummies(correlation_data, columns=['setor', 'prioridade', 'status'])
+        correlation_data = pd.get_dummies(
+            correlation_data, columns=["setor", "prioridade", "status"]
+        )
 
         # Calcula correlações
         corr_matrix = correlation_data.corr()
 
         # Cria mapa de calor
-        fig = go.Figure(data=go.Heatmap(
-            z=corr_matrix.values,
-            x=corr_matrix.columns,
-            y=corr_matrix.columns,
-            colorscale='RdBu',
-            zmid=0,
-            colorbar=dict(
-                title='Correlação',
-                titleside='right'
-            ),
-            hoverongaps=False,
-            hovertemplate=(
-                "<b>X:</b> %{x}<br>" +
-                "<b>Y:</b> %{y}<br>" +
-                "<b>Correlação:</b> %{z:.2f}<br>" +
-                "<extra></extra>"
-            )
-        ))
+        fig = go.Figure(
+            data=go.Heatmap(
+                z=corr_matrix.values,
+                x=corr_matrix.columns,
+                y=corr_matrix.columns,
+                colorscale="RdBu",
+                zmid=0,
+                colorbar=dict(title="Correlação", titleside="right"),
+                hoverongaps=False,
+                hovertemplate=(
+                    "<b>X:</b> %{x}<br>"
+                    + "<b>Y:</b> %{y}<br>"
+                    + "<b>Correlação:</b> %{z:.2f}<br>"
+                    + "<extra></extra>"
+                ),
+            )
+        )
 
         # Configuração do layout
         fig.update_layout(
             title="Mapa de Correlações entre Variáveis",
             template="plotly_white",
             xaxis=dict(tickangle=45),
-            yaxis=dict(tickangle=0)
+            yaxis=dict(tickangle=0),
         )
 
         return fig
 
-    def _create_predictive_visualizations(self, df: pd.DataFrame) -> Dict[str, go.Figure]:
+    def _create_predictive_visualizations(
+        self, df: pd.DataFrame
+    ) -> Dict[str, go.Figure]:
         """
         Cria visualizações preditivas.
-        
+
         Args:
             df: DataFrame para análise
-            
+
         Returns:
             Dict[str, go.Figure]: Visualizações preditivas
         """
         predictions = {
             "volume": self._create_volume_prediction(df),
             "sla": self._create_sla_prediction(df),
             "workload": self._create_workload_prediction(df),
-            "trends": self._create_trend_prediction(df)
+            "trends": self._create_trend_prediction(df),
         }
 
         return predictions
 
     def _create_volume_prediction(self, df: pd.DataFrame) -> go.Figure:
         """Cria previsão de volume."""
         if df.empty:
             return go.Figure()
 
         # Agrupa por data
-        daily_volume = df.groupby(
-            df.iloc[:, SSAColumns.EMITIDA_EM].dt.date
-        ).size().reset_index()
-        daily_volume.columns = ['date', 'volume']
+        daily_volume = (
+            df.groupby(df.iloc[:, SSAColumns.EMITIDA_EM].dt.date).size().reset_index()
+        )
+        daily_volume.columns = ["date", "volume"]
 
         # Prepara dados para previsão
         X = np.arange(len(daily_volume)).reshape(-1, 1)
-        y = daily_volume['volume'].values
+        y = daily_volume["volume"].values
 
         # Ajusta modelo linear
         model = LinearRegression()
         model.fit(X, y)
 
         # Projeta próximos 7 dias
         future_dates = [
-            daily_volume['date'].iloc[-1] + pd.Timedelta(days=i)
-            for i in range(1, 8)
+            daily_volume["date"].iloc[-1] + pd.Timedelta(days=i) for i in range(1, 8)
         ]
         future_X = np.arange(len(X), len(X) + 7).reshape(-1, 1)
         predictions = model.predict(future_X)
 
         # Cria visualização
         fig = go.Figure()
 
         # Dados históricos
-        fig.add_trace(go.Scatter(
-            x=daily_volume['date'],
-            y=daily_volume['volume'],
-            mode='lines+markers',
-            name='Histórico',
-            line=dict(color=DashboardTheme.COLORS["primary"])
-        ))
+        fig.add_trace(
+            go.Scatter(
+                x=daily_volume["date"],
+                y=daily_volume["volume"],
+                mode="lines+markers",
+                name="Histórico",
+                line=dict(color=DashboardTheme.COLORS["primary"]),
+            )
+        )
 
         # Previsões
-        fig.add_trace(go.Scatter(
-            x=future_dates,
-            y=predictions,
-            mode='lines+markers',
-            name='Previsão',
-            line=dict(
-                color=DashboardTheme.COLORS["warning"],
-                dash='dash'
-            )
-        ))
+        fig.add_trace(
+            go.Scatter(
+                x=future_dates,
+                y=predictions,
+                mode="lines+markers",
+                name="Previsão",
+                line=dict(color=DashboardTheme.COLORS["warning"], dash="dash"),
+            )
+        )
 
         # Intervalo de confiança
         confidence = np.std(y) * 1.96  # 95% de confiança
-        fig.add_trace(go.Scatter(
-            x=future_dates,
-            y=predictions + confidence,
-            mode='lines',
-            line=dict(width=0),
-            showlegend=False,
-            hoverinfo='skip'
-        ))
-        fig.add_trace(go.Scatter(
-            x=future_dates,
-            y=predictions - confidence,
-            mode='lines',
-            line=dict(width=0),
-            fillcolor='rgba(68, 68, 68, 0.2)',
-            fill='tonexty',
-            showlegend=False,
-            hoverinfo='skip'
-        ))
+        fig.add_trace(
+            go.Scatter(
+                x=future_dates,
+                y=predictions + confidence,
+                mode="lines",
+                line=dict(width=0),
+                showlegend=False,
+                hoverinfo="skip",
+            )
+        )
+        fig.add_trace(
+            go.Scatter(
+                x=future_dates,
+                y=predictions - confidence,
+                mode="lines",
+                line=dict(width=0),
+                fillcolor="rgba(68, 68, 68, 0.2)",
+                fill="tonexty",
+                showlegend=False,
+                hoverinfo="skip",
+            )
+        )
 
         fig.update_layout(
             title="Previsão de Volume de SSAs",
             xaxis_title="Data",
             yaxis_title="Quantidade de SSAs",
             template="plotly_white",
-            hovermode='x unified'
+            hovermode="x unified",
         )
 
         return fig
 
     def _create_sla_prediction(self, df: pd.DataFrame) -> go.Figure:
@@ -4435,71 +4580,65 @@
         # Calcula cumprimento de SLA por dia
         sla_compliance = []
         for date in df.iloc[:, SSAColumns.EMITIDA_EM].dt.date.unique():
             day_df = df[df.iloc[:, SSAColumns.EMITIDA_EM].dt.date == date]
             compliance = self.kpi_calculator._calculate_sla_compliance(day_df)
-            sla_compliance.append({
-                'date': date,
-                'compliance': compliance
-            })
+            sla_compliance.append({"date": date, "compliance": compliance})
 
         sla_df = pd.DataFrame(sla_compliance)
 
         # Prepara dados para previsão
         X = np.arange(len(sla_df)).reshape(-1, 1)
-        y = sla_df['compliance'].values
+        y = sla_df["compliance"].values
 
         # Ajusta modelo logístico
         model = LogisticRegression()
         model.fit(X, y > 0.8)  # Classifica como bom se > 80%
 
         # Projeta próximos 7 dias
         future_dates = [
-            sla_df['date'].iloc[-1] + pd.Timedelta(days=i)
-            for i in range(1, 8)
+            sla_df["date"].iloc[-1] + pd.Timedelta(days=i) for i in range(1, 8)
         ]
         future_X = np.arange(len(X), len(X) + 7).reshape(-1, 1)
         predictions_prob = model.predict_proba(future_X)[:, 1]
 
         # Cria visualização
         fig = go.Figure()
 
         # Dados históricos
-        fig.add_trace(go.Scatter(
-            x=sla_df['date'],
-            y=sla_df['compliance'],
-            mode='lines+markers',
-            name='Histórico',
-            line=dict(color=DashboardTheme.COLORS["primary"])
-        ))
+        fig.add_trace(
+            go.Scatter(
+                x=sla_df["date"],
+                y=sla_df["compliance"],
+                mode="lines+markers",
+                name="Histórico",
+                line=dict(color=DashboardTheme.COLORS["primary"]),
+            )
+        )
 
         # Previsões
-        fig.add_trace(go.Scatter(
-            x=future_dates,
-            y=predictions_prob,
-            mode='lines+markers',
-            name='Probabilidade',
-            line=dict(
-                color=DashboardTheme.COLORS["warning"],
-                dash='dash'
-            )
-        ))
+        fig.add_trace(
+            go.Scatter(
+                x=future_dates,
+                y=predictions_prob,
+                mode="lines+markers",
+                name="Probabilidade",
+                line=dict(color=DashboardTheme.COLORS["warning"], dash="dash"),
+            )
+        )
 
         # Linha de meta
         fig.add_hline(
-            y=0.8,
-            line_dash="dot",
-            line_color="red",
-            annotation_text="Meta (80%)"
+            y=0.8, line_dash="dot", line_color="red", annotation_text="Meta (80%)"
         )
 
         fig.update_layout(
             title="Previsão de Cumprimento de SLA",
             xaxis_title="Data",
             yaxis_title="Taxa de Cumprimento",
             template="plotly_white",
-            hovermode='x unified'
+            hovermode="x unified",
         )
 
         return fig
 
     def _create_workload_prediction(self, df: pd.DataFrame) -> go.Figure:
@@ -4512,82 +4651,86 @@
         for date in df.iloc[:, SSAColumns.EMITIDA_EM].dt.date.unique():
             day_df = df[df.iloc[:, SSAColumns.EMITIDA_EM].dt.date == date]
 
             # Conta SSAs ativas por responsável
             active_ssas = day_df[
-                ~day_df.iloc[:, SSAColumns.SITUACAO].isin(['Concluída', 'Fechada'])
+                ~day_df.iloc[:, SSAColumns.SITUACAO].isin(["Concluída", "Fechada"])
             ]
 
-            workload = len(active_ssas) / len(
-                pd.concat([
-                    active_ssas.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO],
-                    active_ssas.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO]
-                ]).unique()
-            ) if not active_ssas.empty else 0
-
-            workload_data.append({
-                'date': date,
-                'workload': workload
-            })
+            workload = (
+                len(active_ssas)
+                / len(
+                    pd.concat(
+                        [
+                            active_ssas.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO],
+                            active_ssas.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO],
+                        ]
+                    ).unique()
+                )
+                if not active_ssas.empty
+                else 0
+            )
+
+            workload_data.append({"date": date, "workload": workload})
 
         workload_df = pd.DataFrame(workload_data)
 
         # Prepara dados para previsão
         X = np.arange(len(workload_df)).reshape(-1, 1)
-        y = workload_df['workload'].values
+        y = workload_df["workload"].values
 
         # Ajusta modelo de regressão
         model = LinearRegression()
         model.fit(X, y)
 
         # Projeta próximos 7 dias
         future_dates = [
-            workload_df['date'].iloc[-1] + pd.Timedelta(days=i)
-            for i in range(1, 8)
+            workload_df["date"].iloc[-1] + pd.Timedelta(days=i) for i in range(1, 8)
         ]
         future_X = np.arange(len(X), len(X) + 7).reshape(-1, 1)
         predictions = model.predict(future_X)
 
         # Cria visualização
         fig = go.Figure()
 
         # Dados históricos
-        fig.add_trace(go.Scatter(
-            x=workload_df['date'],
-            y=workload_df['workload'],
-            mode='lines+markers',
-            name='Histórico',
-            line=dict(color=DashboardTheme.COLORS["primary"])
-        ))
+        fig.add_trace(
+            go.Scatter(
+                x=workload_df["date"],
+                y=workload_df["workload"],
+                mode="lines+markers",
+                name="Histórico",
+                line=dict(color=DashboardTheme.COLORS["primary"]),
+            )
+        )
 
         # Previsões
-        fig.add_trace(go.Scatter(
-            x=future_dates,
-            y=predictions,
-            mode='lines+markers',
-            name='Previsão',
-            line=dict(
-                color=DashboardTheme.COLORS["warning"],
-                dash='dash'
-            )
-        ))
+        fig.add_trace(
+            go.Scatter(
+                x=future_dates,
+                y=predictions,
+                mode="lines+markers",
+                name="Previsão",
+                line=dict(color=DashboardTheme.COLORS["warning"], dash="dash"),
+            )
+        )
 
         # Linha de capacidade ideal
-        media_historica = workload_df['workload'].mean()
+        media_historica = workload_df["workload"].mean()
         fig.add_hline(
             y=media_historica,
             line_dash="dot",
             line_color="green",
-            annotation_text="Média Histórica"
+            annotation_text="Média Histórica",
         )
 
         fig.update_layout(
             title="Previsão de Carga de Trabalho por Responsável",
             xaxis_title="Data",
             yaxis_title="SSAs/Responsável",
             template="plotly_white",
-            hovermode='x unified'
+            hovermode="x unified",
         )
 
         return fig
 
     def _create_trend_prediction(self, df: pd.DataFrame) -> go.Figure:
@@ -4596,76 +4739,86 @@
             return go.Figure()
 
         # Analisa tendências por tipo de SSA
         trends = []
         for col in [SSAColumns.GRAU_PRIORIDADE_EMISSAO, SSAColumns.SETOR_EXECUTOR]:
-            daily_counts = df.groupby([
-                df.iloc[:, SSAColumns.EMITIDA_EM].dt.date,
-                df.iloc[:, col]
-            ]).size().unstack(fill_value=0)
+            daily_counts = (
+                df.groupby([df.iloc[:, SSAColumns.EMITIDA_EM].dt.date, df.iloc[:, col]])
+                .size()
+                .unstack(fill_value=0)
+            )
 
             # Calcula tendência para cada categoria
             for category in daily_counts.columns:
                 slope, intercept = np.polyfit(
-                    range(len(daily_counts)), 
-                    daily_counts[category], 
-                    1
+                    range(len(daily_counts)), daily_counts[category], 1
                 )
-                trends.append({
-                    'categoria': f"{SSAColumns.get_name(col)}: {category}",
-                    'tendencia': slope,
-                    'media': daily_counts[category].mean()
-                })
+                trends.append(
+                    {
+                        "categoria": f"{SSAColumns.get_name(col)}: {category}",
+                        "tendencia": slope,
+                        "media": daily_counts[category].mean(),
+                    }
+                )
 
         # Ordena por magnitude da tendência
         trends_df = pd.DataFrame(trends)
-        trends_df['tendencia_abs'] = abs(trends_df['tendencia'])
-        trends_df = trends_df.sort_values('tendencia_abs', ascending=True)
+        trends_df["tendencia_abs"] = abs(trends_df["tendencia"])
+        trends_df = trends_df.sort_values("tendencia_abs", ascending=True)
 
         # Cria visualização
         fig = go.Figure()
 
         # Barras de tendência
-        fig.add_trace(go.Bar(
-            y=trends_df['categoria'],
-            x=trends_df['tendencia'],
-            orientation='h',
-            marker_color=[
-                DashboardTheme.COLORS["success"] if x > 0 else DashboardTheme.COLORS["danger"]
-                for x in trends_df['tendencia']
-            ],
-            name='Tendência'
-        ))
+        fig.add_trace(
+            go.Bar(
+                y=trends_df["categoria"],
+                x=trends_df["tendencia"],
+                orientation="h",
+                marker_color=[
+                    (
+                        DashboardTheme.COLORS["success"]
+                        if x > 0
+                        else DashboardTheme.COLORS["danger"]
+                    )
+                    for x in trends_df["tendencia"]
+                ],
+                name="Tendência",
+            )
+        )
 
         fig.update_layout(
             title="Análise de Tendências por Categoria",
             xaxis_title="Variação Diária Média",
             yaxis_title="Categoria",
             template="plotly_white",
             showlegend=True,
-            height=max(400, len(trends_df) * 30)  # Ajusta altura baseado no número de categorias
+            height=max(
+                400, len(trends_df) * 30
+            ),  # Ajusta altura baseado no número de categorias
         )
 
         return fig
 
     def _setup_analysis_callbacks(self):
         """Configura callbacks para análises avançadas."""
+
         @self.app.callback(
             [
                 Output("analysis-content", "children"),
-                Output("analysis-title", "children")
+                Output("analysis-title", "children"),
             ],
             [
                 Input("analysis-type-select", "value"),
                 Input("date-range-filter", "start_date"),
-                Input("date-range-filter", "end_date")
-            ]
+                Input("date-range-filter", "end_date"),
+            ],
         )
         def update_analysis_content(analysis_type, start_date, end_date):
             """
             Atualiza conteúdo da análise baseado na seleção.
-            
+
             Args:
                 analysis_type: Tipo de análise selecionada
                 start_date: Data inicial do período
                 end_date: Data final do período
             """
@@ -4684,69 +4837,91 @@
             elif analysis_type == "performance":
                 return self._create_performance_analysis(df_filtered)
             else:
                 return self._create_overview_analysis(df_filtered)
 
-    def _create_predictive_analysis(self, df: pd.DataFrame) -> Tuple[List[dbc.Card], str]:
+    def _create_predictive_analysis(
+        self, df: pd.DataFrame
+    ) -> Tuple[List[dbc.Card], str]:
         """
         Cria análise preditiva.
-        
+
         Args:
             df: DataFrame filtrado para análise
-            
+
         Returns:
             Tuple[List[dbc.Card], str]: Conteúdo e título da análise
         """
         predictions = self._create_predictive_visualizations(df)
 
         content = [
             # Card de Previsão de Volume
-            dbc.Card([
-                dbc.CardHeader(html.H5("Previsão de Volume")),
-                dbc.CardBody([
-                    dcc.Graph(figure=predictions["volume"]),
-                    html.P("Análise de tendências e projeção de volume de SSAs")
-                ])
-            ], className="mb-4"),
-            
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Previsão de Volume")),
+                    dbc.CardBody(
+                        [
+                            dcc.Graph(figure=predictions["volume"]),
+                            html.P(
+                                "Análise de tendências e projeção de volume de SSAs"
+                            ),
+                        ]
+                    ),
+                ],
+                className="mb-4",
+            ),
             # Card de Previsão de SLA
-            dbc.Card([
-                dbc.CardHeader(html.H5("Previsão de SLA")),
-                dbc.CardBody([
-                    dcc.Graph(figure=predictions["sla"]),
-                    html.P("Projeção de cumprimento de SLA")
-                ])
-            ], className="mb-4"),
-            
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Previsão de SLA")),
+                    dbc.CardBody(
+                        [
+                            dcc.Graph(figure=predictions["sla"]),
+                            html.P("Projeção de cumprimento de SLA"),
+                        ]
+                    ),
+                ],
+                className="mb-4",
+            ),
             # Card de Previsão de Carga
-            dbc.Card([
-                dbc.CardHeader(html.H5("Previsão de Carga")),
-                dbc.CardBody([
-                    dcc.Graph(figure=predictions["workload"]),
-                    html.P("Análise de tendências de carga de trabalho")
-                ])
-            ], className="mb-4"),
-            
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Previsão de Carga")),
+                    dbc.CardBody(
+                        [
+                            dcc.Graph(figure=predictions["workload"]),
+                            html.P("Análise de tendências de carga de trabalho"),
+                        ]
+                    ),
+                ],
+                className="mb-4",
+            ),
             # Card de Tendências
-            dbc.Card([
-                dbc.CardHeader(html.H5("Análise de Tendências")),
-                dbc.CardBody([
-                    dcc.Graph(figure=predictions["trends"]),
-                    html.P("Identificação de padrões e tendências")
-                ])
-            ])
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Análise de Tendências")),
+                    dbc.CardBody(
+                        [
+                            dcc.Graph(figure=predictions["trends"]),
+                            html.P("Identificação de padrões e tendências"),
+                        ]
+                    ),
+                ]
+            ),
         ]
 
         return content, "Análise Preditiva"
 
-    def _create_correlation_analysis(self, df: pd.DataFrame) -> Tuple[List[dbc.Card], str]:
+    def _create_correlation_analysis(
+        self, df: pd.DataFrame
+    ) -> Tuple[List[dbc.Card], str]:
         """
         Cria análise de correlações.
-        
+
         Args:
             df: DataFrame filtrado para análise
-            
+
         Returns:
             Tuple[List[dbc.Card], str]: Conteúdo e título da análise
         """
         # Cria heatmap de correlação
         heatmap = self._create_correlation_heatmap(df)
@@ -4754,173 +4929,213 @@
         # Identifica correlações significativas
         correlations = self._identify_significant_correlations(df)
 
         content = [
             # Card do Mapa de Calor
-            dbc.Card([
-                dbc.CardHeader(html.H5("Mapa de Correlações")),
-                dbc.CardBody([
-                    dcc.Graph(figure=heatmap),
-                    html.P("Visualização de correlações entre variáveis")
-                ])
-            ], className="mb-4"),
-            
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Mapa de Correlações")),
+                    dbc.CardBody(
+                        [
+                            dcc.Graph(figure=heatmap),
+                            html.P("Visualização de correlações entre variáveis"),
+                        ]
+                    ),
+                ],
+                className="mb-4",
+            ),
             # Card de Correlações Significativas
-            dbc.Card([
-                dbc.CardHeader(html.H5("Correlações Significativas")),
-                dbc.CardBody([
-                    html.Div([
-                        dbc.Alert([
-                            html.H6(f"{corr['var1']} × {corr['var2']}"),
-                            html.P(f"Correlação: {corr['correlation']:.2f}"),
-                            html.P(corr['interpretation'])
-                        ], color=corr['color'])
-                        for corr in correlations
-                    ])
-                ])
-            ])
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Correlações Significativas")),
+                    dbc.CardBody(
+                        [
+                            html.Div(
+                                [
+                                    dbc.Alert(
+                                        [
+                                            html.H6(f"{corr['var1']} × {corr['var2']}"),
+                                            html.P(
+                                                f"Correlação: {corr['correlation']:.2f}"
+                                            ),
+                                            html.P(corr["interpretation"]),
+                                        ],
+                                        color=corr["color"],
+                                    )
+                                    for corr in correlations
+                                ]
+                            )
+                        ]
+                    ),
+                ]
+            ),
         ]
 
         return content, "Análise de Correlações"
 
     def _identify_significant_correlations(self, df: pd.DataFrame) -> List[Dict]:
         """
         Identifica correlações significativas nos dados.
-        
+
         Args:
             df: DataFrame para análise
-            
+
         Returns:
             List[Dict]: Lista de correlações significativas
         """
         correlations = []
 
         # Prepara dados para correlação
-        correlation_data = pd.DataFrame({
-            'tempo_resposta': (datetime.now() - df.iloc[:, SSAColumns.EMITIDA_EM]).dt.total_seconds() / 3600,
-            'tem_programacao': df.iloc[:, SSAColumns.RESPONSAVEL_PROGRAMACAO].notna(),
-            'tem_execucao': df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].notna(),
-            'is_critica': df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == 'S3.7',
-            'is_simples': df.iloc[:, SSAColumns.EXECUCAO_SIMPLES] == 'Sim'
-        })
+        correlation_data = pd.DataFrame(
+            {
+                "tempo_resposta": (
+                    datetime.now() - df.iloc[:, SSAColumns.EMITIDA_EM]
+                ).dt.total_seconds()
+                / 3600,
+                "tem_programacao": df.iloc[
+                    :, SSAColumns.RESPONSAVEL_PROGRAMACAO
+                ].notna(),
+                "tem_execucao": df.iloc[:, SSAColumns.RESPONSAVEL_EXECUCAO].notna(),
+                "is_critica": df.iloc[:, SSAColumns.GRAU_PRIORIDADE_EMISSAO] == "S3.7",
+                "is_simples": df.iloc[:, SSAColumns.EXECUCAO_SIMPLES] == "Sim",
+            }
+        )
 
         # Calcula correlações
         corr_matrix = correlation_data.corr()
 
         # Identifica correlações significativas (|corr| > 0.3)
         for i in range(len(corr_matrix.columns)):
             for j in range(i + 1, len(corr_matrix.columns)):
                 corr = corr_matrix.iloc[i, j]
                 if abs(corr) > 0.3:
-                    correlations.append({
-                        'var1': corr_matrix.columns[i],
-                        'var2': corr_matrix.columns[j],
-                        'correlation': corr,
-                        'color': 'success' if corr > 0 else 'danger',
-                        'interpretation': self._interpret_correlation(
-                            corr_matrix.columns[i],
-                            corr_matrix.columns[j],
-                            corr
-                        )
-                    })
-
-        return sorted(correlations, key=lambda x: abs(x['correlation']), reverse=True)
+                    correlations.append(
+                        {
+                            "var1": corr_matrix.columns[i],
+                            "var2": corr_matrix.columns[j],
+                            "correlation": corr,
+                            "color": "success" if corr > 0 else "danger",
+                            "interpretation": self._interpret_correlation(
+                                corr_matrix.columns[i], corr_matrix.columns[j], corr
+                            ),
+                        }
+                    )
+
+        return sorted(correlations, key=lambda x: abs(x["correlation"]), reverse=True)
 
     def _interpret_correlation(self, var1: str, var2: str, correlation: float) -> str:
         """
         Interpreta o significado de uma correlação.
-        
+
         Args:
             var1: Primeira variável
             var2: Segunda variável
             correlation: Valor da correlação
-            
+
         Returns:
             str: Interpretação da correlação
         """
         direction = "positiva" if correlation > 0 else "negativa"
         strength = (
-            "forte" if abs(correlation) > 0.7 else
-            "moderada" if abs(correlation) > 0.5 else
-            "fraca"
+            "forte"
+            if abs(correlation) > 0.7
+            else "moderada" if abs(correlation) > 0.5 else "fraca"
         )
 
         interpretations = {
-            ('tempo_resposta', 'tem_programacao'): {
-                'positive': 'SSAs programadas tendem a ter maior tempo de resposta',
-                'negative': 'SSAs programadas tendem a ter menor tempo de resposta'
+            ("tempo_resposta", "tem_programacao"): {
+                "positive": "SSAs programadas tendem a ter maior tempo de resposta",
+                "negative": "SSAs programadas tendem a ter menor tempo de resposta",
             },
-            ('tempo_resposta', 'is_critica'): {
-                'positive': 'SSAs críticas estão levando mais tempo',
-                'negative': 'SSAs críticas estão sendo tratadas mais rapidamente'
+            ("tempo_resposta", "is_critica"): {
+                "positive": "SSAs críticas estão levando mais tempo",
+                "negative": "SSAs críticas estão sendo tratadas mais rapidamente",
             },
-            ('is_simples', 'tem_execucao'): {
-                'positive': 'SSAs simples são executadas mais rapidamente',
-                'negative': 'SSAs simples estão demorando mais para execução'
-            }
+            ("is_simples", "tem_execucao"): {
+                "positive": "SSAs simples são executadas mais rapidamente",
+                "negative": "SSAs simples estão demorando mais para execução",
+            },
         }
 
         key = tuple(sorted([var1, var2]))
         if key in interpretations:
-            specific = interpretations[key]['positive' if correlation > 0 else 'negative']
+            specific = interpretations[key][
+                "positive" if correlation > 0 else "negative"
+            ]
             return f"Correlação {direction} {strength}: {specific}"
 
         return f"Correlação {direction} {strength} entre {var1} e {var2}"
 
     def _create_flow_analysis(self, df: pd.DataFrame) -> Tuple[List[dbc.Card], str]:
         """
         Cria análise de fluxo.
-        
+
         Args:
             df: DataFrame filtrado para análise
-            
+
         Returns:
             Tuple[List[dbc.Card], str]: Conteúdo e título da análise
         """
         # Cria visualizações de fluxo
         sankey = self._create_flow_visualization(df)
         network = self._create_network_visualization(df)
         treemap = self._create_hierarchy_visualization(df)
 
         content = [
             # Card do Diagrama Sankey
-            dbc.Card([
-                dbc.CardHeader(html.H5("Fluxo de SSAs")),
-                dbc.CardBody([
-                    dcc.Graph(figure=sankey),
-                    html.P("Visualização do fluxo de SSAs entre setores e estados")
-                ])
-            ], className="mb-4"),
-            
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Fluxo de SSAs")),
+                    dbc.CardBody(
+                        [
+                            dcc.Graph(figure=sankey),
+                            html.P(
+                                "Visualização do fluxo de SSAs entre setores e estados"
+                            ),
+                        ]
+                    ),
+                ],
+                className="mb-4",
+            ),
             # Card da Rede de Relacionamentos
-            dbc.Card([
-                dbc.CardHeader(html.H5("Rede de Relacionamentos")),
-                dbc.CardBody([
-                    dcc.Graph(figure=network),
-                    html.P("Visualização das relações entre setores")
-                ])
-            ], className="mb-4"),
-            
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Rede de Relacionamentos")),
+                    dbc.CardBody(
+                        [
+                            dcc.Graph(figure=network),
+                            html.P("Visualização das relações entre setores"),
+                        ]
+                    ),
+                ],
+                className="mb-4",
+            ),
             # Card da Hierarquia
-            dbc.Card([
-                dbc.CardHeader(html.H5("Estrutura Hierárquica")),
-                dbc.CardBody([
-                    dcc.Graph(figure=treemap),
-                    html.P("Visualização hierárquica da distribuição de SSAs")
-                ])
-            ])
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Estrutura Hierárquica")),
+                    dbc.CardBody(
+                        [
+                            dcc.Graph(figure=treemap),
+                            html.P("Visualização hierárquica da distribuição de SSAs"),
+                        ]
+                    ),
+                ]
+            ),
         ]
 
         return content, "Análise de Fluxo"
 
-    def _create_performance_analysis(self, df: pd.DataFrame) -> Tuple[List[dbc.Card], str]:
+    def _create_performance_analysis(
+        self, df: pd.DataFrame
+    ) -> Tuple[List[dbc.Card], str]:
         """
         Cria análise de performance.
-        
+
         Args:
             df: DataFrame filtrado para análise
-            
+
         Returns:
             Tuple[List[dbc.Card], str]: Conteúdo e título da análise
         """
         # Calcula KPIs
         kpis = self.kpi_calculator.calculate_efficiency_metrics()
@@ -4930,53 +5145,92 @@
         # Cria gráficos de performance
         performance_charts = self._create_performance_charts(df)
 
         content = [
             # Card de KPIs
-            dbc.Card([
-                dbc.CardHeader(html.H5("Indicadores de Performance")),
-                dbc.CardBody([
-                    dbc.Row([
-                        dbc.Col([
-                            dbc.Card([
-                                dbc.CardBody([
-                                    html.H4(f"{kpis['taxa_programacao']*100:.1f}%"),
-                                    html.P("Taxa de Programação")
-                                ])
-                            ], className="text-center")
-                        ]),
-                        dbc.Col([
-                            dbc.Card([
-                                dbc.CardBody([
-                                    html.H4(f"{kpis['taxa_execucao_simples']*100:.1f}%"),
-                                    html.P("Taxa de Execução Simples")
-                                ])
-                            ], className="text-center")
-                        ]),
-                        dbc.Col([
-                            dbc.Card([
-                                dbc.CardBody([
-                                    html.H4(f"{responsiveness.get('taxa_sla', 0)*100:.1f}%"),
-                                    html.P("Cumprimento de SLA")
-                                ])
-                            ], className="text-center")
-                        ])
-                    ])
-                ])
-            ], className="mb-4"),
-            
+            dbc.Card(
+                [
+                    dbc.CardHeader(html.H5("Indicadores de Performance")),
+                    dbc.CardBody(
+                        [
+                            dbc.Row(
+                                [
+                                    dbc.Col(
+                                        [
+                                            dbc.Card(
+                                                [
+                                                    dbc.CardBody(
+                                                        [
+                                                            html.H4(
+                                                                f"{kpis['taxa_programacao']*100:.1f}%"
+                                                            ),
+                                                            html.P(
+                                                                "Taxa de Programação"
+                                                            ),
+                                                        ]
+                                                    )
+                                                ],
+                                                className="text-center",
+                                            )
+                                        ]
+                                    ),
+                                    dbc.Col(
+                                        [
+                                            dbc.Card(
+                                                [
+                                                    dbc.CardBody(
+                                                        [
+                                                            html.H4(
+                                                                f"{kpis['taxa_execucao_simples']*100:.1f}%"
+                                                            ),
+                                                            html.P(
+                                                                "Taxa de Execução Simples"
+                                                            ),
+                                                        ]
+                                                    )
+                                                ],
+                                                className="text-center",
+                                            )
+                                        ]
+                                    ),
+                                    dbc.Col(
+                                        [
+                                            dbc.Card(
+                                                [
+                                                    dbc.CardBody(
+                                                        [
+                                                            html.H4(
+                                                                f"{responsiveness.get('taxa_sla', 0)*100:.1f}%"
+                                                            ),
+                                                            html.P(
+                                                                "Cumprimento de SLA"
+                                                            ),
+                                                        ]
+                                                    )
+                                                ],
+                                                className="text-center",
+                                            )
+                                        ]
+                                    ),
+                                ]
+                            )
+                        ]
+                    ),
+                ],
+                className="mb-4",
+            ),
             # Cards de Gráficos de Performance
             *[
-                dbc.Card([
-                    dbc.CardHeader(html.H5(title)),
-                    dbc.CardBody([
-                        dcc.Graph(figure=fig),
-                        html.P(description)
-                    ])
-                ], className="mb-4")
+                dbc.Card(
+                    [
+                        dbc.CardHeader(html.H5(title)),
+                        dbc.CardBody([dcc.Graph(figure=fig), html.P(description)]),
+                    ],
+                    className="mb-4",
+                )
                 for title, fig, description in performance_charts
-            ]
+            ],
         ]
 
         return content, "Análise de Performance"
 
 
@@ -5106,148 +5360,172 @@
         logger.error(traceback.format_exc())
         raise
     finally:
         logger.info("Finalizando aplicação...")
 
-    def _create_performance_charts(self, df: pd.DataFrame) -> List[Tuple[str, go.Figure, str]]:
+    def _create_performance_charts(
+        self, df: pd.DataFrame
+    ) -> List[Tuple[str, go.Figure, str]]:
         """
         Cria gráficos para análise de performance.
-        
+
         Args:
             df: DataFrame para análise
-            
+
         Returns:
             List[Tuple[str, go.Figure, str]]: Lista de (título, figura, descrição)
         """
         charts = []
-        
+
         # Evolução temporal de performance (continuação)
         temporal_perf.update_layout(
             title="Evolução de Performance",
             xaxis_title="Data",
             yaxis_title="Score",
             template="plotly_white",
-            hovermode='x unified'
-        )
-        
-        charts.append((
-            "Evolução Temporal",
-            temporal_perf,
-            "Evolução dos indicadores de performance ao longo do tempo"
-        ))
+            hovermode="x unified",
+        )
+
+        charts.append(
+            (
+                "Evolução Temporal",
+                temporal_perf,
+                "Evolução dos indicadores de performance ao longo do tempo",
+            )
+        )
 
         # Performance por setor
         sector_perf = go.Figure()
         sector_metrics = {}
-        
+
         for sector in df.iloc[:, SSAColumns.SETOR_EXECUTOR].unique():
             sector_df = df[df.iloc[:, SSAColumns.SETOR_EXECUTOR] == sector]
             sector_metrics[sector] = {
-                'sla': self.kpi_calculator._calculate_sla_compliance(sector_df),
-                'quality': self.kpi_calculator._calculate_quality_score(
+                "sla": self.kpi_calculator._calculate_sla_compliance(sector_df),
+                "quality": self.kpi_calculator._calculate_quality_score(
                     self.kpi_calculator.calculate_quality_metrics()
                 ),
-                'efficiency': self.kpi_calculator.calculate_efficiency_metrics()['taxa_execucao_simples']
+                "efficiency": self.kpi_calculator.calculate_efficiency_metrics()[
+                    "taxa_execucao_simples"
+                ],
             }
 
-        sector_perf.add_trace(go.Bar(
-            name='SLA',
-            x=list(sector_metrics.keys()),
-            y=[m['sla'] for m in sector_metrics.values()],
-            marker_color=DashboardTheme.COLORS["primary"]
-        ))
-        
-        sector_perf.add_trace(go.Bar(
-            name='Qualidade',
-            x=list(sector_metrics.keys()),
-            y=[m['quality'] for m in sector_metrics.values()],
-            marker_color=DashboardTheme.COLORS["success"]
-        ))
-        
-        sector_perf.add_trace(go.Bar(
-            name='Eficiência',
-            x=list(sector_metrics.keys()),
-            y=[m['efficiency'] for m in sector_metrics.values()],
-            marker_color=DashboardTheme.COLORS["info"]
-        ))
-        
+        sector_perf.add_trace(
+            go.Bar(
+                name="SLA",
+                x=list(sector_metrics.keys()),
+                y=[m["sla"] for m in sector_metrics.values()],
+                marker_color=DashboardTheme.COLORS["primary"],
+            )
+        )
+
+        sector_perf.add_trace(
+            go.Bar(
+                name="Qualidade",
+                x=list(sector_metrics.keys()),
+                y=[m["quality"] for m in sector_metrics.values()],
+                marker_color=DashboardTheme.COLORS["success"],
+            )
+        )
+
+        sector_perf.add_trace(
+            go.Bar(
+                name="Eficiência",
+                x=list(sector_metrics.keys()),
+                y=[m["efficiency"] for m in sector_metrics.values()],
+                marker_color=DashboardTheme.COLORS["info"],
+            )
+        )
+
         sector_perf.update_layout(
             title="Performance por Setor",
             xaxis_title="Setor",
             yaxis_title="Score",
             template="plotly_white",
-            barmode='group'
-        )
-        
-        charts.append((
-            "Performance Setorial",
-            sector_perf,
-            "Comparativo de performance entre setores"
-        ))
+            barmode="group",
+        )
+
+        charts.append(
+            (
+                "Performance Setorial",
+                sector_perf,
+                "Comparativo de performance entre setores",
+            )
+        )
 
         # Distribuição de tempos de resposta
         response_times = df.apply(
-            lambda row: (datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]).total_seconds() / 3600,
-            axis=1
-        )
-        
-        time_dist = go.Figure(data=[
-            go.Histogram(
-                x=response_times,
-                nbinsx=30,
-                marker_color=DashboardTheme.COLORS["primary"]
-            )
-        ])
-        
+            lambda row: (
+                datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]
+            ).total_seconds()
+            / 3600,
+            axis=1,
+        )
+
+        time_dist = go.Figure(
+            data=[
+                go.Histogram(
+                    x=response_times,
+                    nbinsx=30,
+                    marker_color=DashboardTheme.COLORS["primary"],
+                )
+            ]
+        )
+
         time_dist.add_vline(
             x=response_times.mean(),
             line_dash="dash",
             line_color="red",
-            annotation_text="Média"
-        )
-        
+            annotation_text="Média",
+        )
+
         time_dist.update_layout(
             title="Distribuição de Tempos de Resposta",
             xaxis_title="Horas",
             yaxis_title="Frequência",
-            template="plotly_white"
-        )
-        
-        charts.append((
-            "Tempos de Resposta",
-            time_dist,
-            "Análise da distribuição dos tempos de resposta"
-        ))
+            template="plotly_white",
+        )
+
+        charts.append(
+            (
+                "Tempos de Resposta",
+                time_dist,
+                "Análise da distribuição dos tempos de resposta",
+            )
+        )
 
         return charts
 
     def _setup_alert_system(self):
         """Configura sistema de alertas."""
+
         @self.app.callback(
             Output("alert-container", "children"),
-            [Input("interval-component", "n_intervals")]
+            [Input("interval-component", "n_intervals")],
         )
         def update_alerts(n):
             """Atualiza alertas baseado nas condições atuais."""
             alerts = []
-            
+
             # Verifica SLA
             sla_compliance = self.kpi_calculator._calculate_sla_compliance(self.df)
             if sla_compliance < 0.8:
                 alerts.append(
                     dbc.Alert(
                         [
                             html.H4("Alerta de SLA", className="alert-heading"),
-                            html.P(f"Taxa de cumprimento de SLA está em {sla_compliance*100:.1f}%"),
+                            html.P(
+                                f"Taxa de cumprimento de SLA está em {sla_compliance*100:.1f}%"
+                            ),
                             html.Hr(),
                             html.P(
                                 "Recomendação: Revisar priorização e alocação de recursos",
-                                className="mb-0"
-                            )
+                                className="mb-0",
+                            ),
                         ],
                         color="danger",
-                        dismissable=True
+                        dismissable=True,
                     )
                 )
 
             # Verifica sobrecarga
             overload = self.kpi_calculator._identify_overload()
@@ -5258,76 +5536,78 @@
                             html.H4("Alerta de Sobrecarga", className="alert-heading"),
                             html.P(f"Detectados {len(overload)} casos de sobrecarga"),
                             html.Hr(),
                             html.P(
                                 "Recomendação: Redistribuir carga de trabalho",
-                                className="mb-0"
-                            )
+                                className="mb-0",
+                            ),
                         ],
                         color="warning",
-                        dismissable=True
+                        dismissable=True,
                     )
                 )
 
             # Verifica tendências
             trends = self.kpi_calculator.calculate_trend_indicators()
             if trends["alertas"]:
                 alerts.append(
                     dbc.Alert(
                         [
                             html.H4("Alerta de Tendência", className="alert-heading"),
-                            html.P(f"Detectadas {len(trends['alertas'])} tendências significativas"),
+                            html.P(
+                                f"Detectadas {len(trends['alertas'])} tendências significativas"
+                            ),
                             html.Hr(),
-                            html.P(
-                                trends["alertas"][0]["mensagem"],
-                                className="mb-0"
-                            )
+                            html.P(trends["alertas"][0]["mensagem"], className="mb-0"),
                         ],
                         color="info",
-                        dismissable=True
+                        dismissable=True,
                     )
                 )
 
             return alerts
 
     def _setup_customization_options(self):
         """Configura opções de customização."""
+
         @self.app.callback(
-            Output("dashboard-container", "style"),
-            [Input("theme-selector", "value")]
+            Output("dashboard-container", "style"), [Input("theme-selector", "value")]
         )
         def update_theme(theme):
             """Atualiza tema do dashboard."""
             if theme == "dark":
                 return {
                     "backgroundColor": DashboardTheme.COLORS["dark"],
                     "color": "white",
-                    "minHeight": "100vh"
+                    "minHeight": "100vh",
                 }
             return {
                 "backgroundColor": DashboardTheme.COLORS["light"],
                 "color": "black",
-                "minHeight": "100vh"
+                "minHeight": "100vh",
             }
 
         @self.app.callback(
             [Output(f"chart-{i}", "style") for i in range(6)],
-            [Input("layout-selector", "value")]
+            [Input("layout-selector", "value")],
         )
         def update_layout(layout):
             """Atualiza layout dos gráficos."""
             if layout == "compact":
                 return [{"height": "300px"} for _ in range(6)]
             return [{"height": "500px"} for _ in range(6)]
 
     def _setup_export_functionality(self):
         """Configura funcionalidades de exportação."""
+
         @self.app.callback(
             Output("download-dataframe-xlsx", "data"),
             [Input("btn-export-excel", "n_clicks")],
-            [State("date-range-filter", "start_date"),
-             State("date-range-filter", "end_date")]
+            [
+                State("date-range-filter", "start_date"),
+                State("date-range-filter", "end_date"),
+            ],
         )
         def export_excel(n_clicks, start_date, end_date):
             """Exporta dados para Excel."""
             if not n_clicks:
                 raise PreventUpdate
@@ -5338,66 +5618,67 @@
             )
             df_filtered = self.df[mask]
 
             # Prepara arquivo
             output = BytesIO()
-            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
+            with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                 # Dados principais
-                df_filtered.to_excel(writer, sheet_name='Dados', index=False)
-                
+                df_filtered.to_excel(writer, sheet_name="Dados", index=False)
+
                 # Análises
                 self._export_analysis_sheets(writer, df_filtered)
-                
+
                 # Métricas
                 self._export_metrics_sheet(writer, df_filtered)
 
             data = output.getvalue()
             return dcc.send_bytes(data, f"analise_ssas_{datetime.now():%Y%m%d}.xlsx")
 
     def _export_analysis_sheets(self, writer: pd.ExcelWriter, df: pd.DataFrame):
         """Exporta abas de análise."""
         # Performance
-        performance = pd.DataFrame([
-            self.kpi_calculator.calculate_efficiency_metrics()
-        ])
-        performance.to_excel(writer, sheet_name='Performance', index=False)
+        performance = pd.DataFrame([self.kpi_calculator.calculate_efficiency_metrics()])
+        performance.to_excel(writer, sheet_name="Performance", index=False)
 
         # Tendências
-        trends = pd.DataFrame([
-            self.kpi_calculator.calculate_trend_indicators()
-        ])
-        trends.to_excel(writer, sheet_name='Tendências', index=False)
+        trends = pd.DataFrame([self.kpi_calculator.calculate_trend_indicators()])
+        trends.to_excel(writer, sheet_name="Tendências", index=False)
 
         # Correlações
-        correlations = pd.DataFrame(
-            self._identify_significant_correlations(df)
-        )
-        correlations.to_excel(writer, sheet_name='Correlações', index=False)
+        correlations = pd.DataFrame(self._identify_significant_correlations(df))
+        correlations.to_excel(writer, sheet_name="Correlações", index=False)
 
     def _export_metrics_sheet(self, writer: pd.ExcelWriter, df: pd.DataFrame):
         """Exporta aba de métricas."""
         metrics = []
-        
+
         # Métricas por setor
         for setor in df.iloc[:, SSAColumns.SETOR_EXECUTOR].unique():
             setor_df = df[df.iloc[:, SSAColumns.SETOR_EXECUTOR] == setor]
-            metrics.append({
-                'setor': setor,
-                'total_ssas': len(setor_df),
-                'sla': self.kpi_calculator._calculate_sla_compliance(setor_df),
-                'tempo_medio': setor_df.apply(
-                    lambda row: (datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]).total_seconds() / 3600,
-                    axis=1
-                ).mean()
-            })
-
-        pd.DataFrame(metrics).to_excel(writer, sheet_name='Métricas', index=False)
-
-    def run_server(self, debug: bool = True, host: str = DEFAULT_HOST, port: int = DEFAULT_PORT):
+            metrics.append(
+                {
+                    "setor": setor,
+                    "total_ssas": len(setor_df),
+                    "sla": self.kpi_calculator._calculate_sla_compliance(setor_df),
+                    "tempo_medio": setor_df.apply(
+                        lambda row: (
+                            datetime.now() - row.iloc[SSAColumns.EMITIDA_EM]
+                        ).total_seconds()
+                        / 3600,
+                        axis=1,
+                    ).mean(),
+                }
+            )
+
+        pd.DataFrame(metrics).to_excel(writer, sheet_name="Métricas", index=False)
+
+    def run_server(
+        self, debug: bool = True, host: str = DEFAULT_HOST, port: int = DEFAULT_PORT
+    ):
         """
         Inicia o servidor do dashboard.
-        
+
         Args:
             debug: Modo debug
             host: Host do servidor
             port: Porta do servidor
         """
@@ -5413,22 +5694,23 @@
 
 
 def setup_error_handling():
     """
     Configura sistema avançado de tratamento de erros.
-    
+
     Returns:
         Dict: Configurações de tratamento de erros
     """
+
     def error_handler(error: Exception, context: str = None) -> Dict:
         """Handler personalizado de erros."""
         error_info = {
             "timestamp": datetime.now().isoformat(),
             "type": type(error).__name__,
             "message": str(error),
             "traceback": traceback.format_exc(),
-            "context": context
+            "context": context,
         }
 
         # Log do erro
         logger.error(
             f"Erro em {context}: {error_info['type']}\n"
@@ -5451,11 +5733,11 @@
             logger.error(f"Erro ao enviar notificação: {e}")
 
     return {
         "handler": error_handler,
         "log_dir": "logs/errors",
-        "notification_enabled": True
+        "notification_enabled": True,
     }
 
 
 def setup_performance_monitoring():
     """
@@ -5926,14 +6208,15 @@
 
 
 def setup_security():
     """
     Configura sistema de segurança.
-    
+
     Returns:
         Dict: Configurações de segurança
     """
+
     class SecurityManager:
         def __init__(self):
             self.active_sessions = {}
             self.blocked_ips = set()
             self.attempt_counts = {}
@@ -5953,22 +6236,24 @@
             # Validação de credenciais (implementar integração com sistema de autenticação)
             is_valid = self._validate_credentials(username, password)
 
             if not is_valid:
                 self.attempt_counts[ip] = self.attempt_counts.get(ip, 0) + 1
-                logger.warning(f"Tentativa de login inválida de {ip} para usuário {username}")
+                logger.warning(
+                    f"Tentativa de login inválida de {ip} para usuário {username}"
+                )
                 return False
 
             # Limpa tentativas após sucesso
             self.attempt_counts.pop(ip, None)
 
             # Cria sessão
             session_id = self._generate_session_id()
             self.active_sessions[session_id] = {
                 "username": username,
                 "ip": ip,
-                "created_at": datetime.now()
+                "created_at": datetime.now(),
             }
 
             return True
 
         def _validate_credentials(self, username: str, password: str) -> bool:
@@ -5977,24 +6262,29 @@
             return True
 
         def _generate_session_id(self) -> str:
             """Gera ID de sessão único."""
             import uuid
+
             return str(uuid.uuid4())
 
         def validate_session(self, session_id: str, ip: str) -> bool:
             """Valida sessão ativa."""
             if session_id not in self.active_sessions:
                 return False
 
             session = self.active_sessions[session_id]
             if session["ip"] != ip:
-                logger.warning(f"IP diferente para sessão {session_id}: esperado {session['ip']}, recebido {ip}")
+                logger.warning(
+                    f"IP diferente para sessão {session_id}: esperado {session['ip']}, recebido {ip}"
+                )
                 return False
 
             # Verifica expiração
-            if (datetime.now() - session["created_at"]).total_seconds() > 3600:  # 1 hora
+            if (
+                datetime.now() - session["created_at"]
+            ).total_seconds() > 3600:  # 1 hora
                 self.active_sessions.pop(session_id)
                 return False
 
             return True
 
@@ -6002,11 +6292,11 @@
             """Registra log de auditoria."""
             audit_entry = {
                 "timestamp": datetime.now().isoformat(),
                 "action": action,
                 "user": user,
-                "details": details
+                "details": details,
             }
 
             logger.info(f"Audit: {audit_entry}")
             # Implementar persistência do log de auditoria
 
@@ -6226,93 +6516,97 @@
             raise ValueError(f"Formato não suportado: {format}")
 
 
 class BackupManager:
     """Gerenciador de backup do sistema."""
-    
+
     def __init__(self):
         """Inicializa o gerenciador de backup."""
         self.backup_dir = "backups"
         self.max_backups = 10
         os.makedirs(self.backup_dir, exist_ok=True)
-    
+
     def create_backup(self, data: pd.DataFrame) -> str:
         """
         Cria backup dos dados.
-        
+
         Args:
             data: DataFrame para backup
-            
+
         Returns:
             str: Caminho do arquivo de backup
         """
         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
         filename = f"backup_{timestamp}.pkl"
         filepath = os.path.join(self.backup_dir, filename)
-        
+
         # Salva dados
         data.to_pickle(filepath)
-        
+
         # Mantém limite de backups
         self._cleanup_old_backups()
-        
+
         logger.info(f"Backup criado: {filepath}")
         return filepath
-    
+
     def _cleanup_old_backups(self):
         """Remove backups antigos."""
-        backups = sorted([
-            os.path.join(self.backup_dir, f)
-            for f in os.listdir(self.backup_dir)
-            if f.startswith("backup_") and f.endswith(".pkl")
-        ])
-        
+        backups = sorted(
+            [
+                os.path.join(self.backup_dir, f)
+                for f in os.listdir(self.backup_dir)
+                if f.startswith("backup_") and f.endswith(".pkl")
+            ]
+        )
+
         while len(backups) > self.max_backups:
             oldest = backups.pop(0)
             os.remove(oldest)
             logger.info(f"Backup removido: {oldest}")
-    
+
     def restore_backup(self, filepath: str) -> pd.DataFrame:
         """
         Restaura backup.
-        
+
         Args:
             filepath: Caminho do arquivo de backup
-            
+
         Returns:
             pd.DataFrame: Dados restaurados
         """
         if not os.path.exists(filepath):
             raise FileNotFoundError(f"Backup não encontrado: {filepath}")
-        
+
         # Carrega dados
         data = pd.read_pickle(filepath)
         logger.info(f"Backup restaurado: {filepath}")
-        
+
         return data
-    
+
     def list_backups(self) -> List[Dict]:
         """
         Lista backups disponíveis.
-        
+
         Returns:
             List[Dict]: Lista de backups com metadados
         """
         backups = []
         for filename in os.listdir(self.backup_dir):
             if filename.startswith("backup_") and filename.endswith(".pkl"):
                 filepath = os.path.join(self.backup_dir, filename)
                 stat = os.stat(filepath)
-                backups.append({
-                    "filename": filename,
-                    "filepath": filepath,
-                    "size": stat.st_size,
-                    "created_at": datetime.fromtimestamp(stat.st_ctime)
-                })
-        
+                backups.append(
+                    {
+                        "filename": filename,
+                        "filepath": filepath,
+                        "size": stat.st_size,
+                        "created_at": datetime.fromtimestamp(stat.st_ctime),
+                    }
+                )
+
         return sorted(backups, key=lambda x: x["created_at"], reverse=True)
-    
+
     def cleanup(self):
         """Limpa recursos do gerenciador de backup."""
         try:
             logger.info("Finalizando gerenciador de backup")
         except Exception as e:
@@ -6320,14 +6614,14 @@
 
 
 def setup_backup() -> BackupManager:
     """
     Configura sistema de backup.
-    
+
     Returns:
         BackupManager: Instância configurada do gerenciador de backup
-        
+
     Raises:
         Exception: Se houver erro na configuração
     """
     try:
         manager = BackupManager()
@@ -6339,24 +6633,25 @@
 
 
 def setup_maintenance() -> MaintenanceManager:
     """
     Configura e retorna uma instância do gerenciador de manutenção.
-    
+
     Returns:
         MaintenanceManager: Instância configurada do gerenciador
-    
+
     Raises:
         Exception: Se houver erro na configuração
     """
     try:
         manager = MaintenanceManager()
         logger.info("Sistema de manutenção configurado com sucesso")
         return manager
     except Exception as e:
         logger.error(f"Erro ao configurar sistema de manutenção: {e}")
         raise
+
 
 class ResourceManager:
     """Gerenciador de recursos do sistema."""
 
     def __init__(self):
@@ -6563,10 +6858,11 @@
                 time.sleep(interval)
 
         threading.Thread(target=monitor, daemon=True).start()
         logger.info(f"Monitoramento iniciado com intervalo de {interval}s")
 
+
 class AdvancedLogger:
     """Sistema avançado de logging."""
 
     def __init__(self):
         self.log_dir = "logs"
@@ -6581,155 +6877,157 @@
         os.makedirs(self.log_dir, exist_ok=True)
 
         # Configuração base
         logging.basicConfig(
             level=logging.INFO,
-            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
         )
 
         # Handler para arquivo com rotação
         file_handler = logging.handlers.RotatingFileHandler(
-            filename=os.path.join(self.log_dir, 'app.log'),
-            maxBytes=10*1024*1024,  # 10MB
+            filename=os.path.join(self.log_dir, "app.log"),
+            maxBytes=10 * 1024 * 1024,  # 10MB
             backupCount=5,
-            encoding='utf-8'
+            encoding="utf-8",
         )
         file_handler.setLevel(logging.INFO)
 
         # Handler para erros
         error_handler = logging.handlers.RotatingFileHandler(
-            filename=os.path.join(self.log_dir, 'error.log'),
-            maxBytes=10*1024*1024,
+            filename=os.path.join(self.log_dir, "error.log"),
+            maxBytes=10 * 1024 * 1024,
             backupCount=5,
-            encoding='utf-8'
+            encoding="utf-8",
         )
         error_handler.setLevel(logging.ERROR)
 
         # Handler para auditoria
         audit_handler = logging.handlers.RotatingFileHandler(
-            filename=os.path.join(self.log_dir, 'audit.log'),
-            maxBytes=10*1024*1024,
+            filename=os.path.join(self.log_dir, "audit.log"),
+            maxBytes=10 * 1024 * 1024,
             backupCount=5,
-            encoding='utf-8'
+            encoding="utf-8",
         )
 
         # Formatadores
         standard_formatter = logging.Formatter(
-            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
         )
         detailed_formatter = logging.Formatter(
-            '%(asctime)s - %(name)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s'
-        )
-        audit_formatter = logging.Formatter(
-            '%(asctime)s - AUDIT - %(message)s'
-        )
+            "%(asctime)s - %(name)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s"
+        )
+        audit_formatter = logging.Formatter("%(asctime)s - AUDIT - %(message)s")
 
         # Aplica formatadores
         file_handler.setFormatter(standard_formatter)
         error_handler.setFormatter(detailed_formatter)
         audit_handler.setFormatter(audit_formatter)
 
         # Registra handlers
         self.handlers = {
-            'file': file_handler,
-            'error': error_handler,
-            'audit': audit_handler
+            "file": file_handler,
+            "error": error_handler,
+            "audit": audit_handler,
         }
 
         # Adiciona handlers ao logger root
         for handler in self.handlers.values():
-            logging.getLogger('').addHandler(handler)
+            logging.getLogger("").addHandler(handler)
 
     def audit_log(self, action: str, user: str, details: Dict):
         """
         Registra log de auditoria.
-        
+
         Args:
             action: Ação realizada
             user: Usuário responsável
             details: Detalhes da ação
         """
-        audit_message = f"USER: {user} - ACTION: {action} - DETAILS: {json.dumps(details)}"
-        logging.getLogger('audit').info(audit_message)
-
-    def get_logs(self, level: str = None, start_date: datetime = None, end_date: datetime = None) -> List[Dict]:
+        audit_message = (
+            f"USER: {user} - ACTION: {action} - DETAILS: {json.dumps(details)}"
+        )
+        logging.getLogger("audit").info(audit_message)
+
+    def get_logs(
+        self, level: str = None, start_date: datetime = None, end_date: datetime = None
+    ) -> List[Dict]:
         """
         Recupera logs filtrados.
-        
+
         Args:
             level: Nível de log
             start_date: Data inicial
             end_date: Data final
-            
+
         Returns:
             List[Dict]: Logs filtrados
         """
         logs = []
-        log_files = {
-            'INFO': 'app.log',
-            'ERROR': 'error.log',
-            'AUDIT': 'audit.log'
-        }
+        log_files = {"INFO": "app.log", "ERROR": "error.log", "AUDIT": "audit.log"}
 
         files_to_check = [log_files[level]] if level else log_files.values()
 
         for filename in files_to_check:
             filepath = os.path.join(self.log_dir, filename)
             if os.path.exists(filepath):
-                with open(filepath, 'r', encoding='utf-8') as f:
+                with open(filepath, "r", encoding="utf-8") as f:
                     for line in f:
                         log_entry = self._parse_log_line(line)
-                        if log_entry and self._filter_log_entry(log_entry, level, start_date, end_date):
+                        if log_entry and self._filter_log_entry(
+                            log_entry, level, start_date, end_date
+                        ):
                             logs.append(log_entry)
 
-        return sorted(logs, key=lambda x: x['timestamp'])
+        return sorted(logs, key=lambda x: x["timestamp"])
 
     def _parse_log_line(self, line: str) -> Optional[Dict]:
         """
         Converte linha de log em dicionário.
-        
+
         Args:
             line: Linha do log
-            
+
         Returns:
             Optional[Dict]: Entrada de log parseada
         """
         try:
             # Formato: timestamp - name - level - message
-            parts = line.strip().split(' - ', 3)
+            parts = line.strip().split(" - ", 3)
             if len(parts) < 4:
                 return None
 
             return {
-                'timestamp': datetime.strptime(parts[0], '%Y-%m-%d %H:%M:%S,%f'),
-                'name': parts[1],
-                'level': parts[2],
-                'message': parts[3]
+                "timestamp": datetime.strptime(parts[0], "%Y-%m-%d %H:%M:%S,%f"),
+                "name": parts[1],
+                "level": parts[2],
+                "message": parts[3],
             }
         except Exception:
             return None
 
-    def _filter_log_entry(self, entry: Dict, level: str, start_date: datetime, end_date: datetime) -> bool:
+    def _filter_log_entry(
+        self, entry: Dict, level: str, start_date: datetime, end_date: datetime
+    ) -> bool:
         """
         Filtra entrada de log.
-        
+
         Args:
             entry: Entrada de log
             level: Nível desejado
             start_date: Data inicial
             end_date: Data final
-            
+
         Returns:
             bool: True se entrada atende filtros
         """
-        if level and entry['level'] != level:
+        if level and entry["level"] != level:
             return False
 
-        if start_date and entry['timestamp'] < start_date:
+        if start_date and entry["timestamp"] < start_date:
             return False
 
-        if end_date and entry['timestamp'] > end_date:
+        if end_date and entry["timestamp"] > end_date:
             return False
 
         return True
 
 
@@ -6739,54 +7037,54 @@
     logger = logging.getLogger(__name__)
 
     try:
         # Inicialização básica
         logger.info("Iniciando aplicação...")
-        
+
         # Carrega configuração
         config = Config()
-        excel_path = config.get('data', {}).get('excel_path')
-        
+        excel_path = config.get("data", {}).get("excel_path")
+
         if not excel_path:
             raise ValueError("Caminho do arquivo Excel não definido na configuração")
 
         # Sistema de logging
         logging.basicConfig(
-            level=config.get('logging', {}).get('level', 'INFO'),
-            format=config.get('logging', {}).get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+            level=config.get("logging", {}).get("level", "INFO"),
+            format=config.get("logging", {}).get(
+                "format", "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+            ),
         )
 
         # Gerenciador de recursos
         resource_manager = ResourceManager()
         resource_manager.start_monitoring()
-        resources_to_cleanup.append(('resource_manager', resource_manager))
+        resources_to_cleanup.append(("resource_manager", resource_manager))
 
         # Monitor de performance
         performance_monitor = setup_performance_monitoring()
-        resources_to_cleanup.append(('performance_monitor', performance_monitor))
+        resources_to_cleanup.append(("performance_monitor", performance_monitor))
 
         # Sistema de backup
         backup_manager = setup_backup()
-        resources_to_cleanup.append(('backup_manager', backup_manager))
+        resources_to_cleanup.append(("backup_manager", backup_manager))
         logger.info("Sistema de backup configurado com sucesso")
 
         # Sistema de manutenção
         maintenance_manager = setup_maintenance()
-        resources_to_cleanup.append(('maintenance_manager', maintenance_manager))
+        resources_to_cleanup.append(("maintenance_manager", maintenance_manager))
         logger.info("Sistema de manutenção configurado com sucesso")
 
         # Inicia thread de monitoramento de manutenção
         maintenance_thread = threading.Thread(
-            target=run_maintenance_loop,
-            args=(maintenance_manager,),
-            daemon=True
+            target=run_maintenance_loop, args=(maintenance_manager,), daemon=True
         )
         maintenance_thread.start()
 
         # Sistema de segurança
         security_manager = setup_security()
-        resources_to_cleanup.append(('security_manager', security_manager))
+        resources_to_cleanup.append(("security_manager", security_manager))
 
         # Carregamento e processamento de dados
         logger.info("Iniciando carregamento dos dados...")
         loader = DataLoader(excel_path)
 
@@ -6802,33 +7100,37 @@
             ssas = loader.get_ssa_objects()
             logger.info(f"Convertidos {len(ssas)} registros para objetos SSAData")
 
             # Análise inicial
             ssas_alta_prioridade = loader.filter_ssas(prioridade="S3.7")
-            logger.info(f"Total de SSAs com alta prioridade: {len(ssas_alta_prioridade)}")
+            logger.info(
+                f"Total de SSAs com alta prioridade: {len(ssas_alta_prioridade)}"
+            )
 
             # Gera relatório inicial
             logger.info("Gerando relatório inicial...")
             reporter = SSAReporter(df)
             reporter.save_excel_report("relatorio_ssas.xlsx")
             logger.info("Relatório Excel gerado com sucesso")
 
             # Verifica recursos
             resource_alerts = resource_manager.check_resources()
             if resource_alerts:
-                logger.warning(f"Alertas de recursos detectados: {len(resource_alerts)}")
+                logger.warning(
+                    f"Alertas de recursos detectados: {len(resource_alerts)}"
+                )
                 for alert in resource_alerts:
                     logger.warning(f"Alerta: {alert['message']}")
 
             # Inicia dashboard
             logger.info("Iniciando dashboard...")
             app = SSADashboard(df)
 
             # Configurações do servidor
-            host = config.get('server', {}).get('host', '0.0.0.0')
-            port = config.get('server', {}).get('port', 8050)
-            debug = config.get('server', {}).get('debug', False)
+            host = config.get("server", {}).get("host", "0.0.0.0")
+            port = config.get("server", {}).get("port", 8050)
+            debug = config.get("server", {}).get("debug", False)
 
             logger.info(f"Iniciando servidor em {host}:{port}")
             app.run_server(debug=debug, host=host, port=port)
 
         except Exception as e:
would reformat /Users/menon/git/scrap_sam_rework/src/utils/lixo_para_servir_de_base.py

All done! ✨ 🍰 ✨
24 files would be reformatted, 7 files would be left unchanged.
